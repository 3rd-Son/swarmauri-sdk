[
    {
        "document_name": "swarmauri/standard/__init__.py",
        "content": "```swarmauri/standard/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/models/__init__.py",
        "content": "```swarmauri/standard/models/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/models/base/__init__.py",
        "content": "```swarmauri/standard/models/base/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/models/base/ModelBase.py",
        "content": "```swarmauri/standard/models/base/ModelBase.py\nfrom abc import ABC, abstractmethod\nfrom typing import Any\nfrom ....core.models.IModel import IModel\n\nclass ModelBase(IModel, ABC):\n    \"\"\"\n    Concrete implementation of the IModel abstract base class.\n    This version includes managing the model name through a property and a setter.\n    \"\"\"\n    @abstractmethod\n    def __init__(self, model_name: str):\n        self._model_name = model_name\n    \n    @property\n    def model_name(self):\n        return self._model_name\n    \n    @model_name.setter\n    def model_name(self, value: str) -> None:\n        \"\"\"\n        Property setter that sets the name of the model.\n\n        Parameters:\n        - value (str): The new name of the model.\n        \"\"\"\n        self._model_name = value\n       \n    \n\n```"
    },
    {
        "document_name": "swarmauri/standard/models/concrete/__init__.py",
        "content": "```swarmauri/standard/models/concrete/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/models/concrete/OpenAIModel.py",
        "content": "```swarmauri/standard/models/concrete/OpenAIModel.py\nimport json\nfrom typing import List\nfrom openai import OpenAI\nfrom swarmauri.core.models.IPredict import IPredict\nfrom swarmauri.standard.models.base.ModelBase import ModelBase\n\n\nclass OpenAIModel(ModelBase, IPredict):\n    def __init__(self, api_key: str, model_name: str):\n        \"\"\"\n        Initialize the OpenAI model with an API key.\n\n        Parameters:\n        - api_key (str): Your OpenAI API key.\n        \"\"\"\n        self.client = OpenAI(api_key=api_key)\n        super().__init__(model_name)\n        \n    \n    def predict(self, messages, temperature=0.7, max_tokens=256, enable_json=False, stop: List[str] = None):\n        \"\"\"\n        Generate predictions using the OpenAI model.\n\n        Parameters:\n        - messages: Input data/messages for the model.\n        - temperature (float): Sampling temperature.\n        - max_tokens (int): Maximum number of tokens to generate.\n        - enable_json (bool): Format response as JSON.\n        \n        Returns:\n        - The generated message content.\n        \"\"\"\n        if self.client is None:\n            raise Exception(\"OpenAI client is not initialized. Call 'load_model' first.\")\n        \n        if enable_json:\n            response = self.client.chat.completions.create(\n                model=self.model_name,\n                messages=messages,\n                temperature=temperature,\n                response_format={ \"type\": \"json_object\" },\n                max_tokens=max_tokens,\n                top_p=1,\n                frequency_penalty=0,\n                presence_penalty=0,\n                stop=stop\n            )\n        else:\n            response = self.client.chat.completions.create(\n                model=self.model_name,\n                messages=messages,\n                temperature=temperature,\n                max_tokens=max_tokens,\n                top_p=1,\n                frequency_penalty=0,\n                presence_penalty=0,\n                stop=stop\n            )\n        \n        result = json.loads(response.json())\n        message_content = result['choices'][0]['message']['content']\n        \n        return message_content\n```"
    },
    {
        "document_name": "swarmauri/standard/models/concrete/AzureGPT.py",
        "content": "```swarmauri/standard/models/concrete/AzureGPT.py\nimport json\nfrom openai import AzureOpenAI\nfrom ..base.ModelBase import ModelBase\nfrom ....core.models.IPredict import IPredict\n\nclass AzureGPT(ModelBase, IPredict):\n    def __init__(self, azure_endpoint: str, api_key: str, api_version: str, model_name: str):\n        \"\"\"\n        Initialize the Azure model with an API key.\n\n        Parameters:\n        - api_key (str): Your OpenAI API key.\n        \"\"\"\n        self.azure_endpoint = azure_endpoint\n        self.api_key = api_key\n        self.api_version = api_version\n        self.client = AzureOpenAI(\n                azure_endpoint = azure_endpoint, \n                api_key = api_key,  \n                api_version = api_version\n            )\n        super().__init__(model_name)\n       \n\n    \n    def predict(self, messages, temperature=0.7, max_tokens=256, enable_json=True):\n        \"\"\"\n        Generate predictions using the OpenAI model.\n\n        Parameters:\n        - messages: Input data/messages for the model.\n        - temperature (float): Sampling temperature.\n        - max_tokens (int): Maximum number of tokens to generate.\n        - enable_json (bool): Format response as JSON.\n        \n        Returns:\n        - The generated message content.\n        \"\"\"\n        if self.client is None:\n            raise Exception(\"OpenAI client is not initialized. Call 'load_model' first.\")\n        \n        if enable_json:\n            response = self.client.chat.completions.create(\n                model=self.model_name,\n                messages=messages,\n                temperature=temperature,\n                response_format={ \"type\": \"json_object\" },\n                max_tokens=max_tokens,\n                top_p=1,\n                frequency_penalty=0,\n                presence_penalty=0,\n                stop=None\n            )\n        else:\n            response = self.client.chat.completions.create(\n                model=self.model_name,\n                messages=messages,\n                temperature=temperature,\n                max_tokens=max_tokens,\n                top_p=1,\n                frequency_penalty=0,\n                presence_penalty=0,\n                stop=None\n            )\n        \n        result = response.json()\n        message_content = json.loads(result['choices'][0]['message']['content'])\n        \n        return message_content\n```"
    },
    {
        "document_name": "swarmauri/standard/models/concrete/OpenAIImageGenerator.py",
        "content": "```swarmauri/standard/models/concrete/OpenAIImageGenerator.py\nimport json\nfrom openai import OpenAI\nfrom ..base.ModelBase import ModelBase\nfrom ....core.models.IPredict import IPredict\n\nclass OpenAIImageGenerator(ModelBase, IPredict):\n    def __init__(self, api_key: str, model_name: str = \"dall-e\"):\n        \"\"\"\n        Initializes the OpenAI image generator model.\n\n        Parameters:\n        - api_key (str): The API key provided by OpenAI for access to their services.\n        - model_name (str): Name of the image generation model provided by OpenAI.\n                            Defaults to \"dall-e\" for DALL\u00c2\u00b7E, their image generation model.\n        \"\"\"\n        self.client = OpenAI(api_key=api_key)\n        super().__init__(model_name)\n\n    def predict(self, prompt: str, size: str = \"1024x1024\", \n                quality: str = \"standard\", n: int = 1) -> str:\n        \"\"\"\n        Generates an image based on the given prompt and other parameters.\n\n        Parameters:\n        - prompt (str): A description of the image you want to generate.\n        - **kwargs: Additional parameters that the image generation endpoint might use.\n\n        Returns:\n        - str: A URL or identifier for the generated image.\n        \"\"\"\n        try:\n            response = self.client.images.generate(\n                model=self.model_name,\n                prompt=prompt,\n                size=size,\n                quality=quality,\n                n=n\n            )\n            result = response.json()\n            return result\n        \n        except Exception as e:\n            return str(e)\n```"
    },
    {
        "document_name": "swarmauri/standard/models/concrete/OpenAIToolModel.py",
        "content": "```swarmauri/standard/models/concrete/OpenAIToolModel.py\nfrom openai import OpenAI\nfrom swarmauri.standard.models.base.ModelBase import ModelBase\nfrom swarmauri.core.models.IPredict import IPredict\n\nclass OpenAIToolModel(ModelBase, IPredict):\n    def __init__(self, api_key: str, model_name: str = \"gpt-3.5-turbo-0125\"):\n        self.client = OpenAI(api_key=api_key)\n        super().__init__(model_name)\n\n    def predict(self, messages, tools=None, tool_choice=None, temperature=0.7, max_tokens=1024):\n        if tools and not tool_choice:\n            tool_choice = \"auto\"\n        response = self.client.chat.completions.create(\n            model=self.model_name,\n            messages=messages,\n            temperature=temperature,\n            max_tokens=max_tokens,\n            tools=tools,\n            tool_choice=tool_choice,\n        )\n        return response\n```"
    },
    {
        "document_name": "swarmauri/standard/agents/__init__.py",
        "content": "```swarmauri/standard/agents/__init__.py\n# -*- coding: utf-8 -*-\n\n\n```"
    },
    {
        "document_name": "swarmauri/standard/agents/base/__init__.py",
        "content": "```swarmauri/standard/agents/base/__init__.py\n# -*- coding: utf-8 -*-\n\n\n```"
    },
    {
        "document_name": "swarmauri/standard/agents/base/NamedAgentBase.py",
        "content": "```swarmauri/standard/agents/base/NamedAgentBase.py\nfrom typing import Any, Optional\nfrom abc import ABC\nfrom swarmauri.core.agents.IAgentName import IAgentName\n\n\nclass NamedAgentBase(IAgentName,ABC):\n    \n    def __init__(self, name: str):\n        self._name = name\n\n    def exec(self, input_str: Optional[Any]) -> Any:\n        raise NotImplementedError('The `exec` function has not been implemeneted on this class.')\n    \n    @property\n    def name(self) -> str:\n        return self._name\n    \n    @name.setter\n    def name(self, value) -> None:\n        self._name = value     \n```"
    },
    {
        "document_name": "swarmauri/standard/agents/base/ConversationAgentBase.py",
        "content": "```swarmauri/standard/agents/base/ConversationAgentBase.py\nfrom typing import Any, Optional\nfrom abc import ABC\n\nfrom swarmauri.core.agents.IAgentConversation import IAgentConversation\nfrom swarmauri.core.models.IModel import IModel\nfrom swarmauri.core.conversations.IConversation import IConversation\n\nfrom swarmauri.standard.agents.base.AgentBase import AgentBase\n\nclass ConversationAgentBase(AgentBase, IAgentConversation, ABC):\n    def __init__(self, model: IModel, conversation: IConversation):\n        AgentBase.__init__(self, model)\n        self._conversation = conversation\n\n    \n    def exec(self, input_str: Optional[Any]) -> Any:\n        raise NotImplementedError('The `exec` function has not been implemeneted on this class.')\n      \n\n    @property\n    def conversation(self) -> IConversation:\n        return self._conversation\n\n    @conversation.setter\n    def conversation(self, value) -> None:\n        self._conversation = value\n\n\n```"
    },
    {
        "document_name": "swarmauri/standard/agents/base/ToolAgentBase.py",
        "content": "```swarmauri/standard/agents/base/ToolAgentBase.py\nfrom abc import ABC\nfrom typing import Any, Optional\nfrom swarmauri.core.agents.IAgentConversation import IAgentConversation\nfrom swarmauri.core.models.IModel import IModel\nfrom swarmauri.core.conversations.IConversation import IConversation\nfrom swarmauri.core.toolkits.IToolkit import IToolkit\nfrom swarmauri.standard.agents.base.ConversationAgentBase import ConversationAgentBase\n\n\nclass ToolAgentBase(ConversationAgentBase, IAgentConversation, ABC):\n    \n    def __init__(self, \n                 model: IModel, \n                 conversation: IConversation,\n                 toolkit: IToolkit):\n        ConversationAgentBase.__init__(self, model, conversation)\n        self._toolkit = toolkit\n\n    def exec(self, input_str: Optional[Any]) -> Any:\n        raise NotImplementedError('The `exec` function has not been implemeneted on this class.')\n    \n    @property\n    def toolkit(self) -> IToolkit:\n        return self._toolkit\n    \n    @toolkit.setter\n    def toolkit(self, value) -> None:\n        self._toolkit = value        \n\n```"
    },
    {
        "document_name": "swarmauri/standard/agents/base/AgentBase.py",
        "content": "```swarmauri/standard/agents/base/AgentBase.py\nfrom typing import Any, Optional\nfrom abc import ABC\nfrom swarmauri.core.agents.IAgent import IAgent\nfrom swarmauri.core.models.IModel import IModel\n\n\n\nclass AgentBase(IAgent, ABC):\n    def __init__(self, model: IModel):\n        self._model = model\n\n    def exec(self, input_str: Optional[Any]) -> Any:\n        raise NotImplementedError('The `exec` function has not been implemeneted on this class.')\n    \n    @property\n    def model(self) -> IModel:\n        return self._model\n    \n    @model.setter\n    def model(self, value) -> None:\n        self._model = value        \n\n    \n    def __getattr__(self, name):\n        # Example of transforming attribute name from simplified to internal naming convention\n        internal_name = f\"_{name}\"\n        if internal_name in self.__dict__:\n            return self.__dict__[internal_name]\n        raise AttributeError(f\"'{self.__class__.__name__}' object has no attribute '{name}'\")\n    \n    def __setattr__(self, name, value):\n        # Direct assignment to the __dict__ to bypass any potential infinite recursion\n        # from setting attributes that do not explicitly exist.\n        object.__setattr__(self, name, value) \n        \n        \n    def __str__(self):\n        class_name = self.__class__.__name__\n        variables_str = \", \".join(f\"{k}={v}\" for k, v in self.__dict__.items())\n        return f\"<{class_name} {variables_str}>\"\n        \n    def __repr__(self):\n        class_name = self.__class__.__name__\n        variables_str = \", \".join(f\"{k}={v}\" for k, v in self.__dict__.items())\n        return f\"{class_name} ({variables_str})\"\n```"
    },
    {
        "document_name": "swarmauri/standard/agents/base/DocumentAgentBase.py",
        "content": "```swarmauri/standard/agents/base/DocumentAgentBase.py\nfrom typing import Any, Optional\nfrom swarmauri.core.documents.IDocument import IDocument\nfrom swarmauri.core.models.IModel import IModel\nfrom swarmauri.core.conversations.IConversation import IConversation\nfrom swarmauri.core.agents.IAgentDocument import IAgentDocumentStore\nfrom swarmauri.core.document_stores.IDocumentStore import IDocumentStore\nfrom swarmauri.standard.agents.base.ConversationAgentBase import ConversationAgentBase\nfrom swarmauri.standard.agents.base.NamedAgentBase import NamedAgentBase\n\n\nclass DocumentAgentBase(ConversationAgentBase, NamedAgentBase, IAgentDocumentStore):\n    \"\"\"\n    Base class for agents that handle and store documents within their processing scope.\n    Extends ConversationAgentBase and NamedAgentBase to utilize conversational context,\n    naming capabilities, and implements IAgentDocumentStore for document storage.\n    \"\"\"\n\n    def __init__(self, name: str, model: IModel, conversation: IConversation, document_store: IDocumentStore):\n        NamedAgentBase.__init__(self, name=name)  # Initialize name through NamedAgentBase\n        ConversationAgentBase.__init__(self, model, conversation)  # Initialize conversation and model\n        self._document_store = document_store  # Document store initialization\n\n    @property\n    def document_store(self) -> Optional[IDocument]:\n        \"\"\"\n        Gets the document store associated with this agent.\n        \n        Returns:\n            Optional[IDocument]: The document store of the agent, if any.\n        \"\"\"\n        return self._document_store\n\n    @document_store.setter\n    def document_store(self, value: IDocument) -> None:\n        \"\"\"\n        Sets the document store for this agent.\n\n        Args:\n            value (IDocument): The new document store to be associated with the agent.\n        \"\"\"\n        self._document_store = value\n    \n    def exec(self, input_data: Optional[Any]) -> Any:\n        \"\"\"\n        Placeholder method to demonstrate expected behavior of derived classes.\n        Subclasses should implement their specific logic for processing input data and optionally interacting with the document store.\n\n        Args:\n            input_data (Optional[Any]): Input data to process, can be of any format that the agent is designed to handle.\n\n        Returns:\n            Any: The result of processing the input data.\n        \"\"\"\n        raise NotImplementedError(\"Subclasses must implement the exec method.\")\n```"
    },
    {
        "document_name": "swarmauri/standard/agents/concrete/__init__.py",
        "content": "```swarmauri/standard/agents/concrete/__init__.py\n# -*- coding: utf-8 -*-\n\n\n```"
    },
    {
        "document_name": "swarmauri/standard/agents/concrete/ToolAgent.py",
        "content": "```swarmauri/standard/agents/concrete/ToolAgent.py\nfrom typing import Any, Optional, Union, Dict\nimport json\n\nfrom swarmauri.core.models.IModel import IModel\nfrom swarmauri.core.toolkits.IToolkit import IToolkit\nfrom swarmauri.core.conversations.IConversation import IConversation\nfrom swarmauri.core.messages import IMessage\n\nfrom swarmauri.standard.agents.base.ToolAgentBase import ToolAgentBase\nfrom swarmauri.standard.messages.concrete import HumanMessage, AgentMessage, FunctionMessage\n\n\nclass ToolAgent(ToolAgentBase):\n    def __init__(self, \n                 model: IModel, \n                 conversation: IConversation, \n                 toolkit: IToolkit):\n        super().__init__(model, conversation, toolkit)\n\n    def exec(self, input_data: Union[str, IMessage],  model_kwargs: Optional[Dict] = {}) -> Any:\n        conversation = self.conversation\n        model = self.model\n        toolkit = self.toolkit\n        \n\n        # Check if the input is a string, then wrap it in a HumanMessage\n        if isinstance(input_data, str):\n            human_message = HumanMessage(input_data)\n        elif isinstance(input_data, IMessage):\n            human_message = input_data\n        else:\n            raise TypeError(\"Input data must be a string or an instance of Message.\")\n\n        # Add the human message to the conversation\n        conversation.add_message(human_message)\n\n            \n        \n        # Retrieve the conversation history and predict a response\n        messages = conversation.as_dict()\n        \n        prediction = model.predict(messages=messages, \n                                   tools=toolkit.tools, \n                                   tool_choice=\"auto\", \n                                   **model_kwargs)\n        \n        prediction_message = prediction.choices[0].message\n        \n        agent_response = prediction_message.content\n        \n        agent_message = AgentMessage(content=prediction_message.content, \n                                     tool_calls=prediction_message.tool_calls)\n        conversation.add_message(agent_message)\n        \n        tool_calls = prediction.choices[0].message.tool_calls\n        if tool_calls:\n        \n            for tool_call in tool_calls:\n                func_name = tool_call.function.name\n                \n                func_call = toolkit.get_tool_by_name(func_name)\n                func_args = json.loads(tool_call.function.arguments)\n                func_result = func_call(**func_args)\n                \n                func_message = FunctionMessage(func_result, \n                                               name=func_name, \n                                               tool_call_id=tool_call.id)\n                conversation.add_message(func_message)\n            \n            \n            messages = conversation.as_dict()\n            rag_prediction = model.predict(messages=messages, \n                                           tools=toolkit.tools, \n                                           tool_choice=\"none\",\n                                           **model_kwargs)\n            \n            prediction_message = rag_prediction.choices[0].message\n            \n            agent_response = prediction_message.content\n            agent_message = AgentMessage(agent_response)\n            conversation.add_message(agent_message)\n            prediction = rag_prediction\n            \n        return agent_response \n    \n```"
    },
    {
        "document_name": "swarmauri/standard/agents/concrete/ChatSwarmAgent.py",
        "content": "```swarmauri/standard/agents/concrete/ChatSwarmAgent.py\nfrom typing import Any, Optional, Union, Dict\nfrom swarmauri.core.models.IModel import IModel\nfrom swarmauri.core.messages import IMessage\nfrom swarmauri.core.conversations import IConversation\nfrom swarmauri.standard.agents.base.ConversationAgentBase import ConversationAgentBase\nfrom swarmauri.standard.messages.concrete import HumanMessage, AgentMessage\n\nclass ChatSwarmAgent(ConversationAgentBase):\n    def __init__(self, model: IModel, conversation: IConversation):\n        super().__init__(model, conversation)\n\n    def exec(self, input_data: Union[str, IMessage], model_kwargs: Optional[Dict] = {}) -> Any:\n        conversation = self.conversation\n        model = self.model\n\n        # Check if the input is a string, then wrap it in a HumanMessage\n        if isinstance(input_data, str):\n            human_message = HumanMessage(input_data)\n        elif isinstance(input_data, IMessage):\n            human_message = input_data\n        else:\n            raise TypeError(\"Input data must be a string or an instance of Message.\")\n\n        # Add the human message to the conversation\n        conversation.add_message(human_message)\n        \n        # Retrieve the conversation history and predict a response\n        messages = conversation.as_dict()\n        if model_kwargs:\n            prediction = model.predict(messages=messages, **model_kwargs)\n        else:\n            prediction = model.predict(messages=messages)\n        # Create an AgentMessage instance with the model's response and update the conversation\n        agent_message = AgentMessage(prediction)\n        conversation.add_message(agent_message)\n        \n        return prediction\n```"
    },
    {
        "document_name": "swarmauri/standard/agents/concrete/SingleCommandAgent.py",
        "content": "```swarmauri/standard/agents/concrete/SingleCommandAgent.py\nfrom typing import Any, Optional\n\nfrom swarmauri.core.models.IModel import IModel\nfrom swarmauri.core.conversations.IConversation import IConversation\n\nfrom swarmauri.standard.agents.base.AgentBase import AgentBase\n\nclass SingleCommandAgent(AgentBase):\n    def __init__(self, model: IModel, \n                 conversation: IConversation):\n        super().__init__(model, conversation)\n\n    def exec(self, input_str: Optional[str] = None) -> Any:\n        model = self.model\n        prediction = model.predict(input_str)\n        \n        return prediction\n```"
    },
    {
        "document_name": "swarmauri/standard/agents/concrete/SimpleSwarmAgent.py",
        "content": "```swarmauri/standard/agents/concrete/SimpleSwarmAgent.py\nfrom typing import Any, Optional\n\nfrom swarmauri.core.models.IModel import IModel\nfrom swarmauri.core.conversations.IConversation import IConversation\n\n\nfrom swarmauri.standard.agents.base.SwarmAgentBase import AgentBase\nfrom swarmauri.standard.messages.concrete import HumanMessage\n\nclass SimpleSwarmAgent(AgentBase):\n    def __init__(self, model: IModel, \n                 conversation: IConversation):\n        super().__init__(model, conversation)\n\n    def exec(self, input_str: Optional[str] = None) -> Any:\n        conversation = self.conversation\n        model = self.model\n\n        # Construct a new human message (for example purposes)\n        if input_str:\n            human_message = HumanMessage(input_str)\n            conversation.add_message(human_message)\n        \n        messages = conversation.as_dict()\n        prediction = model.predict(messages=messages)\n        return prediction\n```"
    },
    {
        "document_name": "swarmauri/standard/agents/concrete/MultiPartyChatSwarmAgent.py",
        "content": "```swarmauri/standard/agents/concrete/MultiPartyChatSwarmAgent.py\nfrom typing import Any, Optional, Union, Dict\n\nfrom swarmauri.core.models.IModel import IModel\nfrom swarmauri.core.messages import IMessage\n\nfrom swarmauri.standard.agents.base.ConversationAgentBase import ConversationAgentBase\nfrom swarmauri.standard.agents.base.NamedAgentBase import NamedAgentBase\nfrom swarmauri.standard.conversations.concrete.SharedConversation import SharedConversation\nfrom swarmauri.standard.messages.concrete import HumanMessage, AgentMessage\n\nclass MultiPartyChatSwarmAgent(ConversationAgentBase, NamedAgentBase):\n    def __init__(self, \n                 model: IModel, \n                 conversation: SharedConversation,\n                 name: str):\n        ConversationAgentBase.__init__(self, model, conversation)\n        NamedAgentBase.__init__(self, name)\n\n    def exec(self, input_data: Union[str, IMessage] = \"\", model_kwargs: Optional[Dict] = {}) -> Any:\n        conversation = self.conversation\n        model = self.model\n\n        # Check if the input is a string, then wrap it in a HumanMessage\n        if isinstance(input_data, str):\n            human_message = HumanMessage(input_data)\n        elif isinstance(input_data, IMessage):\n            human_message = input_data\n        else:\n            raise TypeError(\"Input data must be a string or an instance of Message.\")\n\n        if input_data != \"\":\n            # Add the human message to the conversation\n            conversation.add_message(human_message, sender_id=self.name)\n        \n        # Retrieve the conversation history and predict a response\n        messages = conversation.as_dict()\n\n        \n        if model_kwargs:\n            prediction = model.predict(messages=messages, **model_kwargs)\n        else:\n            prediction = model.predict(messages=messages)\n        # Create an AgentMessage instance with the model's response and update the conversation\n        if prediction != '':\n            agent_message = AgentMessage(prediction)\n            conversation.add_message(agent_message, sender_id=self.name)\n        \n        return prediction\n```"
    },
    {
        "document_name": "swarmauri/standard/agents/concrete/MultiPartyToolAgent.py",
        "content": "```swarmauri/standard/agents/concrete/MultiPartyToolAgent.py\nfrom typing import Any, Optional, Union, Dict\nimport json\n\nfrom swarmauri.core.models.IModel import IModel\nfrom swarmauri.core.toolkits.IToolkit import IToolkit\nfrom swarmauri.core.conversations.IConversation import IConversation\nfrom swarmauri.core.messages import IMessage\n\nfrom swarmauri.standard.agents.base.ToolAgentBase import ToolAgentBase\nfrom swarmauri.standard.agents.base.NamedAgentBase import NamedAgentBase\nfrom swarmauri.standard.messages.concrete import HumanMessage, AgentMessage, FunctionMessage\n\n\nclass MultiPartyToolAgent(ToolAgentBase, NamedAgentBase):\n    def __init__(self, \n                 model: IModel, \n                 conversation: IConversation, \n                 toolkit: IToolkit,\n                 name: str):\n        ToolAgentBase.__init__(self, model, conversation, toolkit)\n        NamedAgentBase.__init__(self, name)\n\n    def exec(self, input_data: Union[str, IMessage], model_kwargs: Optional[Dict] = {}) -> Any:\n        conversation = self.conversation\n        model = self.model\n        toolkit = self.toolkit\n        \n\n        # Check if the input is a string, then wrap it in a HumanMessage\n        if isinstance(input_data, str):\n            human_message = HumanMessage(input_data)\n        elif isinstance(input_data, IMessage):\n            human_message = input_data\n        else:\n            raise TypeError(\"Input data must be a string or an instance of Message.\")\n\n        if input_data != \"\":\n            # Add the human message to the conversation\n            conversation.add_message(human_message, sender_id=self.name)\n            \n        \n        # Retrieve the conversation history and predict a response\n        messages = conversation.as_dict()\n        \n\n        if model_kwargs:\n            prediction = model.predict(messages=messages, \n                                   tools=toolkit.tools, \n                                   tool_choice=\"auto\",\n                                   **model_kwargs)\n        else:\n            prediction = model.predict(messages=messages)\n        \n        \n        prediction_message = prediction.choices[0].message\n        agent_response = prediction_message.content\n        \n        agent_message = AgentMessage(content=prediction_message.content, \n                                     tool_calls=prediction_message.tool_calls)\n        conversation.add_message(agent_message, sender_id=self.name)\n        \n        tool_calls = prediction.choices[0].message.tool_calls\n        if tool_calls:\n        \n            for tool_call in tool_calls:\n                func_name = tool_call.function.name\n                \n                func_call = toolkit.get_tool_by_name(func_name)\n                func_args = json.loads(tool_call.function.arguments)\n                func_result = func_call(**func_args)\n                \n                func_message = FunctionMessage(func_result, \n                                               name=func_name, \n                                               tool_call_id=tool_call.id)\n                conversation.add_message(func_message, sender_id=self.name)\n            \n            \n            messages = conversation.as_dict()\n            rag_prediction = model.predict(messages=messages, \n                                           tools=toolkit.tools, \n                                           tool_choice=\"none\")\n            \n            prediction_message = rag_prediction.choices[0].message\n            \n            agent_response = prediction_message.content\n            if agent_response != \"\":\n                agent_message = AgentMessage(agent_response)\n                conversation.add_message(agent_message, sender_id=self.name)\n            prediction = rag_prediction\n            \n        return agent_response \n    \n```"
    },
    {
        "document_name": "swarmauri/standard/agents/concrete/RagAgent.py",
        "content": "```swarmauri/standard/agents/concrete/RagAgent.py\nfrom typing import Any, Optional, Union, Dict\nfrom swarmauri.core.messages import IMessage\nfrom swarmauri.core.models.IModel import IModel\nfrom swarmauri.standard.conversations.base.SystemContextBase import SystemContextBase\nfrom swarmauri.standard.agents.base.DocumentAgentBase import DocumentAgentBase\nfrom swarmauri.standard.document_stores.base.DocumentStoreRetrieveBase import DocumentStoreRetrieveBase\n\nfrom swarmauri.standard.messages.concrete import (HumanMessage, \n                                                  SystemMessage,\n                                                  AgentMessage)\n\nclass RagAgent(DocumentAgentBase):\n    \"\"\"\n    RagAgent (Retriever-And-Generator Agent) extends DocumentAgentBase,\n    specialized in retrieving documents based on input queries and generating responses.\n    \"\"\"\n\n    def __init__(self, name: str, model: IModel, conversation: SystemContextBase, document_store: DocumentStoreRetrieveBase):\n        super().__init__(name=name, model=model, conversation=conversation, document_store=document_store)\n\n    def exec(self, \n             input_data: Union[str, IMessage], \n             top_k: int = 5, \n             model_kwargs: Optional[Dict] = {}\n             ) -> Any:\n        conversation = self.conversation\n        model = self.model\n\n        # Check if the input is a string, then wrap it in a HumanMessage\n        if isinstance(input_data, str):\n            human_message = HumanMessage(input_data)\n        elif isinstance(input_data, IMessage):\n            human_message = input_data\n        else:\n            raise TypeError(\"Input data must be a string or an instance of Message.\")\n        \n        # Add the human message to the conversation\n        conversation.add_message(human_message)\n        \n        \n        \n        similar_documents = self.document_store.retrieve(query=input_data, top_k=top_k)\n        substr = '\\n'.join([doc.content for doc in similar_documents])\n        \n        # Use substr to set system context\n        system_context = SystemMessage(substr)\n        conversation.system_context = system_context\n        \n\n        # Retrieve the conversation history and predict a response\n        messages = conversation.as_dict()\n        if model_kwargs:\n            prediction = model.predict(messages=messages, **model_kwargs)\n        else:\n            prediction = model.predict(messages=messages)\n            \n        # Create an AgentMessage instance with the model's response and update the conversation\n        agent_message = AgentMessage(prediction)\n        conversation.add_message(agent_message)\n        \n        return prediction\n    \n    \n    \n\n```"
    },
    {
        "document_name": "swarmauri/standard/agents/concrete/GenerativeRagAgent.py",
        "content": "```swarmauri/standard/agents/concrete/GenerativeRagAgent.py\nfrom typing import Any, Optional, Union, Dict\nfrom swarmauri.core.messages import IMessage\nfrom swarmauri.core.models.IModel import IModel\nfrom swarmauri.standard.conversations.base.SystemContextBase import SystemContextBase\nfrom swarmauri.standard.agents.base.DocumentAgentBase import DocumentAgentBase\nfrom swarmauri.standard.document_stores.base.DocumentStoreRetrieveBase import DocumentStoreRetrieveBase\nfrom swarmauri.standard.documents.concrete.Document import Document\nfrom swarmauri.standard.chunkers.concrete.MdSnippetChunker import MdSnippetChunker\nfrom swarmauri.standard.messages.concrete import (HumanMessage, \n                                                  SystemMessage,\n                                                  AgentMessage)\n\nclass GenerativeRagAgent(DocumentAgentBase):\n    \"\"\"\n    RagAgent (Retriever-And-Generator Agent) extends DocumentAgentBase,\n    specialized in retrieving documents based on input queries and generating responses.\n    \"\"\"\n\n    def __init__(self, name: str, model: IModel, conversation: SystemContextBase, document_store: DocumentStoreRetrieveBase):\n        super().__init__(name=name, model=model, conversation=conversation, document_store=document_store)\n\n    def exec(self, \n             input_data: Union[str, IMessage], \n             top_k: int = 5, \n             model_kwargs: Optional[Dict] = {}\n             ) -> Any:\n        conversation = self.conversation\n        model = self.model\n\n        # Check if the input is a string, then wrap it in a HumanMessage\n        if isinstance(input_data, str):\n            human_message = HumanMessage(input_data)\n        elif isinstance(input_data, IMessage):\n            human_message = input_data\n        else:\n            raise TypeError(\"Input data must be a string or an instance of Message.\")\n        \n        # Add the human message to the conversation\n        conversation.add_message(human_message)\n        \n        \n        \n        similar_documents = self.document_store.retrieve(query=input_data, top_k=top_k)\n        substr = '\\n'.join([doc.content for doc in similar_documents])\n        \n        # Use substr to set system context\n        system_context = SystemMessage(substr)\n        conversation.system_context = system_context\n        \n\n        # Retrieve the conversation history and predict a response\n        messages = conversation.as_dict()\n        if model_kwargs:\n            prediction = model.predict(messages=messages, **model_kwargs)\n        else:\n            prediction = model.predict(messages=messages)\n            \n        # Create an AgentMessage instance with the model's response and update the conversation\n        agent_message = AgentMessage(prediction)\n        conversation.add_message(agent_message)\n        \n        chunker = MdSnippetChunker()\n        \n        new_documents = [Document(doc_id=self.document_store.document_count()+1,\n                                     content=each[2], \n                                     metadata={\"source\": \"RagSaverAgent\", \n                                               \"language\": each[1],\n                                               \"comments\": each[0]})\n                     for each in chunker.chunk_text(prediction)]\n\n        self.document_store.add_documents(new_documents)\n        \n        return prediction\n    \n    \n    \n\n```"
    },
    {
        "document_name": "swarmauri/standard/utils/__init__.py",
        "content": "```swarmauri/standard/utils/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/utils/load_documents_from_json.py",
        "content": "```swarmauri/standard/utils/load_documents_from_json.py\nimport json\nfrom typing import List\nfrom swarmauri.standard.documents.concrete.Document import Document\n\ndef load_documents_from_json(json_file_path):\n    documents = []\n    with open(json_file_path, 'r') as f:\n        data = json.load(f)\n    documents = [Document(doc_id=str(_), content=doc['content'], metadata={\"document_name\": doc['document_name']}) for _, doc in enumerate(data) if doc['content']]\n    return documents\n```"
    },
    {
        "document_name": "swarmauri/standard/conversations/__init__.py",
        "content": "```swarmauri/standard/conversations/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/conversations/base/__init__.py",
        "content": "```swarmauri/standard/conversations/base/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/conversations/base/ConversationBase.py",
        "content": "```swarmauri/standard/conversations/base/ConversationBase.py\nfrom abc import ABC\nfrom typing import List, Union\nfrom ....core.messages.IMessage import IMessage\nfrom ....core.conversations.IConversation import IConversation\n\nclass ConversationBase(IConversation, ABC):\n    \"\"\"\n    Concrete implementation of IConversation, managing conversation history and operations.\n    \"\"\"\n    \n    def __init__(self):\n        self._history: List[IMessage] = []\n\n    @property\n    def history(self) -> List[IMessage]:\n        \"\"\"\n        Provides read-only access to the conversation history.\n        \"\"\"\n        return self._history\n    \n    def add_message(self, message: IMessage):\n        self._history.append(message)\n\n    def get_last(self) -> Union[IMessage, None]:\n        if self._history:\n            return self._history[-1]\n        return None\n\n    def clear_history(self):\n        self._history.clear()\n\n    def as_dict(self) -> List[dict]:\n        return [message.as_dict() for message in self.history] # This must utilize the public self.history\n    \n    \n    # def __repr__(self):\n        # return repr([message.as_dict() for message in self._history])\n```"
    },
    {
        "document_name": "swarmauri/standard/conversations/base/SystemContextBase.py",
        "content": "```swarmauri/standard/conversations/base/SystemContextBase.py\nfrom abc import ABC\nfrom typing import Optional, Union\nfrom swarmauri.core.conversations.ISystemContext import ISystemContext\nfrom swarmauri.standard.messages.concrete.SystemMessage import SystemMessage\nfrom swarmauri.standard.conversations.base.ConversationBase import ConversationBase\n\nclass SystemContextBase(ConversationBase, ISystemContext, ABC):\n    def __init__(self, *args, system_message_content: Optional[SystemMessage] = None):\n        ConversationBase.__init__(self)\n        # Automatically handle both string and SystemMessage types for initializing system context\n        self._system_context = None  # Initialize with None\n        if system_message_content:\n            self.system_context = system_message_content\n    \n    @property\n    def system_context(self) -> Union[SystemMessage, None]:\n        \"\"\"Get the system context message. Raises an error if it's not set.\"\"\"\n        if self._system_context is None:\n            raise ValueError(\"System context has not been set.\")\n        return self._system_context\n    \n    @system_context.setter\n    def system_context(self, new_system_message: Union[SystemMessage, str]) -> None:\n        \"\"\"\n        Set a new system context message. The new system message can be a string or \n        an instance of SystemMessage. If it's a string, it converts it to a SystemMessage.\n        \"\"\"\n        if isinstance(new_system_message, SystemMessage):\n            self._system_context = new_system_message\n        elif isinstance(new_system_message, str):\n            self._system_context = SystemMessage(new_system_message)\n        else:\n            raise ValueError(\"System context must be a string or a SystemMessage instance.\")\n```"
    },
    {
        "document_name": "swarmauri/standard/conversations/concrete/__init__.py",
        "content": "```swarmauri/standard/conversations/concrete/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/conversations/concrete/LimitedSizeConversation.py",
        "content": "```swarmauri/standard/conversations/concrete/LimitedSizeConversation.py\nfrom ..base.ConversationBase import ConversationBase\nfrom ....core.messages.IMessage import IMessage\nfrom ....core.conversations.IMaxSize import IMaxSize\n\nclass LimitedSizeConversation(ConversationBase, IMaxSize):\n    def __init__(self, max_size: int):\n        super().__init__()\n        self._max_size = max_size\n        \n    @property\n    def max_size(self) -> int:\n        \"\"\"\n        Provides read-only access to the conversation history.\n        \"\"\"\n        return self._max_size\n    \n    @max_size.setter\n    def max_size(self, new_max_size: int) -> int:\n        \"\"\"\n        Provides read-only access to the conversation history.\n        \"\"\"\n        if new_max_size > 0:\n            self._max_size = int\n        else:\n            raise ValueError('Cannot set conversation size to 0.')\n\n\n    def add_message(self, message: IMessage):\n        \"\"\"Adds a message and ensures the conversation does not exceed the max size.\"\"\"\n        super().add_message(message)\n        self._enforce_max_size_limit()\n\n    def _enforce_max_size_limit(self):\n        \"\"\"\n        Enforces the maximum size limit of the conversation history.\n        If the current history size exceeds the maximum size, the oldest messages are removed.\n        \"\"\"\n        while len(self._history) > self.max_size:\n            self._history.pop(0)\n```"
    },
    {
        "document_name": "swarmauri/standard/conversations/concrete/SimpleConversation.py",
        "content": "```swarmauri/standard/conversations/concrete/SimpleConversation.py\nfrom typing import List, Union\nfrom ....core.messages.IMessage import IMessage\nfrom ..base.ConversationBase import ConversationBase\n\nclass SimpleConversation(ConversationBase):\n    \"\"\"\n    Concrete implementation of IConversation, managing conversation history and operations.\n    \"\"\"\n    \n    def __init__(self):\n       super().__init__()\n```"
    },
    {
        "document_name": "swarmauri/standard/conversations/concrete/LimitedSystemContextConversation.py",
        "content": "```swarmauri/standard/conversations/concrete/LimitedSystemContextConversation.py\nfrom typing import Optional, Union, List\nfrom swarmauri.core.messages.IMessage import IMessage\nfrom swarmauri.core.conversations.IMaxSize import IMaxSize\nfrom swarmauri.standard.conversations.base.SystemContextBase import SystemContextBase\nfrom swarmauri.standard.messages.concrete.SystemMessage import SystemMessage\n\nclass LimitedSystemContextConversation(SystemContextBase, IMaxSize):\n    def __init__(self, max_size: int, system_message_content: Optional[SystemMessage] = None):\n        \"\"\"\n        Initializes the conversation with a system context message and a maximum history size.\n        \n        Parameters:\n            max_size (int): The maximum number of messages allowed in the conversation history.\n            system_message_content (Optional[str], optional): The initial system message content. Can be a string.\n        \"\"\"\n        SystemContextBase.__init__(self, system_message_content=system_message_content if system_message_content else \"\")  # Initialize SystemContext with a SystemMessage\n        self._max_size = max_size  # Set the maximum size\n    \n    @property\n    def history(self) -> List[IMessage]:\n        \"\"\"\n        Provides read-only access to the conversation history.\n        \"\"\"\n        \n        \n        res = [] \n        res.append(self.system_context)\n        res.extend(self._history)\n        return res\n        \n        \n    @property\n    def max_size(self) -> int:\n        \"\"\"\n        Provides access to the max_size property.\n        \"\"\"\n        return self._max_size\n    \n    @max_size.setter\n    def max_size(self, new_max_size: int) -> None:\n        \"\"\"\n        Sets a new maximum size for the conversation history.\n        \"\"\"\n        if new_max_size <= 0:\n            raise ValueError(\"max_size must be greater than 0.\")\n        self._max_size = new_max_size\n\n    def add_message(self, message: IMessage):\n        \"\"\"\n        Adds a message to the conversation history and ensures history does not exceed the max size.\n        \"\"\"\n        if isinstance(message, SystemMessage):\n            raise ValueError(f\"System context cannot be set through this method on {self.__class_name__}.\")\n        else:\n            super().add_message(message)\n        self._enforce_max_size_limit()\n        \n    def _enforce_max_size_limit(self):\n        \"\"\"\n        Remove messages from the beginning of the conversation history if the limit is exceeded.\n        \"\"\"\n        while len(self._history) + 1 > self._max_size:\n            self._history.pop(0)\n\n    @property\n    def system_context(self) -> Union[SystemMessage, None]:\n        \"\"\"Get the system context message. Raises an error if it's not set.\"\"\"\n        if self._system_context is None:\n            raise ValueError(\"System context has not been set.\")\n        return self._system_context\n\n\n    @system_context.setter\n    def system_context(self, new_system_message: Union[SystemMessage, str]) -> None:\n        \"\"\"\n        Set a new system context message. The new system message can be a string or \n        an instance of SystemMessage. If it's a string, it converts it to a SystemMessage.\n        \"\"\"\n        if isinstance(new_system_message, SystemMessage):\n            self._system_context = new_system_message\n        elif isinstance(new_system_message, str):\n            self._system_context = SystemMessage(new_system_message)\n        else:\n            raise ValueError(\"System context must be a string or a SystemMessage instance.\")\n            \n```"
    },
    {
        "document_name": "swarmauri/standard/conversations/concrete/SharedConversation.py",
        "content": "```swarmauri/standard/conversations/concrete/SharedConversation.py\nimport inspect\nfrom threading import Lock\nfrom typing import Optional, Dict, List, Tuple\nfrom swarmauri.core.messages.IMessage import IMessage\nfrom swarmauri.standard.conversations.base.ConversationBase import ConversationBase\nfrom swarmauri.standard.messages.concrete.HumanMessage import HumanMessage\nfrom swarmauri.standard.messages.concrete.SystemMessage import SystemMessage\n\nclass SharedConversation(ConversationBase):\n    \"\"\"\n    A thread-safe conversation class that supports individual system contexts for each SwarmAgent.\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        self._lock = Lock()  # A lock to ensure thread safety\n        self._agent_system_contexts: Dict[str, SystemMessage] = {}  # Store system contexts for each agent\n        self._history: List[Tuple[str, IMessage]] = []  # Stores tuples of (sender_id, IMessage)\n\n\n    @property\n    def history(self):\n        history = []\n        for each in self._history:\n            history.append((each[0], each[1]))\n        return history\n\n    def add_message(self, message: IMessage, sender_id: str):\n        with self._lock:\n            self._history.append((sender_id, message))\n\n    def reset_messages(self) -> None:\n        self._history = []\n        \n\n    def _get_caller_name(self) -> Optional[str]:\n        for frame_info in inspect.stack():\n            # Check each frame for an instance with a 'name' attribute in its local variables\n            local_variables = frame_info.frame.f_locals\n            for var_name, var_value in local_variables.items():\n                if hasattr(var_value, 'name'):\n                    # Found an instance with a 'name' attribute. Return its value.\n                    return getattr(var_value, 'name')\n        # No suitable caller found\n        return None\n\n    def as_dict(self) -> List[Dict]:\n        caller_name = self._get_caller_name()\n        history = []\n\n        with self._lock:\n            # If Caller is not one of the agents, then give history\n            if caller_name not in self._agent_system_contexts.keys():\n                for sender_id, message in self._history:\n                    history.append((sender_id, message.as_dict()))\n                \n                \n            else:\n                system_context = self.get_system_context(caller_name)\n                #print(caller_name, system_context, type(system_context))\n                if type(system_context) == str:\n                    history.append(SystemMessage(system_context).as_dict())\n                else:\n                    history.append(system_context.as_dict())\n                    \n                for sender_id, message in self._history:\n                    #print(caller_name, sender_id, message, type(message))\n                    if sender_id == caller_name:\n                        if message.__class__.__name__ == 'AgentMessage' or 'FunctionMessage':\n                            # The caller is the sender; treat as AgentMessage\n                            history.append(message.as_dict())\n                            \n                            # Print to see content that is empty.\n                            #if not message.content:\n                                #print('\\n\\t\\t\\t=>', message, message.content)\n                    else:\n                        if message.content:\n                            # The caller is not the sender; treat as HumanMessage\n                            history.append(HumanMessage(message.content).as_dict())\n        return history\n    \n    def get_last(self) -> IMessage:\n        with self._lock:\n            return super().get_last()\n\n\n    def clear_history(self):\n        with self._lock:\n            super().clear_history()\n\n\n        \n\n    def set_system_context(self, agent_id: str, context: SystemMessage):\n        \"\"\"\n        Sets the system context for a specific agent.\n\n        Args:\n            agent_id (str): Unique identifier for the agent.\n            context (SystemMessage): The context message to be set for the agent.\n        \"\"\"\n        with self._lock:\n            self._agent_system_contexts[agent_id] = context\n\n    def get_system_context(self, agent_id: str) -> Optional[SystemMessage]:\n        \"\"\"\n        Retrieves the system context for a specific agent.\n\n        Args:\n            agent_id (str): Unique identifier for the agent.\n\n        Returns:\n            Optional[SystemMessage]: The context message of the agent, or None if not found.\n        \"\"\"\n        return self._agent_system_contexts.get(agent_id, None)\n```"
    },
    {
        "document_name": "swarmauri/standard/documents/__init__.py",
        "content": "```swarmauri/standard/documents/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/documents/base/__init__.py",
        "content": "```swarmauri/standard/documents/base/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/documents/base/EmbeddedBase.py",
        "content": "```swarmauri/standard/documents/base/EmbeddedBase.py\nfrom abc import ABC\nfrom typing import List, Any\nfrom swarmauri.core.documents.IEmbed import IEmbed\nfrom swarmauri.core.vectors.IVector import IVector\n\nclass EmbeddedBase(IEmbed, ABC):\n    def __init__(self, embedding):\n        self._embedding = embedding\n            \n    @property\n    def embedding(self) -> IVector:\n        return self._embedding\n\n    @embedding.setter\n    def embedding(self, value: IVector) -> None:\n        self._embedding = value\n```"
    },
    {
        "document_name": "swarmauri/standard/documents/base/DocumentBase.py",
        "content": "```swarmauri/standard/documents/base/DocumentBase.py\nfrom abc import ABC, abstractmethod\nfrom typing import Dict\nfrom swarmauri.core.documents.IDocument import IDocument\n\nclass DocumentBase(IDocument, ABC):\n    \n    def __init__(self, doc_id,  content, metadata):\n        self._id = doc_id\n        self._content = content\n        self._metadata = metadata        \n    \n    def __str__(self):\n        return f\"Document ID: {self.id}, Content: {self.content}, Metadata: {self.metadata}\"\n\n    def __repr__(self):\n        return f\"Document(id={self.id}, content={self.content}, metadata={self.metadata})\"\n\n    def as_dict(self):\n        return self.__dict__\n    \n    @property\n    def id(self) -> str:\n        \"\"\"\n        Get the document's ID.\n        \"\"\"\n        return self._id\n\n    @id.setter\n    def id(self, value: str) -> None:\n        \"\"\"\n        Set the document's ID.\n        \"\"\"\n        self._id = value\n\n    @property\n    def content(self) -> str:\n        \"\"\"\n        Get the document's content.\n        \"\"\"\n        return self._content\n\n    @content.setter\n    def content(self, value: str) -> None:\n        \"\"\"\n        Set the document's content.\n        \"\"\"\n        if value:\n            self._content = value\n        else:\n            raise ValueError('Cannot create a document with no content.')\n\n    @property\n    def metadata(self) -> Dict:\n        \"\"\"\n        Get the document's metadata.\n        \"\"\"\n        return self._metadata\n\n    @metadata.setter\n    def metadata(self, value: Dict) -> None:\n        \"\"\"\n        Set the document's metadata.\n        \"\"\"\n        self._metadata = value\n```"
    },
    {
        "document_name": "swarmauri/standard/documents/concrete/__init__.py",
        "content": "```swarmauri/standard/documents/concrete/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/documents/concrete/EmbeddedDocument.py",
        "content": "```swarmauri/standard/documents/concrete/EmbeddedDocument.py\nfrom typing import Optional, Any\nfrom swarmauri.standard.documents.base.DocumentBase import DocumentBase\nfrom swarmauri.standard.documents.base.EmbeddedBase import EmbeddedBase\n\nclass EmbeddedDocument(DocumentBase, EmbeddedBase):\n    def __init__(self, doc_id,  content, metadata, embedding: Optional[Any] = None):\n        DocumentBase.__init__(self, doc_id=doc_id, content=content, metadata=metadata)\n        EmbeddedBase.__init__(self, embedding=embedding)\n\n\n```"
    },
    {
        "document_name": "swarmauri/standard/documents/concrete/Document.py",
        "content": "```swarmauri/standard/documents/concrete/Document.py\nfrom swarmauri.standard.documents.base.DocumentBase import DocumentBase\n\nclass Document(DocumentBase):\n    pass\n    \n        \n\n```"
    },
    {
        "document_name": "swarmauri/standard/messages/__init__.py",
        "content": "```swarmauri/standard/messages/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/messages/base/__init__.py",
        "content": "```swarmauri/standard/messages/base/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/messages/base/MessageBase.py",
        "content": "```swarmauri/standard/messages/base/MessageBase.py\nfrom abc import ABC, abstractmethod\nfrom swarmauri.core.messages.IMessage import IMessage\n\nclass MessageBase(IMessage, ABC):\n    \n    @abstractmethod\n    def __init__(self, role: str, content: str):\n        self._role = role\n        self._content = content\n    \n    @property\n    def role(self) -> str:\n        return self._role\n    \n    @property\n    def content(self) -> str:\n        return self._content\n\n    \n    def as_dict(self) -> dict:\n        \"\"\"\n        Dynamically return a dictionary representation of the object,\n        including all properties.\n        \"\"\"\n        result_dict = {}\n        # Iterate over all attributes\n        for attr in dir(self):\n            # Skip private attributes and anything not considered a property\n            if attr.startswith(\"_\") or callable(getattr(self, attr)):\n                continue\n            result_dict[attr] = getattr(self, attr)\n            \n        return result_dict\n\n    def __repr__(self) -> str:\n        \"\"\"\n        Return the string representation of the ConcreteMessage.\n        \"\"\"\n        return f\"{self.__class__.__name__}(role='{self.role}')\"\n    \n    def __getattr__(self, name):\n        \"\"\"\n        Return the value of the named attribute of the instance.\n        \"\"\"\n        try:\n            return self.__dict__[name]\n        except KeyError:\n            raise AttributeError(f\"'{type(self).__name__}' object has no attribute '{name}'\")\n\n    \n    def __setattr__(self, name, value):\n        \"\"\"\n        Set the value of the named attribute of the instance.\n        \"\"\"\n        self.__dict__[name] = value\n```"
    },
    {
        "document_name": "swarmauri/standard/messages/concrete/__init__.py",
        "content": "```swarmauri/standard/messages/concrete/__init__.py\nfrom .HumanMessage import HumanMessage\nfrom .AgentMessage import AgentMessage\nfrom .FunctionMessage import FunctionMessage\nfrom .SystemMessage import SystemMessage\n```"
    },
    {
        "document_name": "swarmauri/standard/messages/concrete/AgentMessage.py",
        "content": "```swarmauri/standard/messages/concrete/AgentMessage.py\nfrom typing import Optional, Any\nfrom swarmauri.standard.messages.base.MessageBase import MessageBase\n\n\nclass AgentMessage(MessageBase):\n    def __init__(self, content, tool_calls: Optional[Any] = None):\n        super().__init__(role='assistant', content=content)\n        if tool_calls:\n            self.tool_calls = tool_calls\n```"
    },
    {
        "document_name": "swarmauri/standard/messages/concrete/HumanMessage.py",
        "content": "```swarmauri/standard/messages/concrete/HumanMessage.py\nfrom swarmauri.standard.messages.base.MessageBase import MessageBase\n\nclass HumanMessage(MessageBase):\n    \"\"\"\n    Represents a message created by a human user.\n\n    Extends the `Message` class to specifically represent messages input by human users in a conversational\n    interface. It contains the message content and assigns the type \"HumanMessage\" to distinguish it from\n    other types of messages.\n    \"\"\"\n\n    def __init__(self, content, name=None):\n        \"\"\"\n        Initializes a new instance of HumanMessage with specified content.\n\n        Args:\n            content (str): The text content of the human-created message.\n            name (str, optional): The name of the human sender.\n        \"\"\"\n        super().__init__(role='user', content=content)\n\n\n```"
    },
    {
        "document_name": "swarmauri/standard/messages/concrete/FunctionMessage.py",
        "content": "```swarmauri/standard/messages/concrete/FunctionMessage.py\nfrom swarmauri.standard.messages.base.MessageBase import MessageBase\n\n\nclass FunctionMessage(MessageBase):\n    \"\"\"\n    Represents a message created by a human user.\n\n    This class extends the `Message` class to specifically represent messages that\n    are input by human users in a conversational interface. It contains the message\n    content and assigns the type \"HumanMessage\" to distinguish it from other types\n    of messages.\n\n    Attributes:\n        content (str): The text content of the message.\n\n    Methods:\n        display: Returns a dictionary representation of the message for display,\n                 tagging it with the role \"user\".\n    \"\"\"\n\n    def __init__(self, content, name, tool_call_id):\n        super().__init__(role='tool', content=content)\n        self.name = name\n        self.tool_call_id = tool_call_id\n    \n```"
    },
    {
        "document_name": "swarmauri/standard/messages/concrete/SystemMessage.py",
        "content": "```swarmauri/standard/messages/concrete/SystemMessage.py\nfrom swarmauri.standard.messages.base.MessageBase import MessageBase\n\nclass SystemMessage(MessageBase):\n    \"\"\"\n    SystemMessage class represents a message generated by the system. \n    \n    This type of message is used to communicate system-level information such as \n    errors, notifications, or updates to the user. Inherits from the Message base class.\n    \n    Attributes:\n        content (str): The content of the system message.\n    \"\"\"\n    \n    def __init__(self, content):\n        super().__init__(role='system', content=content)\n    \n\n```"
    },
    {
        "document_name": "swarmauri/standard/parsers/__init__.py",
        "content": "```swarmauri/standard/parsers/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/parsers/base/__init__.py",
        "content": "```swarmauri/standard/parsers/base/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/parsers/concrete/__init__.py",
        "content": "```swarmauri/standard/parsers/concrete/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/parsers/concrete/CSVParser.py",
        "content": "```swarmauri/standard/parsers/concrete/CSVParser.py\nimport csv\nfrom io import StringIO\nfrom typing import List, Union, Any\nfrom ....core.parsers.IParser import IParser\nfrom ....core.documents.IDocument import IDocument\nfrom ....standard.documents.concrete.Document import Document\n\nclass CSVParser(IParser):\n    \"\"\"\n    Concrete implementation of IParser for parsing CSV formatted text into Document instances.\n\n    The parser can handle input as a CSV formatted string or from a file, with each row\n    represented as a separate Document. Assumes the first row is the header which will\n    be used as keys for document metadata.\n    \"\"\"\n\n    def parse(self, data: Union[str, Any]) -> List[IDocument]:\n        \"\"\"\n        Parses the given CSV data into a list of Document instances.\n\n        Parameters:\n        - data (Union[str, Any]): The input data to parse, either as a CSV string or file path.\n\n        Returns:\n        - List[IDocument]: A list of documents parsed from the CSV data.\n        \"\"\"\n        # Prepare an in-memory string buffer if the data is provided as a string\n        if isinstance(data, str):\n            data_stream = StringIO(data)\n        else:\n            raise ValueError(\"Data provided is not a valid CSV string\")\n\n        # Create a list to hold the parsed documents\n        documents: List[IDocument] = []\n\n        # Read CSV content row by row\n        reader = csv.DictReader(data_stream)\n        for row in reader:\n            # Each row represents a document, where the column headers are metadata fields\n            document = Document(doc_id=row.get('id', None), \n                                        content=row.get('content', ''), \n                                        metadata=row)\n            documents.append(document)\n\n        return documents\n```"
    },
    {
        "document_name": "swarmauri/standard/parsers/concrete/EntityRecognitionParser.py",
        "content": "```swarmauri/standard/parsers/concrete/EntityRecognitionParser.py\nimport spacy\nfrom typing import List, Union, Any\nfrom ....core.parsers.IParser import IParser\nfrom ....core.documents.IDocument import IDocument\nfrom ....standard.documents.concrete.Document import Document\n\nclass EntityRecognitionParser(IParser):\n    \"\"\"\n    EntityRecognitionParser leverages NER capabilities to parse text and \n    extract entities with their respective tags such as PERSON, LOCATION, ORGANIZATION, etc.\n    \"\"\"\n\n    def __init__(self):\n        # Load a SpaCy model. The small model is used for demonstration; larger models provide improved accuracy.\n        self.nlp = spacy.load(\"en_core_web_sm\")\n    \n    def parse(self, text: Union[str, Any]) -> List[IDocument]:\n        \"\"\"\n        Parses the input text, identifies entities, and returns a list of documents with entities tagged.\n\n        Parameters:\n        - text (Union[str, Any]): The input text to be parsed and analyzed for entities.\n\n        Returns:\n        - List[IDocument]: A list of IDocument instances representing the identified entities in the text.\n        \"\"\"\n        # Ensure the input is a string type before processing\n        if not isinstance(text, str):\n            text = str(text)\n        \n        # Apply the NER model\n        doc = self.nlp(text)\n\n        # Compile identified entities into documents\n        entities_docs = []\n        for ent in doc.ents:\n            # Create a document for each entity with metadata carrying entity type\n            entity_doc = Document(doc_id=ent.text, content=ent.text, metadata={\"entity_type\": ent.label_})\n            entities_docs.append(entity_doc)\n        \n        return entities_docs\n```"
    },
    {
        "document_name": "swarmauri/standard/parsers/concrete/HtmlTagStripParser.py",
        "content": "```swarmauri/standard/parsers/concrete/HtmlTagStripParser.py\nfrom ....core.parsers.IParser import IParser\nfrom ....core.documents.IDocument import IDocument\nfrom ....standard.documents.concrete.Document import Document\nimport html\nimport re\n\nclass HTMLTagStripParser(IParser):\n    \"\"\"\n    A concrete parser that removes HTML tags and unescapes HTML content,\n    leaving plain text.\n    \"\"\"\n\n    def parse(self, data):\n        \"\"\"\n        Strips HTML tags from input data and unescapes HTML content.\n        \n        Args:\n            data (str): The HTML content to be parsed.\n        \n        Returns:\n            List[IDocument]: A list containing a single IDocument instance of the stripped text.\n        \"\"\"\n\n        # Ensure that input is a string\n        if not isinstance(data, str):\n            raise ValueError(\"HTMLTagStripParser expects input data to be of type str.\")\n        \n        # Remove HTML tags\n        text = re.sub('<[^<]+?>', '', data)  # Matches anything in < > and replaces it with empty string\n        \n        # Unescape HTML entities\n        text = html.unescape(text)\n\n        # Wrap the cleaned text into a Document and return it in a list\n        document = Document(doc_id=\"1\", content=text, metadata={\"original_length\": len(data)})\n        \n        return [document]\n```"
    },
    {
        "document_name": "swarmauri/standard/parsers/concrete/KeywordExtractorParser.py",
        "content": "```swarmauri/standard/parsers/concrete/KeywordExtractorParser.py\nimport yake\nfrom typing import List, Union, Any\nfrom ....core.parsers.IParser import IParser\nfrom ....core.documents.IDocument import IDocument\nfrom ....standard.documents.concrete.Document import Document\n\nclass KeywordExtractorParser(IParser):\n    \"\"\"\n    Extracts keywords from text using the YAKE keyword extraction library.\n    \"\"\"\n\n    def __init__(self, lang: str = 'en', num_keywords: int = 10):\n        \"\"\"\n        Initialize the keyword extractor with specified language and number of keywords.\n\n        Parameters:\n        - lang (str): The language of the text for keyword extraction. Default is 'en' for English.\n        - num_keywords (int): The number of top keywords to extract. Default is 10.\n        \"\"\"\n        self.lang = lang\n        self.num_keywords = num_keywords\n        # Initialize YAKE extractor with specified parameters\n        self.kw_extractor = yake.KeywordExtractor(lan=lang, n=3, dedupLim=0.9, dedupFunc='seqm', windowsSize=1, top=num_keywords, features=None)\n\n    def parse(self, data: Union[str, Any]) -> List[IDocument]:\n        \"\"\"\n        Extract keywords from input text and return as list of IDocument instances containing keyword information.\n\n        Parameters:\n        - data (Union[str, Any]): The input text from which to extract keywords.\n\n        Returns:\n        - List[IDocument]: A list of IDocument instances, each containing information about an extracted keyword.\n        \"\"\"\n        # Ensure data is in string format for analysis\n        text = str(data) if not isinstance(data, str) else data\n\n        # Extract keywords using YAKE\n        keywords = self.kw_extractor.extract_keywords(text)\n\n        # Create Document instances for each keyword\n        documents = [Document(doc_id=str(index), content=keyword, metadata={\"score\": score}) for index, (keyword, score) in enumerate(keywords)]\n        \n        return documents\n```"
    },
    {
        "document_name": "swarmauri/standard/parsers/concrete/MarkdownParser.py",
        "content": "```swarmauri/standard/parsers/concrete/MarkdownParser.py\nimport re\nfrom markdown import markdown\nfrom bs4 import BeautifulSoup\nfrom ....core.parsers.IParser import IParser\nfrom ....core.documents.IDocument import IDocument\nfrom ....standard.documents.concrete.Document import Document\n\nclass MarkdownParser(IParser):\n    \"\"\"\n    A concrete implementation of the IParser interface that parses Markdown text.\n    \n    This parser takes Markdown formatted text, converts it to HTML using Python's\n    markdown library, and then uses BeautifulSoup to extract plain text content. The\n    resulting plain text is then wrapped into IDocument instances.\n    \"\"\"\n    \n    def parse(self, data: str) -> list[IDocument]:\n        \"\"\"\n        Parses the input Markdown data into a list of IDocument instances.\n        \n        Parameters:\n        - data (str): The input Markdown formatted text to be parsed.\n        \n        Returns:\n        - list[IDocument]: A list containing a single IDocument instance with the parsed text content.\n        \"\"\"\n        # Convert Markdown to HTML\n        html_content = markdown(data)\n        \n        # Use BeautifulSoup to extract text content from HTML\n        soup = BeautifulSoup(html_content, features=\"html.parser\")\n        plain_text = soup.get_text(separator=\" \", strip=True)\n        \n        # Generate a document ID\n        doc_id = \"1\"  # For this implementation, a simple fixed ID is used. \n                      # A more complex system might generate unique IDs for each piece of text.\n\n        # Create and return a list containing the single extracted plain text document\n        document = Document(doc_id=doc_id, content=plain_text, metadata={\"source_format\": \"markdown\"})\n        return [document]\n```"
    },
    {
        "document_name": "swarmauri/standard/parsers/concrete/OpenAPISpecParser.py",
        "content": "```swarmauri/standard/parsers/concrete/OpenAPISpecParser.py\nfrom typing import List, Union, Any\nimport yaml\nfrom ....core.parsers.IParser import IParser\nfrom ....core.documents.IDocument import IDocument\nfrom ....standard.documents.concrete.Document import Document\n\nclass OpenAPISpecParser(IParser):\n    \"\"\"\n    A parser that processes OpenAPI Specification files (YAML or JSON format)\n    and extracts information into structured Document instances. \n    This is useful for building documentation, APIs inventory, or analyzing the API specification.\n    \"\"\"\n\n    def parse(self, data: Union[str, Any]) -> List[IDocument]:\n        \"\"\"\n        Parses an OpenAPI Specification from a YAML or JSON string into a list of Document instances.\n\n        Parameters:\n        - data (Union[str, Any]): The OpenAPI specification in YAML or JSON format as a string.\n\n        Returns:\n        - List[IDocument]: A list of Document instances representing the parsed information.\n        \"\"\"\n        try:\n            # Load the OpenAPI spec into a Python dictionary\n            spec_dict = yaml.safe_load(data)\n        except yaml.YAMLError as e:\n            raise ValueError(f\"Failed to parse the OpenAPI specification: {e}\")\n        \n        documents = []\n        # Iterate over paths in the OpenAPI spec to extract endpoint information\n        for path, path_item in spec_dict.get(\"paths\", {}).items():\n            for method, operation in path_item.items():\n                # Create a Document instance for each operation\n                doc_id = f\"{path}_{method}\"\n                content = yaml.dump(operation)\n                metadata = {\n                    \"path\": path,\n                    \"method\": method.upper(),\n                    \"operationId\": operation.get(\"operationId\", \"\")\n                }\n                document = Document(doc_id=doc_id, content=content, metadata=metadata)\n                documents.append(document)\n\n        return documents\n```"
    },
    {
        "document_name": "swarmauri/standard/parsers/concrete/PhoneNumberExtractorParser.py",
        "content": "```swarmauri/standard/parsers/concrete/PhoneNumberExtractorParser.py\nimport re\nfrom typing import List, Union, Any\nfrom ....core.parsers.IParser import IParser\nfrom ....core.documents.IDocument import IDocument\nfrom ....standard.documents.concrete.Document import Document\n\nclass PhoneNumberExtractorParser(IParser):\n    \"\"\"\n    A parser that extracts phone numbers from the input text.\n    Utilizes regular expressions to identify phone numbers in various formats.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initializes the PhoneNumberExtractorParser.\n        \"\"\"\n        super().__init__()\n\n    def parse(self, data: Union[str, Any]) -> List[IDocument]:\n        \"\"\"\n        Parses the input data, looking for phone numbers employing a regular expression.\n        Each phone number found is contained in a separate IDocument instance.\n\n        Parameters:\n        - data (Union[str, Any]): The input text to be parsed for phone numbers.\n\n        Returns:\n        - List[IDocument]: A list of IDocument instances, each containing a phone number.\n        \"\"\"\n        # Define a regular expression for phone numbers.\n        # This is a simple example and might not capture all phone number formats accurately.\n        phone_regex = r'\\+?\\d[\\d -]{8,}\\d'\n\n        # Find all occurrences of phone numbers in the text\n        phone_numbers = re.findall(phone_regex, str(data))\n\n        # Create a new IDocument for each phone number found\n        documents = [Document(doc_id=str(index), content=phone_number, metadata={}) for index, phone_number in enumerate(phone_numbers)]\n\n        return documents\n```"
    },
    {
        "document_name": "swarmauri/standard/parsers/concrete/PythonParser.py",
        "content": "```swarmauri/standard/parsers/concrete/PythonParser.py\nfrom typing import List, Union, Any\nfrom ....core.parsers.IParser import IParser\nfrom ....core.documents.IDocument import IDocument\nfrom ....standard.documents.concrete.Document import Document\nimport ast\nimport uuid\n\nclass PythonParser(IParser):\n    \"\"\"\n    A parser that processes Python source code to extract structural elements\n    such as functions, classes, and their docstrings.\n    \n    This parser utilizes the `ast` module to parse the Python code into an abstract syntax tree (AST)\n    and then walks the tree to extract relevant information.\n    \"\"\"\n    \n    def parse(self, data: Union[str, Any]) -> List[IDocument]:\n        \"\"\"\n        Parses the given Python source code to extract structural elements.\n\n        Args:\n            data (Union[str, Any]): The input Python source code as a string.\n\n        Returns:\n            List[IDocument]: A list of IDocument objects, each representing a structural element \n                             extracted from the code along with its metadata.\n        \"\"\"\n        if not isinstance(data, str):\n            raise ValueError(\"PythonParser expects a string input.\")\n        \n        documents = []\n        tree = ast.parse(data)\n        \n        for node in ast.walk(tree):\n            if isinstance(node, ast.FunctionDef) or isinstance(node, ast.ClassDef):\n                element_name = node.name\n                docstring = ast.get_docstring(node)\n                \n                # Generate a unique ID for each element\n                doc_id = str(uuid.uuid4())\n                \n                # Create a metadata dictionary\n                metadata = {\n                    \"type\": \"function\" if isinstance(node, ast.FunctionDef) else \"class\",\n                    \"name\": element_name,\n                    \"docstring\": docstring\n                }\n                \n                # Create a Document for each structural element\n                document = Document(doc_id=doc_id, content=docstring, metadata=metadata)\n                documents.append(document)\n                \n        return documents\n```"
    },
    {
        "document_name": "swarmauri/standard/parsers/concrete/RegExParser.py",
        "content": "```swarmauri/standard/parsers/concrete/RegExParser.py\nimport re\nfrom typing import List, Union, Any\nfrom ....core.parsers.IParser import IParser\nfrom ....core.documents.IDocument import IDocument\nfrom ....standard.documents.concrete.Document import Document\n\nclass RegExParser(IParser):\n    \"\"\"\n    A parser that uses a regular expression to extract information from text.\n    \"\"\"\n\n    def __init__(self, pattern: str):\n        \"\"\"\n        Initializes the RegExParser with a specific regular expression pattern.\n\n        Parameters:\n        - pattern (str): The regular expression pattern used for parsing the text.\n        \"\"\"\n        self.pattern = pattern\n\n    def parse(self, data: Union[str, Any]) -> List[IDocument]:\n        \"\"\"\n        Parses the input data using the specified regular expression pattern and\n        returns a list of IDocument instances containing the extracted information.\n\n        Parameters:\n        - data (Union[str, Any]): The input data to be parsed. It can be a string or any format \n                                   that the concrete implementation can handle.\n\n        Returns:\n        - List[IDocument]: A list of IDocument instances containing the parsed information.\n        \"\"\"\n        # Ensure data is a string\n        if not isinstance(data, str):\n            data = str(data)\n\n        # Use the regular expression pattern to find all matches in the text\n        matches = re.findall(self.pattern, data)\n\n        # Create a Document for each match and collect them into a list\n        documents = [Document(doc_id=str(i+1), content=match, metadata={}) for i, match in enumerate(matches)]\n\n        return documents\n```"
    },
    {
        "document_name": "swarmauri/standard/parsers/concrete/TextBlobNounParser.py",
        "content": "```swarmauri/standard/parsers/concrete/TextBlobNounParser.py\nfrom typing import List, Union, Any\nfrom textblob import TextBlob\nfrom ....core.parsers.IParser import IParser\nfrom ....core.documents.IDocument import IDocument\nfrom ....standard.documents.concrete.Document import Document\n\nclass TextBlobNounParser(IParser):\n    \"\"\"\n    A concrete implementation of IParser using TextBlob for Natural Language Processing tasks.\n    \n    This parser leverages TextBlob's functionalities such as noun phrase extraction, \n    sentiment analysis, classification, language translation, and more for parsing texts.\n    \"\"\"\n    \n    def parse(self, data: Union[str, Any]) -> List[IDocument]:\n        \"\"\"\n        Parses the input data using TextBlob to perform basic NLP tasks \n        and returns a list of documents with the parsed information.\n        \n        Parameters:\n        - data (Union[str, Any]): The input data to parse, expected to be text data for this parser.\n        \n        Returns:\n        - List[IDocument]: A list of documents with metadata generated from the parsing process.\n        \"\"\"\n        # Ensure the data is a string\n        if not isinstance(data, str):\n            raise ValueError(\"TextBlobParser expects a string as input data.\")\n        \n        # Use TextBlob for NLP tasks\n        blob = TextBlob(data)\n        \n        # Extracts noun phrases to demonstrate one of TextBlob's capabilities. \n        # In practice, this parser could be expanded to include more sophisticated processing.\n        noun_phrases = list(blob.noun_phrases)\n        \n        # Example: Wrap the extracted noun phrases into an IDocument instance\n        # In real scenarios, you might want to include more details, like sentiment, POS tags, etc.\n        document = Document(doc_id=\"0\", content=data, metadata={\"noun_phrases\": noun_phrases})\n        \n        return [document]\n```"
    },
    {
        "document_name": "swarmauri/standard/parsers/concrete/TextBlobSentenceParser.py",
        "content": "```swarmauri/standard/parsers/concrete/TextBlobSentenceParser.py\nfrom textblob import TextBlob\nfrom ....core.parsers.IParser import IParser\nfrom ....core.documents.IDocument import IDocument\nfrom ....standard.documents.concrete.Document import Document\nfrom typing import List, Union, Any\n\nclass TextBlobParser(IParser):\n    \"\"\"\n    A parser that leverages TextBlob to break text into sentences.\n\n    This parser uses the natural language processing capabilities of TextBlob\n    to accurately identify sentence boundaries within large blocks of text.\n    \"\"\"\n\n    def parse(self, data: Union[str, Any]) -> List[IDocument]:\n        \"\"\"\n        Parses the input text into sentence-based document chunks using TextBlob.\n\n        Args:\n            data (Union[str, Any]): The input text to be parsed.\n\n        Returns:\n            List[IDocument]: A list of IDocument instances, each representing a sentence.\n        \"\"\"\n        # Ensure the input is a string\n        if not isinstance(data, str):\n            data = str(data)\n\n        # Utilize TextBlob for sentence tokenization\n        blob = TextBlob(data)\n        sentences = blob.sentences\n\n        # Create a document instance for each sentence\n        documents = [\n            Document(doc_id=str(index), content=str(sentence), metadata={'parser': 'TextBlobParser'})\n            for index, sentence in enumerate(sentences)\n        ]\n\n        return documents\n```"
    },
    {
        "document_name": "swarmauri/standard/parsers/concrete/TFIDFParser.py",
        "content": "```swarmauri/standard/parsers/concrete/TFIDFParser.py\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom swarmauri.core.parsers.IParser import IParser\nfrom swarmauri.core.documents.IDocument import IDocument\nfrom swarmauri.standard.documents.concrete.Document import Document\n\nclass TFIDFParser(IParser):\n    def __init__(self):\n        self.vectorizer = TfidfVectorizer()\n        super().__init__()\n\n    def parse(self, data):\n        # Assuming `data` is a list of strings (documents)\n        tfidf_matrix = self.vectorizer.fit_transform(data)\n        # Depending on how you want to use the output, you could return Document objects\n        # For demonstration, let's return a list of IDocument with vectorized content\n        documents = [Document(doc_id=str(i), content=vector, metadata={}) for i, vector in enumerate(tfidf_matrix.toarray())]\n        return documents\n```"
    },
    {
        "document_name": "swarmauri/standard/parsers/concrete/URLExtractorParser.py",
        "content": "```swarmauri/standard/parsers/concrete/URLExtractorParser.py\nfrom typing import List, Union, Any\nfrom urllib.parse import urlparse\nimport re\nfrom ....core.parsers.IParser import IParser\nfrom ....core.documents.IDocument import IDocument\nfrom ....standard.documents.concrete.Document import Document\n\nclass URLExtractorParser(IParser):\n    \"\"\"\n    A concrete implementation of IParser that extracts URLs from text.\n    \n    This parser scans the input text for any URLs and creates separate\n    documents for each extracted URL. It utilizes regular expressions\n    to identify URLs within the given text.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initializes the URLExtractorParser.\n        \"\"\"\n        super().__init__()\n    \n    def parse(self, data: Union[str, Any]) -> List[IDocument]:\n        \"\"\"\n        Parse input data (string) and extract URLs, each URL is then represented as a document.\n        \n        Parameters:\n        - data (Union[str, Any]): The input data to be parsed for URLs.\n        \n        Returns:\n        - List[IDocument]: A list of documents, each representing an extracted URL.\n        \"\"\"\n        if not isinstance(data, str):\n            raise ValueError(\"URLExtractorParser expects input data to be of type str.\")\n\n        # Regular expression for finding URLs\n        url_regex = r\"https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+\"\n        \n        # Find all matches in the text\n        urls = re.findall(url_regex, data)\n        \n        # Create a document for each extracted URL\n        documents = [Document(doc_id=str(i), content=url, metadata={\"source\": \"URLExtractor\"}) for i, url in enumerate(urls)]\n        \n        return documents\n```"
    },
    {
        "document_name": "swarmauri/standard/parsers/concrete/XMLParser.py",
        "content": "```swarmauri/standard/parsers/concrete/XMLParser.py\nimport xml.etree.ElementTree as ET\nfrom typing import List, Union, Any\nfrom ....core.parsers.IParser import IParser\nfrom ....core.documents.IDocument import IDocument\nfrom ....standard.documents.concrete.Document import Document\n\nclass XMLParser(IParser):\n    \"\"\"\n    A parser that extracts information from XML data and converts it into IDocument objects.\n    This parser assumes a simple use-case where each targeted XML element represents a separate document.\n    \"\"\"\n\n    def __init__(self, element_tag: str):\n        \"\"\"\n        Initialize the XMLParser with the tag name of the XML elements to be extracted as documents.\n\n        Parameters:\n        - element_tag (str): The tag name of XML elements to parse into documents.\n        \"\"\"\n        self.element_tag = element_tag\n\n    def parse(self, data: Union[str, Any]) -> List[IDocument]:\n        \"\"\"\n        Parses XML data and converts elements with the specified tag into IDocument instances.\n\n        Parameters:\n        - data (Union[str, Any]): The XML data as a string to be parsed.\n\n        Returns:\n        - List[IDocument]: A list of IDocument instances created from the XML elements.\n        \"\"\"\n        if isinstance(data, str):\n            root = ET.fromstring(data)  # Parse the XML string into an ElementTree element\n        else:\n            raise TypeError(\"Data for XMLParser must be a string containing valid XML.\")\n\n        documents = []\n        for element in root.findall(self.element_tag):\n            # Extracting content and metadata from each element\n            content = \"\".join(element.itertext())  # Get text content\n            metadata = {child.tag: child.text for child in element}  # Extract child elements as metadata\n\n            # Create a Document instance for each element\n            doc = Document(doc_id=None, content=content, metadata=metadata)\n            documents.append(doc)\n\n        return documents\n```"
    },
    {
        "document_name": "swarmauri/standard/parsers/concrete/BERTEmbeddingParser.py",
        "content": "```swarmauri/standard/parsers/concrete/BERTEmbeddingParser.py\nfrom typing import List, Union, Any\nfrom transformers import BertTokenizer, BertModel\nimport torch\nfrom swarmauri.core.parsers.IParser import IParser\nfrom swarmauri.core.documents.IDocument import IDocument\nfrom swarmauri.standard.documents.concrete.Document import Document\n\nclass BERTEmbeddingParser(IParser):\n    \"\"\"\n    A parser that transforms input text into document embeddings using BERT.\n    \n    This parser tokenizes the input text, passes it through a pre-trained BERT model,\n    and uses the resulting embeddings as the document content.\n    \"\"\"\n\n    def __init__(self, model_name: str = 'bert-base-uncased'):\n        \"\"\"\n        Initializes the BERTEmbeddingParser with a specific BERT model.\n        \n        Parameters:\n        - model_name (str): The name of the pre-trained BERT model to use.\n        \"\"\"\n        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n        self.model = BertModel.from_pretrained(model_name)\n        self.model.eval()  # Set model to evaluation mode\n\n    \n    def parse(self, data: Union[str, Any]) -> List[IDocument]:\n        \"\"\"\n        Tokenizes input data and generates embeddings using a BERT model.\n\n        Parameters:\n        - data (Union[str, Any]): Input data, expected to be a single string or batch of strings.\n\n        Returns:\n        - List[IDocument]: A list containing a single IDocument instance with BERT embeddings as content.\n        \"\"\"\n        \n        # Tokenization\n        inputs = self.tokenizer(data, return_tensors='pt', padding=True, truncation=True, max_length=512)\n\n        # Generate embeddings\n        with torch.no_grad():\n            outputs = self.model(**inputs)\n\n        # Use the last hidden state as document embeddings (batch_size, sequence_length, hidden_size)\n        embeddings = outputs.last_hidden_state\n        \n        # Convert to list of numpy arrays\n        embeddings = embeddings.detach().cpu().numpy()\n        \n        # For simplicity, let's consider the mean of embeddings across tokens to represent the document\n        doc_embeddings = embeddings.mean(axis=1)\n        \n        # Creating document object(s)\n        documents = [Document(doc_id=str(i), content=emb, metadata={\"source\": \"BERTEmbeddingParser\"}) for i, emb in enumerate(doc_embeddings)]\n        \n        return documents\n```"
    },
    {
        "document_name": "swarmauri/standard/prompts/__init__.py",
        "content": "```swarmauri/standard/prompts/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/prompts/base/__init__.py",
        "content": "```swarmauri/standard/prompts/base/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/prompts/concrete/__init__.py",
        "content": "```swarmauri/standard/prompts/concrete/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/prompts/concrete/Prompt.py",
        "content": "```swarmauri/standard/prompts/concrete/Prompt.py\nfrom ....core.prompts.IPrompt import IPrompt\n\nclass Prompt(IPrompt):\n    \"\"\"\n    The ChatPrompt class represents a simple, chat-like prompt system where a \n    message can be set and retrieved as needed. It's particularly useful in \n    applications involving conversational agents, chatbots, or any system that \n    requires dynamic text-based interactions.\n    \"\"\"\n\n    def __init__(self, prompt: str = \"\"):\n        \"\"\"\n        Initializes an instance of ChatPrompt with an optional initial message.\n        \n        Parameters:\n        - message (str, optional): The initial message for the prompt. Defaults to an empty string.\n        \"\"\"\n        self.prompt = prompt\n\n    def __call__(self, prompt):\n        \"\"\"\n        Enables the instance to be callable, allowing direct retrieval of the message. \n        This method facilitates intuitive access to the prompt's message, mimicking callable \n        behavior seen in functional programming paradigms.\n        \n        Returns:\n        - str: The current message stored in the prompt.\n        \"\"\"\n        return self.prompt\n\n    def set_prompt(self, prompt: str):\n        \"\"\"\n        Updates the internal message of the chat prompt. This method provides a way to change \n        the content of the prompt dynamically, reflecting changes in the conversational context \n        or user inputs.\n        \n        Parameters:\n        - message (str): The new message to set for the prompt.\n        \"\"\"\n        self.prompt = prompt\n\n```"
    },
    {
        "document_name": "swarmauri/standard/prompts/concrete/PromptTemplate.py",
        "content": "```swarmauri/standard/prompts/concrete/PromptTemplate.py\nfrom typing import Dict\nfrom ....core.prompts.IPrompt import IPrompt\n\nclass PromptTemplate(IPrompt):\n    \"\"\"\n    A class that represents a template for generating prompts, \n    allowing dynamic content insertion into pre-defined template slots.\n\n    Attributes:\n        template (str): A string template with placeholders for content insertion.\n        variables (Dict[str, str]): A dictionary mapping placeholder names in the template to their content.\n    \"\"\"\n\n    def __init__(self, template: str = \"\", variables: Dict[str, str] = {}):\n        \"\"\"\n        Initializes a new instance of the PromptTemplate class.\n\n        Args:\n            template (str): The string template for the prompt.\n            variables (Dict[str, str]): A dictionary mapping variables in the template to their values.\n        \"\"\"\n        self.template = template\n        self.variables = variables\n\n    def __call__(self, variables: Dict[str, str] = {}):\n        \"\"\"\n        Generates the prompt string by substituting variables into the template.\n\n        Returns:\n            str: The generated prompt with variables substituted.\n        \"\"\"\n        variables = variables or self.variables\n        formatted_prompt = self.template.format(**variables)\n        return formatted_prompt\n\n    def set_template(self, template: str):\n        \"\"\"\n        Sets a new template string for the prompt.\n\n        Args:\n            template (str): The new string template to use.\n        \"\"\"\n        self.template = template\n\n    def set_variables(self, variables: Dict[str, str]):\n        \"\"\"\n        Sets the variables to be substituted into the template.\n\n        Args:\n            variables (Dict[str, str]): A dictionary of variables to be substituted into the template.\n        \n        Raises:\n            TypeError: If the provided variables argument is not a dictionary.\n        \"\"\"\n        if isinstance(variables, dict):\n            self.variables = variables\n        else:\n            raise TypeError(\"Invalid type. Expected dict for variables.\")\n```"
    },
    {
        "document_name": "swarmauri/standard/states/__init__.py",
        "content": "```swarmauri/standard/states/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/states/base/__init__.py",
        "content": "```swarmauri/standard/states/base/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/states/concrete/__init__.py",
        "content": "```swarmauri/standard/states/concrete/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/swarms/__init__.py",
        "content": "```swarmauri/standard/swarms/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/swarms/base/__init__.py",
        "content": "```swarmauri/standard/swarms/base/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/swarms/base/SwarmComponentBase.py",
        "content": "```swarmauri/standard/swarms/base/SwarmComponentBase.py\nfrom swarmauri.core.swarms.ISwarmComponent import ISwarmComponent\n\nclass SwarmComponentBase(ISwarmComponent):\n    \"\"\"\n    Interface for defining basics of any component within the swarm system.\n    \"\"\"\n    def __init__(self, key: str, name: str, superclass: str, module: str, class_name: str, args=None, kwargs=None):\n        self.key = key\n        self.name = name\n        self.superclass = superclass\n        self.module = module\n        self.class_name = class_name\n        self.args = args or []\n        self.kwargs = kwargs or {}\n    \n```"
    },
    {
        "document_name": "swarmauri/standard/swarms/concrete/__init__.py",
        "content": "```swarmauri/standard/swarms/concrete/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/swarms/concrete/SimpleSwarmFactory.py",
        "content": "```swarmauri/standard/swarms/concrete/SimpleSwarmFactory.py\nimport json\nimport pickle\nfrom typing import List\nfrom swarmauri.core.chains.ISwarmFactory import (\n    ISwarmFactory , \n    CallableChainItem, \n    AgentDefinition, \n    FunctionDefinition\n)\nclass SimpleSwarmFactory(ISwarmFactory):\n    def __init__(self):\n        self.swarms = []\n        self.callable_chains = []\n\n    def create_swarm(self, agents=[]):\n        swarm = {\"agents\": agents}\n        self.swarms.append(swarm)\n        return swarm\n\n    def create_agent(self, agent_definition: AgentDefinition):\n        # For simplicity, agents are stored in a list\n        # Real-world usage might involve more sophisticated management and instantiation based on type and configuration\n        agent = {\"definition\": agent_definition._asdict()}\n        self.agents.append(agent)\n        return agent\n\n    def create_callable_chain(self, chain_definition: List[CallableChainItem]):\n        chain = {\"definition\": [item._asdict() for item in chain_definition]}\n        self.callable_chains.append(chain)\n        return chain\n\n    def register_function(self, function_definition: FunctionDefinition):\n        if function_definition.identifier in self.functions:\n            raise ValueError(f\"Function {function_definition.identifier} is already registered.\")\n        \n        self.functions[function_definition.identifier] = function_definition\n    \n    def export_configuration(self, format_type: str = 'json'):\n        # Now exporting both swarms and callable chains\n        config = {\"swarms\": self.swarms, \"callable_chains\": self.callable_chains}\n        if format_type == \"json\":\n            return json.dumps(config)\n        elif format_type == \"pickle\":\n            return pickle.dumps(config)\n\n    def load_configuration(self, config_data, format_type: str = 'json'):\n        # Loading both swarms and callable chains\n        config = json.loads(config_data) if format_type == \"json\" else pickle.loads(config_data)\n        self.swarms = config.get(\"swarms\", [])\n        self.callable_chains = config.get(\"callable_chains\", [])\n```"
    },
    {
        "document_name": "swarmauri/standard/toolkits/__init__.py",
        "content": "```swarmauri/standard/toolkits/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/toolkits/base/__init__.py",
        "content": "```swarmauri/standard/toolkits/base/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/toolkits/base/ToolkitBase.py",
        "content": "```swarmauri/standard/toolkits/base/ToolkitBase.py\nfrom abc import ABC, abstractmethod\nfrom typing import Dict\nfrom ....core.toolkits.IToolkit import IToolkit\nfrom ....core.tools.ITool import ITool  \n\nclass ToolkitBase(IToolkit, ABC):\n    \"\"\"\n    A class representing a toolkit used by Swarm Agents.\n    Tools are maintained in a dictionary keyed by the tool's name.\n    \"\"\"\n\n    @abstractmethod\n    def __init__(self, initial_tools: Dict[str, ITool] = None):\n        \"\"\"\n        Initialize the Toolkit with an optional dictionary of initial tools.\n        \"\"\"\n        # If initial_tools is provided, use it; otherwise, use an empty dictionary\n        self._tools = initial_tools if initial_tools is not None else {}\n\n    @property\n    def tools(self) -> Dict[str, ITool]:\n        return [self._tools[tool].as_dict() for tool in self._tools]\n\n    def add_tools(self, tools: Dict[str, ITool]):\n        \"\"\"\n        Add multiple tools to the toolkit.\n\n        Parameters:\n            tools (Dict[str, Tool]): A dictionary of tool objects keyed by their names.\n        \"\"\"\n        self._tools.update(tools)\n\n    def add_tool(self, tool: ITool):\n        \"\"\"\n        Add a single tool to the toolkit.\n\n        Parameters:\n            tool (Tool): The tool instance to be added to the toolkit.\n        \"\"\"\n        self._tools[tool.function['name']] = tool\n\n    def remove_tool(self, tool_name: str):\n        \"\"\"\n        Remove a tool from the toolkit by name.\n\n        Parameters:\n            tool_name (str): The name of the tool to be removed from the toolkit.\n        \"\"\"\n        if tool_name in self._tools:\n            del self._tools[tool_name]\n        else:\n            raise ValueError(f\"Tool '{tool_name}' not found in the toolkit.\")\n\n    def get_tool_by_name(self, tool_name: str) -> ITool:\n        \"\"\"\n        Get a tool from the toolkit by name.\n\n        Parameters:\n            tool_name (str): The name of the tool to retrieve.\n\n        Returns:\n            Tool: The tool instance with the specified name.\n        \"\"\"\n        if tool_name in self._tools:\n            return self._tools[tool_name]\n        else:\n            raise ValueError(f\"Tool '{tool_name}' not found in the toolkit.\")\n\n    def __len__(self) -> int:\n        \"\"\"\n        Returns the number of tools in the toolkit.\n\n        Returns:\n            int: The number of tools in the toolkit.\n        \"\"\"\n        return len(self._tools)\n```"
    },
    {
        "document_name": "swarmauri/standard/toolkits/concrete/__init__.py",
        "content": "```swarmauri/standard/toolkits/concrete/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/toolkits/concrete/Toolkit.py",
        "content": "```swarmauri/standard/toolkits/concrete/Toolkit.py\nfrom typing import Dict\nfrom ..base.ToolkitBase import ToolkitBase\nfrom ....core.tools.ITool import ITool\n\nclass Toolkit(ToolkitBase):\n    \"\"\"\n    A class representing a toolkit used by Swarm Agents.\n    Tools are maintained in a dictionary keyed by the tool's name.\n    \"\"\"\n\n    def __init__(self, initial_tools: Dict[str, ITool] = None):\n        \"\"\"\n        Initialize the Toolkit with an optional dictionary of initial tools.\n        \"\"\"\n        \n        super().__init__(initial_tools)\n    \n```"
    },
    {
        "document_name": "swarmauri/standard/tools/__init__.py",
        "content": "```swarmauri/standard/tools/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/tools/base/__init__.py",
        "content": "```swarmauri/standard/tools/base/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/tools/base/ToolBase.py",
        "content": "```swarmauri/standard/tools/base/ToolBase.py\nfrom typing import Optional, List, Any\nfrom abc import ABC, abstractmethod\nimport json\nfrom swarmauri.core.tools.ITool import ITool\n        \nclass ToolBase(ITool, ABC):\n    \n    @abstractmethod\n    def __init__(self, name, description, parameters=[]):\n        self._name = name\n        self._description = description\n        self._parameters = parameters\n        self.type = \"function\"\n        self.function = {\n            \"name\": name,\n            \"description\": description,\n        }\n        \n        # Dynamically constructing the parameters schema\n        properties = {}\n        required = []\n        \n        for param in parameters:\n            properties[param.name] = {\n                \"type\": param.type,\n                \"description\": param.description,\n            }\n            if param.enum:\n                properties[param.name]['enum'] = param.enum\n\n            if param.required:\n                required.append(param.name)\n        \n        self.function['parameters'] = {\n            \"type\": \"object\",\n            \"properties\": properties,\n        }\n        \n        if required:  # Only include 'required' if there are any required parameters\n            self.function['parameters']['required'] = required\n\n\n    @property\n    def name(self):\n        return self._name\n\n    @property\n    def description(self):\n        return self._description\n\n    @property\n    def parameters(self):\n        return self._parameters\n\n    def __iter__(self):\n        yield ('type', self.type)\n        yield ('function', self.function)\n        \n\n    def as_dict(self):\n        return {'type': self.type, 'function': self.function}\n        # return self.__dict__\n\n    def to_json(obj):\n        return json.dumps(obj, default=lambda obj: obj.__dict__)\n\n    def __getstate__(self):\n        return {'type': self.type, 'function': self.function}\n\n\n    def __call__(self, *args, **kwargs):\n        \"\"\"\n        Placeholder method for executing the functionality of the tool.\n        Subclasses should override this method to define specific tool behaviors.\n\n        Parameters:\n        - *args: Variable length argument list.\n        - **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n        raise NotImplementedError(\"Subclasses must implement the call_function method.\")\n```"
    },
    {
        "document_name": "swarmauri/standard/tools/concrete/__init__.py",
        "content": "```swarmauri/standard/tools/concrete/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/tools/concrete/TestTool.py",
        "content": "```swarmauri/standard/tools/concrete/TestTool.py\nimport json\nimport subprocess as sp\nfrom ..base.ToolBase import ToolBase\nfrom .Parameter import Parameter\n\nclass TestTool(ToolBase):\n    def __init__(self):\n        parameters = [\n            Parameter(\n                name=\"program\",\n                type=\"string\",\n                description=\"The program that the user wants to open ('notepad' or 'calc' or 'mspaint')\",\n                required=True,\n                enum=[\"notepad\", \"calc\", \"mspaint\"]\n            )\n        ]\n        \n        super().__init__(name=\"TestTool\", \n                         description=\"This opens a program based on the user's request.\", \n                         parameters=parameters)\n\n    def __call__(self, program) -> str:\n        # sp.check_output(program)\n        # Here you would implement the actual logic for fetching the weather information.\n        # For demonstration, let's just return the parameters as a string.\n        return f\"Program Opened: {program}\\n\"\n\n```"
    },
    {
        "document_name": "swarmauri/standard/tools/concrete/WeatherTool.py",
        "content": "```swarmauri/standard/tools/concrete/WeatherTool.py\nimport json\nfrom ..base.ToolBase import ToolBase\nfrom .Parameter import Parameter\n\nclass WeatherTool(ToolBase):\n    def __init__(self):\n        parameters = [\n            Parameter(\n                name=\"location\",\n                type=\"string\",\n                description=\"The location for which to fetch weather information\",\n                required=True\n            ),\n            Parameter(\n                name=\"unit\",\n                type=\"string\",\n                description=\"The unit for temperature ('fahrenheit' or 'celsius')\",\n                required=True,\n                enum=[\"fahrenheit\", \"celsius\"]\n            )\n        ]\n        \n        super().__init__(name=\"WeatherTool\", description=\"Fetch current weather info for a location\", parameters=parameters)\n\n    def __call__(self, location, unit=\"fahrenheit\") -> str:\n        weather_info = (location, unit)\n        # Here you would implement the actual logic for fetching the weather information.\n        # For demonstration, let's just return the parameters as a string.\n        return f\"Weather Info: {weather_info}\\n\"\n\n```"
    },
    {
        "document_name": "swarmauri/standard/tools/concrete/Parameter.py",
        "content": "```swarmauri/standard/tools/concrete/Parameter.py\nfrom typing import Optional, List, Any\nimport json\nfrom ....core.tools.IParameter import IParameter\n\nclass Parameter(IParameter):\n    \"\"\"\n    A class to represent a parameter for a tool.\n\n    Attributes:\n        name (str): Name of the parameter.\n        type (str): Data type of the parameter (e.g., 'int', 'str', 'float').\n        description (str): A brief description of the parameter.\n        required (bool): Whether the parameter is required or optional.\n        enum (Optional[List[any]]): A list of acceptable values for the parameter, if any.\n    \"\"\"\n\n    def __init__(self, name: str, type: str, description: str, required: bool = True, enum: Optional[List[Any]] = None):\n        \"\"\"\n        Initializes a new instance of the Parameter class.\n\n        Args:\n            name (str): The name of the parameter.\n            type (str): The type of the parameter.\n            description (str): A brief description of what the parameter is used for.\n            required (bool, optional): Specifies if the parameter is required. Defaults to True.\n            enum (Optional[List[Any]], optional): A list of acceptable values for the parameter. Defaults to None.\n        \"\"\"\n        self._name = name\n        self._type = type\n        self._description = description\n        self._required = required\n        self._enum = enum\n        \n    @property\n    def name(self) -> str:\n        \"\"\"\n        Abstract property for getting the name of the parameter.\n        \"\"\"\n        return self._name\n\n    @name.setter\n    def name(self, value: str):\n        \"\"\"\n        Abstract setter for setting the name of the parameter.\n        \"\"\"\n        self._name = value\n\n    @property\n    def type(self) -> str:\n        \"\"\"\n        Abstract property for getting the type of the parameter.\n        \"\"\"\n        return self._type\n\n    @type.setter\n    def type(self, value: str):\n        \"\"\"\n        Abstract setter for setting the type of the parameter.\n        \"\"\"\n        self._type = value\n\n    @property\n    def description(self) -> str:\n        \"\"\"\n        Abstract property for getting the description of the parameter.\n        \"\"\"\n        return self._description\n\n    @description.setter\n    def description(self, value: str):\n        \"\"\"\n        Abstract setter for setting the description of the parameter.\n        \"\"\"\n        self._description = value\n\n    @property\n    def required(self) -> bool:\n        \"\"\"\n        Abstract property for getting the required status of the parameter.\n        \"\"\"\n        return self._required\n\n    @required.setter\n    def required(self, value: bool):\n        \"\"\"\n        Abstract setter for setting the required status of the parameter.\n        \"\"\"\n        self._required = value\n\n    @property\n    def enum(self) -> Optional[List[Any]]:\n        \"\"\"\n        Abstract property for getting the enum list of the parameter.\n        \"\"\"\n        return self._enum\n\n    @enum.setter\n    def enum(self, value: Optional[List[Any]]):\n        \"\"\"\n        Abstract setter for setting the enum list of the parameter.\n        \"\"\"\n        self._enum = value\n```"
    },
    {
        "document_name": "swarmauri/standard/tools/concrete/AdditionTool.py",
        "content": "```swarmauri/standard/tools/concrete/AdditionTool.py\nfrom ..base.ToolBase import ToolBase\nfrom .Parameter import Parameter\n\nclass AdditionTool(ToolBase):\n    \n    def __init__(self):\n        parameters = [\n            Parameter(\n                name=\"x\",\n                type=\"integer\",\n                description=\"The left operand\",\n                required=True\n            ),\n            Parameter(\n                name=\"y\",\n                type=\"integer\",\n                description=\"The right operand\",\n                required=True\n            )\n        ]\n        super().__init__(name=\"TestTool\", \n                         description=\"This opens a program based on the user's request.\", \n                         parameters=parameters)\n\n    def __call__(self, x: int, y: int) -> int:\n        \"\"\"\n        Add two numbers x and y and return the sum.\n\n        Parameters:\n        - x (int): The first number.\n        - y (int): The second number.\n\n        Returns:\n        - int: The sum of x and y.\n        \"\"\"\n        return x + y\n```"
    },
    {
        "document_name": "swarmauri/standard/apis/__init__.py",
        "content": "```swarmauri/standard/apis/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/apis/base/__init__.py",
        "content": "```swarmauri/standard/apis/base/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/apis/concrete/__init__.py",
        "content": "```swarmauri/standard/apis/concrete/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/vector_stores/__init__.py",
        "content": "```swarmauri/standard/vector_stores/__init__.py\n# -*- coding: utf-8 -*-\n\n\n```"
    },
    {
        "document_name": "swarmauri/standard/vector_stores/base/__init__.py",
        "content": "```swarmauri/standard/vector_stores/base/__init__.py\n# -*- coding: utf-8 -*-\n\n\n```"
    },
    {
        "document_name": "swarmauri/standard/vector_stores/base/VectorDocumentStoreBase.py",
        "content": "```swarmauri/standard/vector_stores/base/VectorDocumentStoreBase.py\nimport json\nfrom abc import ABC, abstractmethod\nfrom typing import List, Optional\nfrom swarmauri.core.documents.IDocument import IDocument\nfrom swarmauri.core.document_stores.IDocumentStore import IDocumentStore\n\nclass VectorDocumentStoreBase(IDocumentStore, ABC):\n    \"\"\"\n    Abstract base class for document stores, implementing the IDocumentStore interface.\n\n    This class provides a standard API for adding, updating, getting, and deleting documents in a store.\n    The specifics of storing (e.g., in a database, in-memory, or file system) are to be implemented by concrete subclasses.\n    \"\"\"\n\n    @abstractmethod\n    def add_document(self, document: IDocument) -> None:\n        \"\"\"\n        Add a single document to the document store.\n\n        Parameters:\n        - document (IDocument): The document to be added to the store.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def add_documents(self, documents: List[IDocument]) -> None:\n        \"\"\"\n        Add multiple documents to the document store in a batch operation.\n\n        Parameters:\n        - documents (List[IDocument]): A list of documents to be added to the store.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_document(self, doc_id: str) -> Optional[IDocument]:\n        \"\"\"\n        Retrieve a single document by its identifier.\n\n        Parameters:\n        - doc_id (str): The unique identifier of the document to retrieve.\n\n        Returns:\n        - Optional[IDocument]: The requested document if found; otherwise, None.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_all_documents(self) -> List[IDocument]:\n        \"\"\"\n        Retrieve all documents stored in the document store.\n\n        Returns:\n        - List[IDocument]: A list of all documents in the store.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def update_document(self, doc_id: str, updated_document: IDocument) -> None:\n        \"\"\"\n        Update a document in the document store.\n\n        Parameters:\n        - doc_id (str): The unique identifier of the document to update.\n        - updated_document (IDocument): The updated document instance.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def delete_document(self, doc_id: str) -> None:\n        \"\"\"\n        Delete a document from the document store by its identifier.\n\n        Parameters:\n        - doc_id (str): The unique identifier of the document to delete.\n        \"\"\"\n        pass\n    \n    def document_count(self):\n        return len(self.documents)\n    \n    def dump(self, file_path):\n        with open(file_path, 'w') as f:\n            json.dumps([each.__dict__ for each in self.documents], f, indent=4)\n            \n    def load(self, file_path):\n        with open(file_path, 'r') as f:\n            self.documents = json.loads(f)\n```"
    },
    {
        "document_name": "swarmauri/standard/vector_stores/base/VectorDocumentStoreRetrieveBase.py",
        "content": "```swarmauri/standard/vector_stores/base/VectorDocumentStoreRetrieveBase.py\nfrom abc import ABC, abstractmethod\nfrom typing import List\nfrom swarmauri.core.documents.IDocument import IDocument\nfrom swarmauri.core.document_stores.IDocumentRetrieve import IDocumentRetrieve\nfrom swarmauri.standard.vector_stores.base.VectorDocumentStoreBase import VectorDocumentStoreBase\n\nclass VectorDocumentStoreRetrieveBase(VectorDocumentStoreBase, IDocumentRetrieve, ABC):\n        \n    @abstractmethod\n    def retrieve(self, query: str, top_k: int = 5) -> List[IDocument]:\n        \"\"\"\n        Retrieve the top_k most relevant documents based on the given query.\n        \n        Args:\n            query (str): The query string used for document retrieval.\n            top_k (int): The number of top relevant documents to retrieve.\n        \n        Returns:\n            List[IDocument]: A list of the top_k most relevant documents.\n        \"\"\"\n        pass\n```"
    },
    {
        "document_name": "swarmauri/standard/vector_stores/concrete/__init__.py",
        "content": "```swarmauri/standard/vector_stores/concrete/__init__.py\n# -*- coding: utf-8 -*-\n\n\n```"
    },
    {
        "document_name": "swarmauri/standard/vector_stores/concrete/FaissVectorStore.py",
        "content": "```swarmauri/standard/vector_stores/concrete/FaissVectorStore.py\nimport faiss\nimport numpy as np\nfrom typing import List, Dict\n\nfrom swarmauri.core.vector_stores.IVectorStore import IVectorStore\nfrom swarmauri.core.vector_stores.ISimilarityQuery import ISimilarityQuery\nfrom swarmauri.core.vectors.IVector import IVector\n\nclass FaissVectorStore(IVectorStore, ISimilarityQuery):\n    \"\"\"\n    A vector store that utilizes FAISS for efficient similarity searches.\n    \"\"\"\n\n    def __init__(self, dimension: int, index_type: str = \"IVF256,Flat\"):\n        \"\"\"\n        Initialize the FAISS vector store with the given dimension and index type.\n\n        Parameters:\n        - dimension (int): The dimensionality of the vectors being stored.\n        - index_type (str): The FAISS index type. Defaults to \"IVF256,Flat\" for an inverted file index.\n        \"\"\"\n        self.dimension = dimension\n        self.index = faiss.index_factory(dimension, index_type)\n        self.id_to_vector = {}\n        self.id_to_metadata = {}\n\n    def add_vector(self, vector_id: str, vector: IVector, metadata: Dict = None) -> None:\n        \"\"\"\n        Add a vector along with its identifier and optional metadata to the store.\n\n        Parameters:\n        - vector_id (str): Unique identifier for the vector.\n        - vector (IVector): The high-dimensional vector to be stored.\n        - metadata (Dict, optional): Optional metadata related to the vector.\n        \"\"\"\n        # Ensure the vector is a numpy array and add it to the FAISS index\n        np_vector = np.array(vector.data, dtype='float32').reshape(1, -1)\n        self.index.add(np_vector)\n        self.id_to_vector[vector_id] = vector\n        if metadata:\n            self.id_to_metadata[vector_id] = metadata\n\n    def get_vector(self, vector_id: str) -> IVector:\n        \"\"\"\n        Retrieve a vector by its identifier.\n\n        Parameters:\n        - vector_id (str): The unique identifier for the vector.\n\n        Returns:\n        - IVector: The vector associated with the given ID.\n        \"\"\"\n        return self.id_to_vector.get(vector_id)\n\n    def search_by_similarity_threshold(self, query_vector: List[float], similarity_threshold: float, space_name: str = None) -> List[Dict]:\n        \"\"\"\n        Search vectors exceeding a similarity threshold to a query vector within an optional vector space.\n\n        Parameters:\n        - query_vector (List[float]): The high-dimensional query vector.\n        - similarity_threshold (float): The similarity threshold for filtering results.\n\n        Returns:\n        - List[Dict]: A list of dictionaries with vector IDs, similarity scores, and optional metadata that meet the similarity threshold.\n        \"\"\"\n        # FAISS requires numpy arrays in float32 for searches\n        np_query_vector = np.array(query_vector, dtype='float32').reshape(1, -1)\n\n        # Perform the search. FAISS returns distances, which can be converted to similarities.\n        _, I = self.index.search(np_query_vector, k=self.index.ntotal)  # Searching the entire index\n        results = []\n        for idx in I[0]:\n            vector_id = list(self.id_to_vector.keys())[idx]\n            # Simulate a similarity score based on the FAISS distance metric (e.g., L2 distance for now).\n            # Note: Depending on the index type and application, you might want to convert distances to actual similarities.\n            results.append({\"id\": vector_id, \"score\": similarity_threshold, \"metadata\": self.id_to_metadata.get(vector_id)})\n\n        return results\n```"
    },
    {
        "document_name": "swarmauri/standard/vector_stores/concrete/WeaviateVectorStore.py",
        "content": "```swarmauri/standard/vector_stores/concrete/WeaviateVectorStore.py\nfrom typing import List, Dict\nimport weaviate\nfrom swarmauri.core.vector_stores.IVectorStore import IVectorStore\nfrom swarmauri.core.vectors.IVector import IVector\nfrom swarmauri.standard.vectors.concrete.SimpleVector import SimpleVector\n\nclass WeaviateVectorStore(IVectorStore):\n    def __init__(self, weaviate_url: str):\n        self.client = weaviate.Client(url=weaviate_url)\n        # Set up schema if not exists, etc.\n        pass\n    \n    def add_vector(self, vector_id: str, vector: IVector, metadata: Dict = None) -> None:\n        data_object = {\n            \"vector\": vector.data\n        }\n        if metadata:\n            data_object[\"metadata\"] = metadata\n        self.client.data_object.create(data_object=data_object, class_name=\"Vector\", uuid=vector_id)\n    \n    def get_vector(self, vector_id: str) -> IVector:\n        result = self.client.data_object.get_by_id(vector_id, [\"vector\"])\n        return SimpleVector(result['vector'])\n    \n    def delete_vector(self, vector_id: str) -> None:\n        self.client.data_object.delete(vector_id)\n    \n    def update_vector(self, vector_id: str, new_vector: IVector, new_metadata: Dict = None) -> None:\n        update_object = {\n            \"vector\": new_vector.data\n        }\n        if new_metadata:\n            update_object[\"metadata\"] = new_metadata\n        self.client.data_object.update(object_id=vector_id, data_object=update_object)\n    \n    # Implement other methods like search_by_similarity_threshold from ISimilarityQuery interface, etc.\n```"
    },
    {
        "document_name": "swarmauri/standard/vector_stores/concrete/TFIDFVectorStore.py",
        "content": "```swarmauri/standard/vector_stores/concrete/TFIDFVectorStore.py\nfrom typing import List, Union\nfrom swarmauri.core.documents.IDocument import IDocument\nfrom swarmauri.standard.vectorizers.concrete.TFIDFVectorizer import TFIDFVectorizer\nfrom swarmauri.standard.distances.concrete.CosineDistance import CosineDistance\nfrom swarmauri.standard.vector_stores.base.VectorDocumentStoreRetrieveBase import VectorDocumentStoreRetrieveBase\n\nclass TFIDFVectorStore(VectorDocumentStoreRetrieveBase):\n    def __init__(self):\n        self.vectorizer = TFIDFVectorizer()\n        self.metric = CosineDistance()\n        self.documents = []      \n\n    def add_document(self, document: IDocument) -> None:\n        self.documents.append(document)\n        # Recalculate TF-IDF matrix for the current set of documents\n        self.vectorizer.fit([doc.content for doc in self.documents])\n\n    def add_documents(self, documents: List[IDocument]) -> None:\n        self.documents.extend(documents)\n        # Recalculate TF-IDF matrix for the current set of documents\n        self.vectorizer.fit([doc.content for doc in self.documents])\n\n    def get_document(self, doc_id: str) -> Union[IDocument, None]:\n        for document in self.documents:\n            if document.id == doc_id:\n                return document\n        return None\n\n    def get_all_documents(self) -> List[IDocument]:\n        return self.documents\n\n    def delete_document(self, doc_id: str) -> None:\n        self.documents = [doc for doc in self.documents if doc.id != doc_id]\n        # Recalculate TF-IDF matrix for the current set of documents\n        self.vectorizer.fit([doc.content for doc in self.documents])\n\n    def update_document(self, doc_id: str, updated_document: IDocument) -> None:\n        for i, document in enumerate(self.documents):\n            if document.id == doc_id:\n                self.documents[i] = updated_document\n                break\n\n        # Recalculate TF-IDF matrix for the current set of documents\n        self.vectorizer.fit([doc.content for doc in self.documents])\n\n    def retrieve(self, query: str, top_k: int = 5) -> List[IDocument]:\n        transform_matrix = self.vectorizer.fit_transform(query, self.documents)\n\n        # The inferred vector is the last vector in the transformed_matrix\n        # The rest of the matrix is what we are comparing\n        distances = self.metric.distances(transform_matrix[-1], transform_matrix[:-1])  \n\n        # Get the indices of the top_k most similar (least distant) documents\n        top_k_indices = sorted(range(len(distances)), key=lambda i: distances[i])[:top_k]\n        return [self.documents[i] for i in top_k_indices]\n\n```"
    },
    {
        "document_name": "swarmauri/standard/vector_stores/concrete/BERTVectorStore.py",
        "content": "```swarmauri/standard/vector_stores/concrete/BERTVectorStore.py\nfrom typing import List, Union\nfrom swarmauri.core.documents.IDocument import IDocument\nfrom swarmauri.standard.documents.concrete.EmbeddedDocument import EmbeddedDocument\nfrom swarmauri.standard.vectorizers.concrete.BERTEmbeddingVectorizer import BERTEmbeddingVectorizer\nfrom swarmauri.standard.distances.concrete.CosineDistance import CosineDistance\nfrom swarmauri.standard.vector_stores.base.VectorDocumentStoreRetrieveBase import VectorDocumentStoreRetrieveBase\n\nclass BERTVectorStore(VectorDocumentStoreRetrieveBase):\n    def __init__(self):\n        self.documents: List[EmbeddedDocument] = []\n        self.vectorizer = BERTEmbeddingVectorizer()  # Assuming this is already implemented\n        self.metric = CosineDistance()\n\n    def add_document(self, document: IDocument) -> None:\n        \"\"\"\n        Override: Now documents are expected to have labels for fine-tuning when added. \n        For unsupervised use-cases, labels can be ignored at the vectorizer level.\n        \"\"\"\n        self.documents.append(document)\n        documents_text = [doc.content for doc in self.documents]\n        documents_labels = [doc.metadata['label'] for doc in self.documents]\n        self.vectorizer.fit(documents_text, documents_labels)\n        embeddings = self.vectorizer.infer_vector(document.content)\n\n        embedded_document = EmbeddedDocument(doc_id=document.id, \n            content=document.content, \n            metadata=document.metadata, \n            embedding=embeddings)\n\n        self.documents.append(embedded_document)\n\n    def add_documents(self, documents: List[IDocument]) -> None:\n        # Batch addition of documents with potential fine-tuning trigger\n        self.documents.extend(documents)\n        documents_text = [doc.content for doc in documents]\n        documents_labels = [doc.metadata['label'] for doc in self.documents]\n        self.vectorizer.fit(documents_text, documents_labels)\n\n    def get_document(self, doc_id: str) -> Union[EmbeddedDocument, None]:\n        for document in self.documents:\n            if document.id == doc_id:\n                return document\n        return None\n        \n    def get_all_documents(self) -> List[EmbeddedDocument]:\n        return self.documents\n\n    def delete_document(self, doc_id: str) -> None:\n        self.documents = [doc for doc in self.documents if doc.id != doc_id]\n\n    def update_document(self, doc_id: str) -> None:\n        raise NotImplementedError('Update_document not implemented on BERTDocumentStore class.')\n        \n    def retrieve(self, query: str, top_k: int = 5) -> List[IDocument]:\n        query_vector = self.vectorizer.infer_vector(query)\n        document_vectors = [doc.embedding for doc in self.documents]\n        distances = [self.metric.similarities(query_vector, document_vectors)]\n        \n        # Get the indices of the top_k most similar documents\n        top_k_indices = sorted(range(len(distances)), key=lambda i: distances[i], reverse=True)[:top_k]\n        \n        return [self.documents[i] for i in top_k_indices]\n\n```"
    },
    {
        "document_name": "swarmauri/standard/vector_stores/concrete/Doc2VecVectorStore.py",
        "content": "```swarmauri/standard/vector_stores/concrete/Doc2VecVectorStore.py\nfrom typing import List, Union\nfrom swarmauri.core.documents.IDocument import IDocument\nfrom swarmauri.standard.documents.concrete.EmbeddedDocument import EmbeddedDocument\nfrom swarmauri.standard.vectorizers.concrete.Doc2VecVectorizer import Doc2VecVectorizer\nfrom swarmauri.standard.distances.concrete.CosineDistance import CosineDistance\nfrom swarmauri.standard.vector_stores.base.VectorDocumentStoreRetrieveBase import VectorDocumentStoreRetrieveBase\n\nclass Doc2VecVectorStore(VectorDocumentStoreRetrieveBase):\n    def __init__(self):\n        self.vectorizer = Doc2VecVectorizer()\n        self.metric = CosineDistance()\n        self.documents = []      \n\n    def add_document(self, document: IDocument) -> None:\n        self.documents.append(document)\n        self._recalculate_embeddings()\n\n    def add_documents(self, documents: List[IDocument]) -> None:\n        self.documents.extend(documents)\n        self._recalculate_embeddings()\n\n    def get_document(self, doc_id: str) -> Union[EmbeddedDocument, None]:\n        for document in self.documents:\n            if document.id == doc_id:\n                return document\n        return None\n\n    def get_all_documents(self) -> List[EmbeddedDocument]:\n        return self.documents\n\n    def delete_document(self, doc_id: str) -> None:\n        self.documents = [doc for doc in self.documents if doc.id != doc_id]\n        self._recalculate_embeddings()\n\n    def update_document(self, doc_id: str, updated_document: IDocument) -> None:\n        for i, document in enumerate(self.documents):\n            if document.id == doc_id:\n                self.documents[i] = updated_document\n                break\n        self._recalculate_embeddings()\n\n    def _recalculate_embeddings(self):\n        # Recalculate document embeddings for the current set of documents\n        documents_text = [_d.content for _d in self.documents if _d.content]\n        embeddings = self.vectorizer.fit_transform(documents_text)\n\n        embedded_documents = [EmbeddedDocument(doc_id=_d.id, \n            content=_d.content, \n            metadata=_d.metadata, \n            embedding=embeddings[_count]) for _count, _d in enumerate(self.documents)\n            if _d.content]\n\n        self.documents = embedded_documents\n\n    def retrieve(self, query: str, top_k: int = 5) -> List[IDocument]:\n        query_vector = self.vectorizer.infer_vector(query)\n        document_vectors = [_d.embedding for _d in self.documents if _d.content]\n\n        distances = self.metric.distances(query_vector, document_vectors)\n\n        # Get the indices of the top_k least distant (most similar) documents\n        top_k_indices = sorted(range(len(distances)), key=lambda i: distances[i])[:top_k]\n        \n        return [self.documents[i] for i in top_k_indices]\n\n```"
    },
    {
        "document_name": "swarmauri/standard/vector_stores/concrete/MLMVectorStore.py",
        "content": "```swarmauri/standard/vector_stores/concrete/MLMVectorStore.py\nfrom typing import List, Union\nfrom swarmauri.core.documents.IDocument import IDocument\nfrom swarmauri.standard.documents.concrete.EmbeddedDocument import EmbeddedDocument\nfrom swarmauri.standard.vectorizers.concrete.MLMVectorizer import MLMVectorizer\nfrom swarmauri.standard.distances.concrete.CosineDistance import CosineDistance\nfrom swarmauri.standard.vector_stores.base.VectorDocumentStoreRetrieveBase import VectorDocumentStoreRetrieveBase\n\nclass MLMVectorStore(VectorDocumentStoreRetrieveBase):\n    def __init__(self):\n        self.documents: List[EmbeddedDocument] = []\n        self.vectorizer = MLMVectorizer()  # Assuming this is already implemented\n        self.metric = CosineDistance()\n\n    def add_document(self, document: IDocument) -> None:\n        self.documents.append(document)\n        documents_text = [_d.content for _d in self.documents if _d.content]\n        embeddings = self.vectorizer.fit_transform(documents_text)\n\n        embedded_documents = [EmbeddedDocument(doc_id=_d.id, \n            content=_d.content, \n            metadata=_d.metadata, \n            embedding=embeddings[_count])\n\n        for _count, _d in enumerate(self.documents) if _d.content]\n\n        self.documents = embedded_documents\n\n    def add_documents(self, documents: List[IDocument]) -> None:\n        self.documents.extend(documents)\n        documents_text = [_d.content for _d in self.documents if _d.content]\n        embeddings = self.vectorizer.fit_transform(documents_text)\n\n        embedded_documents = [EmbeddedDocument(doc_id=_d.id, \n            content=_d.content, \n            metadata=_d.metadata, \n            embedding=embeddings[_count]) for _count, _d in enumerate(self.documents) \n            if _d.content]\n\n        self.documents = embedded_documents\n\n    def get_document(self, doc_id: str) -> Union[EmbeddedDocument, None]:\n        for document in self.documents:\n            if document.id == doc_id:\n                return document\n        return None\n        \n    def get_all_documents(self) -> List[EmbeddedDocument]:\n        return self.documents\n\n    def delete_document(self, doc_id: str) -> None:\n        self.documents = [_d for _d in self.documents if _d.id != doc_id]\n\n    def update_document(self, doc_id: str) -> None:\n        raise NotImplementedError('Update_document not implemented on BERTDocumentStore class.')\n        \n    def retrieve(self, query: str, top_k: int = 5) -> List[IDocument]:\n        query_vector = self.vectorizer.infer_vector(query)\n        document_vectors = [_d.embedding for _d in self.documents if _d.content]\n        distances = self.metric.distances(query_vector, document_vectors)\n        \n        # Get the indices of the top_k most similar documents\n        top_k_indices = sorted(range(len(distances)), key=lambda i: distances[i])[:top_k]\n        \n        return [self.documents[i] for i in top_k_indices]\n\n```"
    },
    {
        "document_name": "swarmauri/standard/document_stores/__init__.py",
        "content": "```swarmauri/standard/document_stores/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/document_stores/base/__init__.py",
        "content": "```swarmauri/standard/document_stores/base/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/document_stores/base/DocumentStoreBase.py",
        "content": "```swarmauri/standard/document_stores/base/DocumentStoreBase.py\nimport json\nfrom abc import ABC, abstractmethod\nfrom typing import List, Optional\nfrom swarmauri.core.documents.IDocument import IDocument\nfrom swarmauri.core.document_stores.IDocumentStore import IDocumentStore\n\nclass DocumentStoreBase(IDocumentStore, ABC):\n    \"\"\"\n    Abstract base class for document stores, implementing the IDocumentStore interface.\n\n    This class provides a standard API for adding, updating, getting, and deleting documents in a store.\n    The specifics of storing (e.g., in a database, in-memory, or file system) are to be implemented by concrete subclasses.\n    \"\"\"\n\n    @abstractmethod\n    def add_document(self, document: IDocument) -> None:\n        \"\"\"\n        Add a single document to the document store.\n\n        Parameters:\n        - document (IDocument): The document to be added to the store.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def add_documents(self, documents: List[IDocument]) -> None:\n        \"\"\"\n        Add multiple documents to the document store in a batch operation.\n\n        Parameters:\n        - documents (List[IDocument]): A list of documents to be added to the store.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_document(self, doc_id: str) -> Optional[IDocument]:\n        \"\"\"\n        Retrieve a single document by its identifier.\n\n        Parameters:\n        - doc_id (str): The unique identifier of the document to retrieve.\n\n        Returns:\n        - Optional[IDocument]: The requested document if found; otherwise, None.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_all_documents(self) -> List[IDocument]:\n        \"\"\"\n        Retrieve all documents stored in the document store.\n\n        Returns:\n        - List[IDocument]: A list of all documents in the store.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def update_document(self, doc_id: str, updated_document: IDocument) -> None:\n        \"\"\"\n        Update a document in the document store.\n\n        Parameters:\n        - doc_id (str): The unique identifier of the document to update.\n        - updated_document (IDocument): The updated document instance.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def delete_document(self, doc_id: str) -> None:\n        \"\"\"\n        Delete a document from the document store by its identifier.\n\n        Parameters:\n        - doc_id (str): The unique identifier of the document to delete.\n        \"\"\"\n        pass\n    \n    def document_count(self):\n        return len(self.documents)\n    \n    def dump(self, file_path):\n        with open(file_path, 'w') as f:\n            json.dumps([each.__dict__ for each in self.documents], f, indent=4)\n            \n    def load(self, file_path):\n        with open(file_path, 'r') as f:\n            self.documents = json.loads(f)\n```"
    },
    {
        "document_name": "swarmauri/standard/document_stores/base/DocumentStoreRetrieveBase.py",
        "content": "```swarmauri/standard/document_stores/base/DocumentStoreRetrieveBase.py\nfrom abc import ABC, abstractmethod\nfrom typing import List\nfrom swarmauri.core.document_stores.IDocumentRetrieve import IDocumentRetrieve\nfrom swarmauri.core.documents.IDocument import IDocument\nfrom swarmauri.standard.document_stores.base.DocumentStoreBase import DocumentStoreBase\n\nclass DocumentStoreRetrieveBase(DocumentStoreBase, IDocumentRetrieve, ABC):\n\n        \n    @abstractmethod\n    def retrieve(self, query: str, top_k: int = 5) -> List[IDocument]:\n        \"\"\"\n        Retrieve the top_k most relevant documents based on the given query.\n        \n        Args:\n            query (str): The query string used for document retrieval.\n            top_k (int): The number of top relevant documents to retrieve.\n        \n        Returns:\n            List[IDocument]: A list of the top_k most relevant documents.\n        \"\"\"\n        pass\n```"
    },
    {
        "document_name": "swarmauri/standard/document_stores/concrete/__init__.py",
        "content": "```swarmauri/standard/document_stores/concrete/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/chunkers/__init__.py",
        "content": "```swarmauri/standard/chunkers/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/chunkers/base/__init__.py",
        "content": "```swarmauri/standard/chunkers/base/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/chunkers/concrete/__init__.py",
        "content": "```swarmauri/standard/chunkers/concrete/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/chunkers/concrete/SlidingWindowChunker.py",
        "content": "```swarmauri/standard/chunkers/concrete/SlidingWindowChunker.py\nfrom typing import List\nfrom swarmauri.core.chunkers.IChunker import IChunker\n\nclass SlidingWindowChunker(IChunker):\n    \"\"\"\n    A concrete implementation of IChunker that uses sliding window technique\n    to break the text into chunks.\n    \"\"\"\n    \n    def __init__(self, window_size: int, step_size: int, overlap: bool = True):\n        \"\"\"\n        Initialize the SlidingWindowChunker with specific window and step sizes.\n        \n        Parameters:\n        - window_size (int): The size of the window for each chunk (in terms of number of words).\n        - step_size (int): The step size for the sliding window (in terms of number of words).\n        - overlap (bool, optional): Whether the windows should overlap. Default is True.\n        \"\"\"\n        self.window_size = window_size\n        self.step_size = step_size if overlap else window_size  # Non-overlapping if window size equals step size.\n           \n    def chunk_text(self, text: str, *args, **kwargs) -> List[str]:\n        \"\"\"\n        Splits the input text into chunks based on the sliding window technique.\n        \n        Parameters:\n        - text (str): The input text to be chunked.\n        \n        Returns:\n        - List[str]: A list of text chunks.\n        \"\"\"\n        words = text.split()  # Tokenization by whitespaces. More sophisticated tokenization might be necessary.\n        chunks = []\n        \n        for i in range(0, len(words) - self.window_size + 1, self.step_size):\n            chunk = ' '.join(words[i:i+self.window_size])\n            chunks.append(chunk)\n        \n        return chunks\n```"
    },
    {
        "document_name": "swarmauri/standard/chunkers/concrete/DelimiterBasedChunker.py",
        "content": "```swarmauri/standard/chunkers/concrete/DelimiterBasedChunker.py\nfrom typing import List, Union, Any\nimport re\nfrom swarmauri.core.chunkers.IChunker import IChunker\n\nclass DelimiterBasedChunker(IChunker):\n    \"\"\"\n    A concrete implementation of IChunker that splits text into chunks based on specified delimiters.\n    \"\"\"\n\n    def __init__(self, delimiters: List[str] = None):\n        \"\"\"\n        Initializes the chunker with a list of delimiters.\n\n        Parameters:\n        - delimiters (List[str], optional): A list of strings that will be used as delimiters for splitting the text.\n                                            If not specified, a default list of sentence-ending punctuation is used.\n        \"\"\"\n        if delimiters is None:\n            delimiters = ['.', '!', '?']  # Default delimiters\n        # Create a regex pattern that matches any of the specified delimiters.\n        # The pattern uses re.escape on each delimiter to ensure special regex characters are treated literally.\n        self.delimiter_pattern = f\"({'|'.join(map(re.escape, delimiters))})\"\n    \n    def chunk_text(self, text: Union[str, Any], *args, **kwargs) -> List[str]:\n        \"\"\"\n        Chunks the given text based on the delimiters specified during initialization.\n\n        Parameters:\n        - text (Union[str, Any]): The input text to be chunked.\n\n        Returns:\n        - List[str]: A list of text chunks split based on the specified delimiters.\n        \"\"\"\n        # Split the text based on the delimiter pattern, including the delimiters in the result\n        chunks = re.split(self.delimiter_pattern, text)\n        # Combine delimiters with the preceding text chunk since re.split() separates them\n        combined_chunks = []\n        for i in range(0, len(chunks) - 1, 2):  # Step by 2 to process text chunk with its following delimiter\n            combined_chunks.append(chunks[i] + (chunks[i + 1] if i + 1 < len(chunks) else ''))\n        return combined_chunks\n```"
    },
    {
        "document_name": "swarmauri/standard/chunkers/concrete/FixedLengthChunker.py",
        "content": "```swarmauri/standard/chunkers/concrete/FixedLengthChunker.py\nfrom typing import List, Union, Any\nfrom swarmauri.core.chunkers.IChunker import IChunker\n\nclass FixedLengthChunker(IChunker):\n    \"\"\"\n    Concrete implementation of IChunker that divides text into fixed-length chunks.\n    \n    This chunker breaks the input text into chunks of a specified size, making sure \n    that each chunk has exactly the number of characters specified by the chunk size, \n    except for possibly the last chunk.\n    \"\"\"\n\n    def __init__(self, chunk_size: int):\n        \"\"\"\n        Initializes a new instance of the FixedLengthChunker class with a specific chunk size.\n\n        Parameters:\n        - chunk_size (int): The fixed size (number of characters) for each chunk.\n        \"\"\"\n        self.chunk_size = chunk_size\n\n    def chunk_text(self, text: Union[str, Any], *args, **kwargs) -> List[str]:\n        \"\"\"\n        Splits the input text into fixed-length chunks.\n\n        Parameters:\n        - text (Union[str, Any]): The input text to be chunked.\n        \n        Returns:\n        - List[str]: A list of text chunks, each of a specified fixed length.\n        \"\"\"\n        # Check if the input is a string, if not, attempt to convert to a string.\n        if not isinstance(text, str):\n            text = str(text)\n        \n        # Using list comprehension to split text into chunks of fixed size\n        chunks = [text[i:i+self.chunk_size] for i in range(0, len(text), self.chunk_size)]\n        \n        return chunks\n```"
    },
    {
        "document_name": "swarmauri/standard/chunkers/concrete/SimpleSentenceChunker.py",
        "content": "```swarmauri/standard/chunkers/concrete/SimpleSentenceChunker.py\nimport re\nfrom swarmauri.core.chunkers.IChunker import IChunker\n\nclass SimpleSentenceChunker(IChunker):\n    \"\"\"\n    A simple implementation of the IChunker interface to chunk text into sentences.\n    \n    This class uses basic punctuation marks (., !, and ?) as indicators for sentence boundaries.\n    \"\"\"\n    \n    def chunk_text(self, text, *args, **kwargs):\n        \"\"\"\n        Chunks the given text into sentences using basic punctuation.\n\n        Args:\n            text (str): The input text to be chunked into sentences.\n        \n        Returns:\n            List[str]: A list of sentence chunks.\n        \"\"\"\n        # Split text using a simple regex pattern that looks for periods, exclamation marks, or question marks.\n        # Note: This is a very simple approach and might not work well with abbreviations or other edge cases.\n        sentence_pattern = r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|!)\\s'\n        sentences = re.split(sentence_pattern, text)\n        \n        # Filter out any empty strings that may have resulted from the split operation\n        sentences = [sentence.strip() for sentence in sentences if sentence]\n        \n        return sentences\n```"
    },
    {
        "document_name": "swarmauri/standard/chunkers/concrete/MdSnippetChunker.py",
        "content": "```swarmauri/standard/chunkers/concrete/MdSnippetChunker.py\nfrom typing import List, Union, Any, Optional\nimport re\nfrom swarmauri.core.chunkers.IChunker import IChunker\n\nclass MdSnippetChunker(IChunker):\n    def __init__(self, language: Optional[str] = None):\n        \"\"\"\n        Initializes the MdSnippetChunker with a specific programming language\n        to look for within Markdown fenced code blocks.\n        \"\"\"\n        self.language = language\n    \n    def chunk_text(self, text: Union[str, Any], *args, **kwargs) -> List[tuple]:\n        \"\"\"\n        Extracts paired comments and code blocks from Markdown content based on the \n        specified programming language.\n        \"\"\"\n        if self.language:\n            # If language is specified, directly extract the corresponding blocks\n            pattern = fr'```{self.language}\\s*(.*?)```'\n            scripts = re.findall(pattern, text, re.DOTALL)\n            comments_temp = re.split(pattern, text, flags=re.DOTALL)\n        else:\n            # Extract blocks and languages dynamically if no specific language is provided\n            scripts = []\n            languages = []\n            for match in re.finditer(r'```(\\w+)?\\s*(.*?)```', text, re.DOTALL):\n                if match.group(1) is not None:  # Checks if a language identifier is present\n                    languages.append(match.group(1))\n                    scripts.append(match.group(2))\n                else:\n                    languages.append('')  # Default to an empty string if no language is found\n                    scripts.append(match.group(2))\n            comments_temp = re.split(r'```.*?```', text, flags=re.DOTALL)\n\n        comments = [comment.strip() for comment in comments_temp]\n\n        if text.strip().startswith('```'):\n            comments = [''] + comments\n        if text.strip().endswith('```'):\n            comments.append('')\n\n        if self.language:\n            structured_output = [(comments[i], self.language, scripts[i].strip()) for i in range(len(scripts))]\n        else:\n            structured_output = [(comments[i], languages[i], scripts[i].strip()) for i in range(len(scripts))]\n\n        return structured_output\n\n```"
    },
    {
        "document_name": "swarmauri/standard/vectors/__init__.py",
        "content": "```swarmauri/standard/vectors/__init__.py\n# -*- coding: utf-8 -*-\n\n\n```"
    },
    {
        "document_name": "swarmauri/standard/vectors/base/__init__.py",
        "content": "```swarmauri/standard/vectors/base/__init__.py\n# -*- coding: utf-8 -*-\n\n\n```"
    },
    {
        "document_name": "swarmauri/standard/vectors/base/VectorBase.py",
        "content": "```swarmauri/standard/vectors/base/VectorBase.py\nfrom abc import ABC, abstractmethod\nfrom typing import List\nimport numpy as np\nfrom swarmauri.core.vectors.IVector import IVector\n\nclass VectorBase(IVector, ABC):\n    def __init__(self, data: List[float]):\n        self._data = data\n\n    @property\n    def data(self) -> List[float]:\n        \"\"\"\n        Returns the vector's data.\n        \"\"\"\n        return self._data\n    \n    def to_numpy(self) -> np.ndarray:\n        \"\"\"\n        Converts the vector into a numpy array.\n\n        Returns:\n            np.ndarray: The numpy array representation of the vector.\n        \"\"\"\n        return np.array(self._data)\n    \n    def __repr__(self):\n        return str(self.data)\n    \n    def __len__(self):\n        return len(self.data)\n```"
    },
    {
        "document_name": "swarmauri/standard/vectors/concrete/SimpleVector.py",
        "content": "```swarmauri/standard/vectors/concrete/SimpleVector.py\nfrom typing import List\nfrom swarmauri.standard.vectors.base.VectorBase import VectorBase\n\nclass SimpleVector(VectorBase):\n    def __init__(self, data: List[float]):\n        super().__init__(data)\n        \n```"
    },
    {
        "document_name": "swarmauri/standard/vectors/concrete/__init__.py",
        "content": "```swarmauri/standard/vectors/concrete/__init__.py\n# -*- coding: utf-8 -*-\n\n\n```"
    },
    {
        "document_name": "swarmauri/standard/vectors/concrete/VectorProduct.py",
        "content": "```swarmauri/standard/vectors/concrete/VectorProduct.py\nimport numpy as np\nfrom typing import List\n\nfrom swarmauri.core.vectors.IVector import IVector\nfrom swarmauri.core.vectors.IVectorProduct import IVectorProduct\nfrom swarmauri.standard.vectors.concrete.SimpleVector import SimpleVector\n\nclass VectorProduct(IVectorProduct):\n    def dot_product(self, vector_a: IVector, vector_b: IVector) -> float:\n        a = np.array(vector_a.data).flatten()\n        b = np.array(vector_b.data).flatten()\n        return np.dot(a, b)\n    \n    def cross_product(self, vector_a: IVector, vector_b: IVector) -> IVector:\n        if len(vector_a.data) != 3 or len(vector_b.data) != 3:\n            raise ValueError(\"Cross product is only defined for 3-dimensional vectors\")\n        a = np.array(vector_a.data)\n        b = np.array(vector_b.data)\n        cross = np.cross(a, b)\n        return SimpleVector(cross.tolist())\n    \n    def vector_triple_product(self, vector_a: IVector, vector_b: IVector, vector_c: IVector) -> IVector:\n        a = np.array(vector_a.data)\n        b = np.array(vector_b.data)\n        c = np.array(vector_c.data)\n        result = np.cross(a, np.cross(b, c))\n        return SimpleVector(result.tolist())\n    \n    def scalar_triple_product(self, vector_a: IVector, vector_b: IVector, vector_c: IVector) -> float:\n        a = np.array(vector_a.data)\n        b = np.array(vector_b.data)\n        c = np.array(vector_c.data)\n        return np.dot(a, np.cross(b, c))\n```"
    },
    {
        "document_name": "swarmauri/standard/vectorizers/__init__.py",
        "content": "```swarmauri/standard/vectorizers/__init__.py\n#\n```"
    },
    {
        "document_name": "swarmauri/standard/vectorizers/base/__init__.py",
        "content": "```swarmauri/standard/vectorizers/base/__init__.py\n#\n```"
    },
    {
        "document_name": "swarmauri/standard/vectorizers/concrete/__init__.py",
        "content": "```swarmauri/standard/vectorizers/concrete/__init__.py\n#\n```"
    },
    {
        "document_name": "swarmauri/standard/vectorizers/concrete/Doc2VecVectorizer.py",
        "content": "```swarmauri/standard/vectorizers/concrete/Doc2VecVectorizer.py\nfrom typing import List, Union, Any\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\nfrom swarmauri.core.vectorizers.IVectorize import IVectorize\nfrom swarmauri.core.vectorizers.IFeature import IFeature\nfrom swarmauri.core.vectors.IVector import IVector\nfrom swarmauri.standard.vectors.concrete.SimpleVector import SimpleVector\n\nclass Doc2VecVectorizer(IVectorize, IFeature):\n    def __init__(self):\n        self.model = Doc2Vec(vector_size=2000, window=10, min_count=1, workers=5)\n\n    def extract_features(self):\n        return list(self.model.wv.key_to_index.keys())\n\n    def fit(self, documents: List[str], labels=None) -> None:\n        tagged_data = [TaggedDocument(words=_d.split(), \n            tags=[str(i)]) for i, _d in enumerate(documents)]\n\n        self.model.build_vocab(tagged_data)\n        self.model.train(tagged_data, total_examples=self.model.corpus_count, epochs=self.model.epochs)\n\n    def transform(self, documents: List[str]) -> List[IVector]:\n        vectors = [self.model.infer_vector(doc.split()) for doc in documents]\n        return [SimpleVector(vector) for vector in vectors]\n\n    def fit_transform(self, documents: List[Union[str, Any]], **kwargs) -> List[IVector]:\n        \"\"\"\n        Fine-tunes the MLM and generates embeddings for the provided documents.\n        \"\"\"\n        self.fit(documents, **kwargs)\n        return self.transform(documents)\n\n    def infer_vector(self, data: str) -> IVector:\n        vector = self.model.infer_vector(data.split())\n        return SimpleVector(vector.squeeze().tolist())\n```"
    },
    {
        "document_name": "swarmauri/standard/vectorizers/concrete/MLMVectorizer.py",
        "content": "```swarmauri/standard/vectorizers/concrete/MLMVectorizer.py\nfrom typing import List, Union, Any\nimport numpy as np\n\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torch.optim import AdamW\nfrom transformers import AutoModelForMaskedLM, AutoTokenizer\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset\n\nfrom swarmauri.core.vectorizers.IVectorize import IVectorize\nfrom swarmauri.core.vectorizers.IFeature import IFeature\nfrom swarmauri.core.vectors.IVector import IVector\nfrom swarmauri.standard.vectors.concrete.SimpleVector import SimpleVector\n\n\nclass MLMVectorizer(IVectorize, IFeature):\n    \"\"\"\n    IVectorize implementation that fine-tunes a Masked Language Model (MLM).\n    \"\"\"\n\n    def __init__(self, model_name='bert-base-uncased', \n        batch_size = 32, \n        learning_rate = 5e-5, \n        masking_ratio: float = 0.15, \n        randomness_ratio: float = 0.10):\n        \"\"\"\n        Initializes the vectorizer with a pre-trained MLM model and tokenizer for fine-tuning.\n        \n        Parameters:\n        - model_name (str): Identifier for the pre-trained model and tokenizer.\n        \"\"\"\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForMaskedLM.from_pretrained(model_name)\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.model.to(self.device)\n        self.epochs = 0\n        self.batch_size = batch_size\n        self.learning_rate = learning_rate\n        self.masking_ratio = masking_ratio\n        self.randomness_ratio = randomness_ratio\n        self.mask_token_id = self.tokenizer.convert_tokens_to_ids([self.tokenizer.mask_token])[0]\n\n    def extract_features(self):\n        raise NotImplementedError('Extract_features not implemented on MLMVectorizer.')\n\n    def _mask_tokens(self, encodings):\n        input_ids = encodings.input_ids.to(self.device)\n        attention_mask = encodings.attention_mask.to(self.device)\n\n        labels = input_ids.detach().clone()\n\n        probability_matrix = torch.full(labels.shape, self.masking_ratio, device=self.device)\n        special_tokens_mask = [\n            self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n        ]\n        probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool, device=self.device), value=0.0)\n        masked_indices = torch.bernoulli(probability_matrix).bool()\n\n        labels[~masked_indices] = -100\n        \n        indices_replaced = torch.bernoulli(torch.full(labels.shape, self.masking_ratio, device=self.device)).bool() & masked_indices\n        input_ids[indices_replaced] = self.mask_token_id\n\n        indices_random = torch.bernoulli(torch.full(labels.shape, self.randomness_ratio, device=self.device)).bool() & masked_indices & ~indices_replaced\n        random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long, device=self.device)\n        input_ids[indices_random] = random_words[indices_random]\n\n        return input_ids, attention_mask, labels\n\n    # work on this\n    def fit(self, documents: List[Union[str, Any]]):\n        encodings = self.tokenizer(documents, return_tensors='pt', padding=True, truncation=True, max_length=512)\n        input_ids, attention_mask, labels = self._mask_tokens(encodings)       \n        optimizer = AdamW(self.model.parameters(), lr=self.learning_rate)\n        dataset = TensorDataset(input_ids, attention_mask, labels)\n        data_loader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n\n        self.model.train()\n\n        for batch in data_loader:\n            # Move batch to the correct device\n            batch = {k: v.to(self.device) for k, v in zip(['input_ids', 'attention_mask', 'labels'], batch)}\n            \n            outputs = self.model(**batch)\n            loss = outputs.loss\n\n            # Backpropagation\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        self.epochs += 1\n        print(f\"Epoch {self.epochs} complete. Loss {loss.item()}\")\n\n\n    def transform(self, documents: List[Union[str, Any]]) -> List[IVector]:\n        \"\"\"\n        Generates embeddings for a list of documents using the fine-tuned MLM.\n        \"\"\"\n        embedding_list = []\n        \n        for document in documents:\n            inputs = self.tokenizer(document, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n            with torch.no_grad():\n                outputs = self.model(**inputs)\n            # Extract embedding (for simplicity, averaging the last hidden states)\n            if hasattr(outputs, 'last_hidden_state'):\n                embedding = outputs.last_hidden_state.mean(1)\n            else:\n                # Fallback or corrected attribute access\n                embedding = outputs['logits'].mean(1)\n            embedding = embedding.cpu().numpy()\n            embedding_list.append(SimpleVector(embedding.squeeze().tolist()))\n\n        return embedding_list\n\n    def fit_transform(self, documents: List[Union[str, Any]], **kwargs) -> List[IVector]:\n        \"\"\"\n        Fine-tunes the MLM and generates embeddings for the provided documents.\n        \"\"\"\n        self.fit(documents, **kwargs)\n        return self.transform(documents)\n\n    def infer_vector(self, data: Union[str, Any], *args, **kwargs) -> IVector:\n        \"\"\"\n        Generates an embedding for the input data.\n\n        Parameters:\n        - data (Union[str, Any]): The input data, expected to be a textual representation.\n                                  Could be a single string or a batch of strings.\n        \"\"\"\n        # Tokenize the input data and ensure the tensors are on the correct device.\n        inputs = self.tokenizer(data, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n\n        # Generate embeddings using the model\n        with torch.no_grad():\n            outputs = self.model(**inputs)\n\n        if hasattr(outputs, 'last_hidden_state'):\n            # Access the last layer and calculate the mean across all tokens (simple pooling)\n            embedding = outputs.last_hidden_state.mean(dim=1)\n        else:\n            embedding = outputs['logits'].mean(1)\n        # Move the embeddings back to CPU for compatibility with downstream tasks if necessary\n        embedding = embedding.cpu().numpy()\n\n        return SimpleVector(embedding.squeeze().tolist())\n\n```"
    },
    {
        "document_name": "swarmauri/standard/vectorizers/concrete/TFIDFVectorizer.py",
        "content": "```swarmauri/standard/vectorizers/concrete/TFIDFVectorizer.py\nfrom sklearn.feature_extraction.text import TfidfVectorizer as SklearnTfidfVectorizer\nfrom typing import List, Union, Any\nfrom swarmauri.core.vectorizers.IVectorize import IVectorize\nfrom swarmauri.core.vectorizers.IFeature import IFeature\nfrom swarmauri.core.vectors.IVector import IVector\nfrom swarmauri.standard.vectors.concrete.SimpleVector import SimpleVector\n\nclass TFIDFVectorizer(IVectorize, IFeature):\n    def __init__(self):\n        # Using scikit-learn's TfidfVectorizer as the underlying mechanism\n        self.model = SklearnTfidfVectorizer()\n        super().__init__()\n        \n    def extract_features(self):\n        return self.model.get_feature_names_out()\n\n    def fit(self, data: Union[str, Any]) -> List[IVector]:\n        \"\"\"\n        Vectorizes the input data using the TF-IDF scheme.\n\n        Parameters:\n        - data (Union[str, Any]): The input data to be vectorized. Expected to be a single string (document)\n                                  or a list of strings (corpus).\n\n        Returns:\n        - List[IVector]: A list containing IVector instances, each representing a document's TF-IDF vector.\n        \"\"\"\n        if isinstance(data, str):\n            data = [data]  # Convert a single string into a list for the vectorizer\n        \n        self.fit_matrix = self.model.fit_transform(data)\n\n        # Convert the sparse matrix rows into SimpleVector instances\n        vectors = [SimpleVector(vector.toarray().flatten()) for vector in self.fit_matrix]\n\n        return vectors\n\n    def fit_transform(self, data: Union[str, Any], documents) -> List[IVector]:\n        documents = [doc.content for doc in documents]\n        if isinstance(data, str):\n            data = [data]  # Convert a single string into a list for the vectorizer\n        documents.extend(data)\n\n        transform_matrix = self.model.fit_transform(documents)\n\n        # Convert the sparse matrix rows into SimpleVector instances\n        vectors = [SimpleVector(vector.toarray().flatten()) for vector in transform_matrix]\n        return vectors\n    \n    def transform(self, data: Union[str, Any], documents) -> List[IVector]:\n        raise NotImplementedError('Transform not implemented on TFIDFVectorizer.')\n\n    def infer_vector(self, data: str, documents) -> IVector:\n        documents = [doc.content for doc in documents]\n        documents.append(data)\n        tmp_tfidf_matrix = self.transform(documents)\n        query_vector = tmp_tfidf_matrix[-1]\n        return query_vector\n```"
    },
    {
        "document_name": "swarmauri/standard/tracing/__init__.py",
        "content": "```swarmauri/standard/tracing/__init__.py\n#\n```"
    },
    {
        "document_name": "swarmauri/standard/tracing/base/__init__.py",
        "content": "```swarmauri/standard/tracing/base/__init__.py\n#\n```"
    },
    {
        "document_name": "swarmauri/standard/tracing/concrete/SimpleTracer.py",
        "content": "```swarmauri/standard/tracing/concrete/SimpleTracer.py\nfrom datetime import datetime\nimport uuid\nfrom typing import Dict, Any, Optional\n\nfrom swarmauri.core.tracing.ITracer import ITracer\nfrom swarmauri.standard.tracing.concrete.SimpleTraceContext import SimpleTraceContext\n\nclass SimpleTracer(ITracer):\n    _instance = None  # Singleton instance placeholder\n\n    @classmethod\n    def instance(cls):\n        if cls._instance is None:\n            cls._instance = cls()\n        return cls._instance\n\n    def __init__(self):\n        if SimpleTracer._instance is not None:\n            raise RuntimeError(\"SimpleTracer is a singleton. Use SimpleTracer.instance().\")\n        self.trace_stack = []\n\n    def start_trace(self, name: str, initial_attributes: Optional[Dict[str, Any]] = None) -> SimpleTraceContext:\n        trace_id = str(uuid.uuid4())\n        trace_context = SimpleTraceContext(trace_id, name, initial_attributes)\n        self.trace_stack.append(trace_context)\n        return trace_context\n\n    def end_trace(self):\n        if self.trace_stack:\n            completed_trace = self.trace_stack.pop()\n            completed_trace.close()\n            # Example of simply printing the completed trace; in practice, you might log it or store it elsewhere\n            print(f\"Trace Completed: {completed_trace.name}, Duration: {completed_trace.start_time} to {completed_trace.end_time}, Attributes: {completed_trace.attributes}\")\n\n    def annotate_trace(self, key: str, value: Any):\n        if not self.trace_stack:\n            print(\"No active trace to annotate.\")\n            return\n        current_trace = self.trace_stack[-1]\n        current_trace.add_attribute(key, value)\n```"
    },
    {
        "document_name": "swarmauri/standard/tracing/concrete/TracedVariable.py",
        "content": "```swarmauri/standard/tracing/concrete/TracedVariable.py\nfrom typing import Any\nfrom swarmauri.standard.tracing.concrete.SimpleTracer import SimpleTracer  # Assuming this is the path to the tracer\n\nclass TracedVariable:\n    \"\"\"\n    Wrapper class to trace multiple changes to a variable within the context manager.\n    \"\"\"\n    def __init__(self, name: str, value: Any, tracer: SimpleTracer):\n        self.name = name\n        self._value = value\n        self._tracer = tracer\n        self._changes = []  # Initialize an empty list to track changes\n\n    @property\n    def value(self) -> Any:\n        return self._value\n\n    @value.setter\n    def value(self, new_value: Any):\n        # Record the change before updating the variable's value\n        change_annotation = {\"from\": self._value, \"to\": new_value}\n        self._changes.append(change_annotation)\n        \n        # Update the trace by appending the latest change to the list under a single key\n        # Note that the key is now constant and does not change with each update\n        self._tracer.annotate_trace(key=f\"{self.name}_changes\", value=self._changes)\n        \n        self._value = new_value\n```"
    },
    {
        "document_name": "swarmauri/standard/tracing/concrete/ChainTracer.py",
        "content": "```swarmauri/standard/tracing/concrete/ChainTracer.py\nfrom swarmauri.core.tracing.IChainTracer import IChainTracer\nfrom typing import Callable, List, Tuple, Dict, Any   \n        \nclass ChainTracer(IChainTracer):\n    def __init__(self):\n        self.traces = []\n\n    def process_chain(self, chain: List[Tuple[Callable[..., Any], List[Any], Dict[str, Any]]]) -> \"ChainTracer\":\n        \"\"\"\n        Processes each item in the operation chain by executing the specified external function\n        with its args and kwargs. Logs starting, annotating, and ending the trace based on tuple position.\n\n        Args:\n            chain (List[Tuple[Callable[..., Any], List[Any], Dict[str, Any]]]): A list where each tuple contains:\n                - An external function to execute.\n                - A list of positional arguments for the function.\n                - A dictionary of keyword arguments for the function.\n        \"\"\"\n        for i, (func, args, kwargs) in enumerate(chain):\n            # Infer operation type and log\n            \n            if i == 0:\n                operation = \"Start\"\n                self.start_trace(*args, **kwargs)\n            elif i == len(chain) - 1:\n                operation = \"End\"\n                self.end_trace(*args, **kwargs)\n            else:\n                operation = \"Annotate\"\n                self.annotate_trace(*args, **kwargs)\n                \n            # For the actual external function call\n            result = func(*args, **kwargs)\n            print(f\"Function '{func.__name__}' executed with result: {result}\")\n\n            self.traces.append((operation, func, args, kwargs, result))\n\n        return self\n\n    def start_trace(self, *args, **kwargs) -> None:\n        print(f\"Starting trace with args: {args}, kwargs: {kwargs}\")\n        \n    def annotate_trace(self, *args, **kwargs) -> None:\n        print(f\"Annotating trace with args: {args}, kwargs: {kwargs}\")\n\n    def end_trace(self, *args, **kwargs) -> None:\n        print(f\"Ending trace with args: {args}, kwargs: {kwargs}\")\n\n    def show(self) -> None:\n        for entry in self.traces:\n            print(entry)\n```"
    },
    {
        "document_name": "swarmauri/standard/tracing/concrete/SimpleTraceContext.py",
        "content": "```swarmauri/standard/tracing/concrete/SimpleTraceContext.py\nfrom datetime import datetime\nfrom typing import Dict, Any, Optional\n\nfrom swarmauri.core.tracing.ITraceContext import ITraceContext\n\nclass SimpleTraceContext(ITraceContext):\n    def __init__(self, trace_id: str, name: str, initial_attributes: Optional[Dict[str, Any]] = None):\n        self.trace_id = trace_id\n        self.name = name\n        self.attributes = initial_attributes if initial_attributes else {}\n        self.start_time = datetime.now()\n        self.end_time = None\n\n    def get_trace_id(self) -> str:\n        return self.trace_id\n\n    def add_attribute(self, key: str, value: Any):\n        self.attributes[key] = value\n\n    def close(self):\n        self.end_time = datetime.now()\n```"
    },
    {
        "document_name": "swarmauri/standard/tracing/concrete/VariableTracer.py",
        "content": "```swarmauri/standard/tracing/concrete/VariableTracer.py\nfrom contextlib import contextmanager\n\nfrom swarmauri.standard.tracing.concrete.TracedVariable import TracedVariable\nfrom swarmauri.standard.tracing.concrete.SimpleTracer import SimpleTracer\n\n# Initialize a global instance of SimpleTracer for use across the application\nglobal_tracer = SimpleTracer()\n\n@contextmanager\ndef VariableTracer(name: str, initial_value=None):\n    \"\"\"\n    Context manager for tracing the declaration, modification, and usage of a variable.\n    \"\"\"\n    trace_context = global_tracer.start_trace(name=f\"Variable: {name}\", initial_attributes={\"initial_value\": initial_value})\n    traced_variable = TracedVariable(name, initial_value, global_tracer)\n    \n    try:\n        yield traced_variable\n    finally:\n        # Optionally record any final value or state of the variable before it goes out of scope\n        global_tracer.annotate_trace(key=f\"{name}_final\", value={\"final_value\": traced_variable.value})\n        # End the trace, marking the variable's lifecycle\n        global_tracer.end_trace()\n```"
    },
    {
        "document_name": "swarmauri/standard/tracing/concrete/CallableTracer.py",
        "content": "```swarmauri/standard/tracing/concrete/CallableTracer.py\nimport functools\nfrom swarmauri.standard.tracing.concrete.SimpleTracer import SimpleTracer  # Import SimpleTracer from the previously defined path\n\n# Initialize the global tracer object\ntracer = SimpleTracer()\n\ndef CallableTracer(func):\n    \"\"\"\n    A decorator to trace function or method calls, capturing inputs, outputs, and the caller.\n    \"\"\"\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        # Trying to automatically identify the caller details; practical implementations \n        # might need to adjust based on specific requirements or environment.\n        caller_info = f\"{func.__module__}.{func.__name__}\"\n        \n        # Start a new trace context for this callable\n        trace_context = tracer.start_trace(name=caller_info, initial_attributes={'args': args, 'kwargs': kwargs})\n        \n        try:\n            # Call the actual function/method\n            result = func(*args, **kwargs)\n            tracer.annotate_trace(key=\"result\", value=result)\n            return result\n        except Exception as e:\n            # Optionally annotate the trace with the exception details\n            tracer.annotate_trace(key=\"exception\", value=str(e))\n            raise  # Re-raise the exception to not interfere with the program's flow\n        finally:\n            # End the trace after the function call is complete\n            tracer.end_trace()\n    return wrapper\n```"
    },
    {
        "document_name": "swarmauri/standard/tracing/concrete/__init__.py",
        "content": "```swarmauri/standard/tracing/concrete/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/chains/__init__.py",
        "content": "```swarmauri/standard/chains/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/chains/base/__init__.py",
        "content": "```swarmauri/standard/chains/base/__init__.py\n#\n```"
    },
    {
        "document_name": "swarmauri/standard/chains/base/ChainBase.py",
        "content": "```swarmauri/standard/chains/base/ChainBase.py\nfrom typing import List\nfrom swarmauri.core.chains.IChain import IChain\nfrom swarmauri.core.chains.IChainStep import IChainStep\n\nclass ChainBase(IChain):\n    \"\"\"\n    A base implementation of the IChain interface.\n    \"\"\"\n\n    def __init__(self, \n                 steps: List[IChainStep] = None,\n                 **configs):\n        self.steps = steps if steps is not None else []\n        self.configs = configs\n\n    def add_step(self, step: IChainStep) -> None:\n        self.steps.append(step)\n\n    def remove_step(self, step: IChainStep) -> None:\n        \"\"\"\n        Removes an existing step from the chain. This alters the chain's execution sequence\n        by excluding the specified step from subsequent executions of the chain.\n\n        Parameters:\n            step (IChainStep): The Callable representing the step to remove from the chain.\n        \"\"\"\n\n        raise NotImplementedError('this is not yet impplemented')\n\n    def execute(self, *args, **kwargs) -> Any:\n        raise NotImplementedError('this is not yet impplemented')\n\n    def get_schema_info(self) -> Dict[str, Any]:\n        # Return a serialized version of the Chain instance's configuration\n        return {\n            \"steps\": [str(step) for step in self.steps],\n            \"configs\": self.configs\n        }\n```"
    },
    {
        "document_name": "swarmauri/standard/chains/base/ChainStepBase.py",
        "content": "```swarmauri/standard/chains/base/ChainStepBase.py\nfrom typing import Any, Callable, List, Dict\nfrom swarmauri.core.chains.IChainStep import IChainStep\n\nclass ChainStepBase(IChainStep):\n    \"\"\"\n    Represents a single step within an execution chain.\n    \"\"\"\n    \n    def __init__(self, key: str, method: Callable, args: List[Any] = None, kwargs: Dict[str, Any] = None, ref: str = None):\n        \"\"\"\n        Initialize a chain step.\n\n        Args:\n            key (str): Unique key or identifier for the step.\n            method (Callable): The callable object (function or method) to execute in this step.\n            args (List[Any], optional): Positional arguments for the callable.\n            kwargs (Dict[str, Any], optional): Keyword arguments for the callable.\n            ref (str, optional): Reference to another component or context variable, if applicable.\n        \"\"\"\n        self.key = key\n        self.method = method\n        self.args = args if args is not None else []\n        self.kwargs = kwargs if kwargs is not None else {}\n        self.ref = ref\n        \n\n```"
    },
    {
        "document_name": "swarmauri/standard/chains/concrete/__init__.py",
        "content": "```swarmauri/standard/chains/concrete/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/chains/concrete/CallableChain.py",
        "content": "```swarmauri/standard/chains/concrete/CallableChain.py\nfrom typing import Any, Callable, List, Dict, Optional\nfrom swarmauri.core.chains.ICallableChain import ICallableChain, CallableDefinition\n\n\nclass CallableChain(ICallableChain):\n    def __init__(self, callables: Optional[List[CallableDefinition]] = None):\n        \n        self.callables = callables if callables is not None else []\n\n    def __call__(self, *initial_args, **initial_kwargs):\n        result = None\n        for func, args, kwargs in self.callables:\n            if result is not None:\n                # If there was a previous result, use it as the first argument for the next function\n                args = [result] + list(args)\n            result = func(*args, **kwargs)\n        return result\n    \n    def add_callable(self, func: Callable[[Any], Any], args: List[Any] = None, kwargs: Dict[str, Any] = None) -> None:\n        # Add a new callable to the chain\n        self.callables.append((func, args or [], kwargs or {}))\n    \n    def __or__(self, other: \"CallableChain\") -> \"CallableChain\":\n        if not isinstance(other, CallableChain):\n            raise TypeError(\"Operand must be an instance of CallableChain\")\n        \n        new_chain = CallableChain(self.callables + other.callables)\n        return new_chain\n```"
    },
    {
        "document_name": "swarmauri/standard/distances/__init__.py",
        "content": "```swarmauri/standard/distances/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/distances/base/__init__.py",
        "content": "```swarmauri/standard/distances/base/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/standard/distances/concrete/ChiSquaredDistance.py",
        "content": "```swarmauri/standard/distances/concrete/ChiSquaredDistance.py\nfrom typing import List\nfrom swarmauri.core.distances.IDistanceSimilarity import IDistanceSimilarity\nfrom swarmauri.core.vectors.IVector import IVector\n\nclass ChiSquaredDistance(IDistanceSimilarity):\n    \"\"\"\n    Implementation of the IDistanceSimilarity interface using Chi-squared distance metric.\n    \"\"\"\n\n    def distance(self, vector_a: IVector, vector_b: IVector) -> float:\n        \"\"\"\n        Computes the Chi-squared distance between two vectors.\n        \"\"\"\n        if len(vector_a.data) != len(vector_b.data):\n            raise ValueError(\"Vectors must have the same dimensionality.\")\n\n        chi_squared_distance = 0\n        for a, b in zip(vector_a.data, vector_b.data):\n            if (a + b) != 0:\n                chi_squared_distance += (a - b) ** 2 / (a + b)\n\n        return 0.5 * chi_squared_distance\n\n    def similarity(self, vector_a: IVector, vector_b: IVector) -> float:\n        \"\"\"\n        Compute the similarity between two vectors based on the Chi-squared distance.\n        \"\"\"\n        return 1 / (1 + self.distance(vector_a, vector_b))\n    \n    def distances(self, vector_a: IVector, vectors_b: List[IVector]) -> List[float]:\n        distances = [self.distance(vector_a, vector_b) for vector_b in vectors_b]\n        return distances\n    \n    def similarities(self, vector_a: IVector, vectors_b: List[IVector]) -> List[float]:\n        similarities = [self.similarity(vector_a, vector_b) for vector_b in vectors_b]\n        return similarities\n```"
    },
    {
        "document_name": "swarmauri/standard/distances/concrete/CosineDistance.py",
        "content": "```swarmauri/standard/distances/concrete/CosineDistance.py\nfrom numpy.linalg import norm\nfrom typing import List\nfrom swarmauri.core.distances.IDistanceSimilarity import IDistanceSimilarity\nfrom swarmauri.core.vectors.IVector import IVector\nfrom swarmauri.standard.vectors.concrete.VectorProduct import VectorProduct\n\nclass CosineDistance(IDistanceSimilarity, VectorProduct):\n    \"\"\"\n    Implements cosine distance calculation as an IDistanceSimiliarity interface.\n    Cosine distance measures the cosine of the angle between two non-zero vectors\n    of an inner product space, capturing the orientation rather than the magnitude \n    of these vectors.\n    \"\"\"\n       \n    def distance(self, vector_a: IVector, vector_b: IVector) -> float:\n        \"\"\" \n        Computes the cosine distance between two vectors: 1 - cosine similarity.\n    \n        Args:\n            vector_a (IVector): The first vector in the comparison.\n            vector_b (IVector): The second vector in the comparison.\n    \n        Returns:\n            float: The computed cosine distance between vector_a and vector_b.\n                   It ranges from 0 (completely similar) to 2 (completely dissimilar).\n        \"\"\"\n        norm_a = norm(vector_a.data)\n        norm_b = norm(vector_b.data)\n    \n        # Check if either of the vector norms is close to zero\n        if norm_a < 1e-10 or norm_b < 1e-10:\n            return 1.0  # Return maximum distance for cosine which varies between -1 to 1, so 1 indicates complete dissimilarity\n    \n        # Compute the cosine similarity between the vectors\n        cos_sim = self.dot_product(vector_a, vector_b) / (norm_a * norm_b)\n    \n        # Covert cosine similarity to cosine distance\n        cos_distance = 1 - cos_sim\n    \n        return cos_distance\n    \n    def similarity(self, vector_a: IVector, vector_b: IVector) -> float:\n        \"\"\"\n        Computes the cosine similarity between two vectors.\n\n        Args:\n            vector_a (IVector): The first vector in the comparison.\n            vector_b (IVector): The second vector in the comparison.\n\n        Returns:\n            float: The cosine similarity between vector_a and vector_b.\n        \"\"\"\n        return 1 - self.distance(vector_a, vector_b)\n\n    def distances(self, vector_a: IVector, vectors_b: List[IVector]) -> List[float]:\n        distances = [self.distance(vector_a, vector_b) for vector_b in vectors_b]\n        return distances\n    \n    def similarities(self, vector_a: IVector, vectors_b: List[IVector]) -> List[float]:\n        similarities = [self.similarity(vector_a, vector_b) for vector_b in vectors_b]\n        return similarities\n```"
    },
    {
        "document_name": "swarmauri/standard/distances/concrete/EuclideanDistance.py",
        "content": "```swarmauri/standard/distances/concrete/EuclideanDistance.py\nfrom math import sqrt\nfrom typing import List\nfrom swarmauri.core.distances.IDistanceSimilarity import IDistanceSimilarity\nfrom swarmauri.core.vectors.IVector import IVector\n\n\nclass EuclideanDistance(IDistanceSimilarity):\n    \"\"\"\n    Class to compute the Euclidean distance between two vectors.\n    Implements the IDistanceSimiliarity interface.\n    \"\"\"\n\n    def distance(self, vector_a: IVector, vector_b: IVector) -> float:\n        \"\"\"\n        Computes the Euclidean distance between two vectors.\n\n        Args:\n            vector_a (IVector): The first vector in the comparison.\n            vector_b (IVector): The second vector in the comparison.\n\n        Returns:\n            float: The computed Euclidean distance between vector_a and vector_b.\n        \"\"\"\n        if len(vector_a.data) != len(vector_b.data):\n            raise ValueError(\"Vectors do not have the same dimensionality.\")\n        \n        distance = sqrt(sum((a - b) ** 2 for a, b in zip(vector_a.data, vector_b.data)))\n        return distance\n\n    def similarity(self, vector_a: IVector, vector_b: IVector) -> float:\n        \"\"\"\n        Computes the similarity score as the inverse of the Euclidean distance between two vectors.\n\n        Args:\n            vector_a (IVector): The first vector in the comparison.\n            vector_b (IVector): The second vector in the comparison.\n\n        Returns:\n            float: The similarity score between vector_a and vector_b.\n        \"\"\"\n        distance = self.distance(vector_a, vector_b)\n        return 1 / (1 + distance)\n    \n    def distances(self, vector_a: IVector, vectors_b: List[IVector]) -> List[float]:\n        distances = [self.distance(vector_a, vector_b) for vector_b in vectors_b]\n        return distances\n    \n    def similarities(self, vector_a: IVector, vectors_b: List[IVector]) -> List[float]:\n        similarities = [self.similarity(vector_a, vector_b) for vector_b in vectors_b]\n        return similarities\n```"
    },
    {
        "document_name": "swarmauri/standard/distances/concrete/JaccardIndexDistance.py",
        "content": "```swarmauri/standard/distances/concrete/JaccardIndexDistance.py\nfrom typing import List\nfrom swarmauri.core.distances.IDistanceSimilarity import IDistanceSimilarity\nfrom swarmauri.core.vectors.IVector import IVector\n\nclass JaccardIndexDistance(IDistanceSimilarity):\n    \"\"\"\n    A class implementing Jaccard Index as a similarity and distance metric between two vectors.\n    \"\"\"\n\n    def distance(self, vector_a: IVector, vector_b: IVector) -> float:\n        \"\"\"\n        Computes the Jaccard distance between two vectors.\n\n        The Jaccard distance, which is 1 minus the Jaccard similarity,\n        measures dissimilarity between sample sets. It's defined as\n        1 - (the intersection of the sets divided by the union of the sets).\n\n        Args:\n            vector_a (IVector): The first vector.\n            vector_b (IVector): The second vector.\n\n        Returns:\n            float: The Jaccard distance between vector_a and vector_b.\n        \"\"\"\n        set_a = set(vector_a.data)\n        set_b = set(vector_b.data)\n\n        # Calculate the intersection and union of the two sets.\n        intersection = len(set_a.intersection(set_b))\n        union = len(set_a.union(set_b))\n\n        # In the special case where the union is zero, return 1.0 which implies complete dissimilarity.\n        if union == 0:\n            return 1.0\n\n        # Compute Jaccard similarity and then return the distance as 1 - similarity.\n        jaccard_similarity = intersection / union\n        return 1 - jaccard_similarity\n\n    def similarity(self, vector_a: IVector, vector_b: IVector) -> float:\n        \"\"\"\n        Computes the Jaccard similarity between two vectors.\n\n        Args:\n            vector_a (IVector): The first vector.\n            vector_b (IVector): The second vector.\n\n        Returns:\n            float: Jaccard similarity score between vector_a and vector_b.\n        \"\"\"\n        set_a = set(vector_a.data)\n        set_b = set(vector_b.data)\n\n        # Calculate the intersection and union of the two sets.\n        intersection = len(set_a.intersection(set_b))\n        union = len(set_a.union(set_b))\n\n        # In case the union is zero, which means both vectors have no elements, return 1.0 implying identical sets.\n        if union == 0:\n            return 1.0\n\n        # Compute and return Jaccard similarity.\n        return intersection / union\n    \n    def distances(self, vector_a: IVector, vectors_b: List[IVector]) -> List[float]:\n        distances = [self.distance(vector_a, vector_b) for vector_b in vectors_b]\n        return distances\n    \n    def similarities(self, vector_a: IVector, vectors_b: List[IVector]) -> List[float]:\n        similarities = [self.similarity(vector_a, vector_b) for vector_b in vectors_b]\n        return similarities\n```"
    },
    {
        "document_name": "swarmauri/standard/distances/concrete/LevenshteinDistance.py",
        "content": "```swarmauri/standard/distances/concrete/LevenshteinDistance.py\nfrom typing import List\nimport numpy as np\nfrom swarmauri.core.distances.IDistanceSimilarity import IDistanceSimilarity\nfrom swarmauri.core.vectors.IVector import IVector\n\nclass LevenshteinDistance(IDistanceSimilarity):\n    \"\"\"\n    Implements the IDistance interface to calculate the Levenshtein distance between two vectors.\n    The Levenshtein distance between two strings is given by the minimum number of operations needed to transform\n    one string into the other, where an operation is an insertion, deletion, or substitution of a single character.\n    \"\"\"\n    \n    def distance(self, vector_a: IVector, vector_b: IVector) -> float:\n        \"\"\"\n        Compute the Levenshtein distance between two vectors.\n\n        Note: Since Levenshtein distance is typically calculated between strings,\n        it is assumed that the vectors represent strings where each element of the\n        vector corresponds to the ASCII value of a character in the string.\n\n        Args:\n            vector_a (List[float]): The first vector in the comparison.\n            vector_b (List[float]): The second vector in the comparison.\n\n        Returns:\n           float: The computed Levenshtein distance between vector_a and vector_b.\n        \"\"\"\n        string_a = ''.join([chr(int(round(value))) for value in vector_a.data])\n        string_b = ''.join([chr(int(round(value))) for value in vector_b.data])\n        \n        return self.levenshtein(string_a, string_b)\n    \n    def levenshtein(self, seq1: str, seq2: str) -> float:\n        \"\"\"\n        Calculate the Levenshtein distance between two strings.\n        \n        Args:\n            seq1 (str): The first string.\n            seq2 (str): The second string.\n        \n        Returns:\n            float: The Levenshtein distance between seq1 and seq2.\n        \"\"\"\n        size_x = len(seq1) + 1\n        size_y = len(seq2) + 1\n        matrix = np.zeros((size_x, size_y))\n        \n        for x in range(size_x):\n            matrix[x, 0] = x\n        for y in range(size_y):\n            matrix[0, y] = y\n\n        for x in range(1, size_x):\n            for y in range(1, size_y):\n                if seq1[x-1] == seq2[y-1]:\n                    matrix[x, y] = min(matrix[x-1, y] + 1, matrix[x-1, y-1], matrix[x, y-1] + 1)\n                else:\n                    matrix[x, y] = min(matrix[x-1, y] + 1, matrix[x-1, y-1] + 1, matrix[x, y-1] + 1)\n        \n        return matrix[size_x - 1, size_y - 1]\n    \n    def similarity(self, vector_a: IVector, vector_b: IVector) -> float:\n        string_a = ''.join([chr(int(round(value))) for value in vector_a.data])\n        string_b = ''.join([chr(int(round(value))) for value in vector_b.data])\n        return 1 - self.levenshtein(string_a, string_b) / max(len(vector_a), len(vector_b))\n    \n    def distances(self, vector_a: IVector, vectors_b: List[IVector]) -> List[float]:\n        distances = [self.distance(vector_a, vector_b) for vector_b in vectors_b]\n        return distances\n    \n    def similarities(self, vector_a: IVector, vectors_b: List[IVector]) -> List[float]:\n        similarities = [self.similarity(vector_a, vector_b) for vector_b in vectors_b]\n        return similarities\n```"
    },
    {
        "document_name": "swarmauri/standard/distances/concrete/__init__.py",
        "content": "```swarmauri/standard/distances/concrete/__init__.py\n\n```"
    }
]