[
    {
        "document_name": "swarmauri/experimental/__init__.py",
        "content": "```swarmauri/experimental/__init__.py\n# -*- coding: utf-8 -*-\n\n\n```"
    },
    {
        "document_name": "swarmauri/experimental/tools/LinkedInArticleTool.py",
        "content": "```swarmauri/experimental/tools/LinkedInArticleTool.py\nimport requests\nfrom ...standard.tools.base.ToolBase import ToolBase\nfrom ...standard.tools.concrete.Parameter import Parameter\n\nclass LinkedInArticleTool(ToolBase):\n    \"\"\"\n    A tool to post articles on LinkedIn using the LinkedIn API.\n    \"\"\"\n    def __init__(self, access_token):\n        \"\"\"\n        Initializes the LinkedInArticleTool with the necessary access token.\n        \n        Args:\n            access_token (str): The OAuth access token for authenticating with the LinkedIn API.\n        \"\"\"\n        super().__init__(name=\"LinkedInArticleTool\",\n                         description=\"A tool for posting articles on LinkedIn.\",\n                         parameters=[\n                             Parameter(name=\"title\", type=\"string\", description=\"The title of the article\", required=True),\n                             Parameter(name=\"text\", type=\"string\", description=\"The body text of the article\", required=True),\n                             Parameter(name=\"visibility\", type=\"string\", description=\"The visibility of the article\", required=True, enum=[\"anyone\", \"connectionsOnly\"])\n                         ])\n        self.access_token = access_token\n        \n    def __call__(self, title: str, text: str, visibility: str = \"anyone\") -> str:\n        \"\"\"\n        Posts an article on LinkedIn.\n\n        Args:\n            title (str): The title of the article.\n            text (str): The body text of the article.\n            visibility (str): The visibility of the article, either \"anyone\" or \"connectionsOnly\".\n\n        Returns:\n            str: A message indicating the success or failure of the post operation.\n        \"\"\"\n        # Construct the request URL and payload according to LinkedIn API documentation\n        url = 'https://api.linkedin.com/v2/ugcPosts'\n        headers = {\n            'Authorization': f'Bearer {self.access_token}',\n            'X-Restli-Protocol-Version': '2.0.0',\n            'Content-Type': 'application/json'\n        }\n        \n        payload = {\n            \"author\": \"urn:li:person:YOUR_PERSON_ID_HERE\",\n            \"lifecycleState\": \"PUBLISHED\",\n            \"specificContent\": {\n                \"com.linkedin.ugc.ShareContent\": {\n                    \"shareCommentary\": {\n                        \"text\": text\n                    },\n                    \"shareMediaCategory\": \"ARTICLE\",\n                    \"media\": [\n                        {\n                            \"status\": \"READY\",\n                            \"description\": {\n                                \"text\": title\n                            },\n                            \"originalUrl\": \"URL_OF_THE_ARTICLE_OR_IMAGE\",\n                            \"visibility\": {\n                                \"com.linkedin.ugc.MemberNetworkVisibility\": visibility.upper()\n                            }\n                        }\n                    ]\n                }\n            },\n            \"visibility\": {\n                \"com.linkedin.ugc.MemberNetworkVisibility\": visibility.upper()\n            }\n        }\n     \n        # Make the POST request to LinkedIn's API\n        response = requests.post(url, headers=headers, json=payload)\n        \n        if response.status_code == 201:\n            return f\"Article posted successfully: {response.json().get('id')}\"\n        else:\n            return f\"Failed to post the article. Status Code: {response.status_code} - {response.text}\"\n```"
    },
    {
        "document_name": "swarmauri/experimental/tools/TwitterPostTool.py",
        "content": "```swarmauri/experimental/tools/TwitterPostTool.py\nfrom tweepy import Client\n\nfrom ...standard.tools.base.ToolBase import ToolBase\nfrom ...standard.tools.concrete.Parameter import Parameter\n\nclass TwitterPostTool(ToolBase):\n    def __init__(self, bearer_token):\n        # Initialize parameters necessary for posting a tweet\n        parameters = [\n            Parameter(\n                name=\"status\",\n                type=\"string\",\n                description=\"The status message to post on Twitter\",\n                required=True\n            )\n        ]\n        \n        super().__init__(name=\"TwitterPostTool\", description=\"Post a status update on Twitter\", parameters=parameters)\n        \n        # Initialize Twitter API Client\n        self.client = Client(bearer_token=bearer_token)\n\n    def __call__(self, status: str) -> str:\n        \"\"\"\n        Posts a status on Twitter.\n\n        Args:\n            status (str): The status message to post.\n\n        Returns:\n            str: A confirmation message including the tweet's URL if successful.\n        \"\"\"\n        try:\n            # Using Tweepy to send a tweet\n            response = self.client.create_tweet(text=status)\n            tweet_id = response.data['id']\n            # Constructing URL to the tweet - Adjust the URL to match Twitter API v2 structure if needed\n            tweet_url = f\"https://twitter.com/user/status/{tweet_id}\"\n            return f\"Tweet successful: {tweet_url}\"\n        except Exception as e:\n            return f\"An error occurred: {e}\"\n```"
    },
    {
        "document_name": "swarmauri/experimental/tools/__init__.py",
        "content": "```swarmauri/experimental/tools/__init__.py\n# -*- coding: utf-8 -*-\n\n\n```"
    },
    {
        "document_name": "swarmauri/experimental/tools/OutlookSendMailTool.py",
        "content": "```swarmauri/experimental/tools/OutlookSendMailTool.py\nimport requests\nfrom ....standard.tools.base.ToolBase import ToolBase\nfrom ....standard.tools.concrete.Parameter import Parameter\n\n\nclass OutlookSendMailTool(ToolBase):\n    def __init__(self):\n        parameters = [\n            Parameter(\n                name=\"recipient\",\n                type=\"string\",\n                description=\"The email address of the recipient\",\n                required=True\n            ),\n            Parameter(\n                name=\"subject\",\n                type=\"string\",\n                description=\"The subject of the email\",\n                required=True\n            ),\n            Parameter(\n                name=\"body\",\n                type=\"string\",\n                description=\"The HTML body of the email\",\n                required=True\n            )\n        ]\n        \n        super().__init__(name=\"OutlookSendMailTool\", \n                         description=\"Sends an email using the Outlook service.\",\n                         parameters=parameters)\n\n        # Add your Microsoft Graph API credentials and endpoint URL here\n        self.tenant_id = \"YOUR_TENANT_ID\"\n        self.client_id = \"YOUR_CLIENT_ID\"\n        self.client_secret = \"YOUR_CLIENT_SECRET\"\n        self.scope = [\"https://graph.microsoft.com/.default\"]\n        self.token_url = f\"https://login.microsoftonline.com/{self.tenant_id}/oauth2/v2.0/token\"\n        self.graph_endpoint = \"https://graph.microsoft.com/v1.0\"\n\n    def get_access_token(self):\n        data = {\n            \"client_id\": self.client_id,\n            \"scope\": \" \".join(self.scope),\n            \"client_secret\": self.client_secret,\n            \"grant_type\": \"client_credentials\"\n        }\n        response = requests.post(self.token_url, data=data)\n        response.raise_for_status()\n        return response.json().get(\"access_token\")\n\n    def __call__(self, recipient, subject, body):\n        access_token = self.get_access_token()\n\n        headers = {\n            \"Authorization\": f\"Bearer {access_token}\",\n            \"Content-Type\": \"application/json\"\n        }\n\n        email_data = {\n            \"message\": {\n                \"subject\": subject,\n                \"body\": {\n                    \"contentType\": \"HTML\",\n                    \"content\": body\n                },\n                \"toRecipients\": [\n                    {\n                        \"emailAddress\": {\n                            \"address\": recipient\n                        }\n                    }\n                ]\n            }\n        }\n\n        send_mail_endpoint = f\"{self.graph_endpoint}/users/{self.client_id}/sendMail\"\n        response = requests.post(send_mail_endpoint, json=email_data, headers=headers)\n        if response.status_code == 202:\n            return \"Email sent successfully\"\n        else:\n            return f\"Failed to send email, status code {response.status_code}\"\n```"
    },
    {
        "document_name": "swarmauri/experimental/tools/CypherQueryTool.py",
        "content": "```swarmauri/experimental/tools/CypherQueryTool.py\nfrom ..base.ToolBase import ToolBase\nfrom .Parameter import Parameter\nfrom neo4j import GraphDatabase\nimport json\n\nclass CypherQueryTool(ToolBase):\n    def __init__(self, uri: str, user: str, password: str):\n        self.uri = uri\n        self.user = user\n        self.password = password\n        \n        # Define only the 'query' parameter since uri, user, and password are set at initialization\n        parameters = [\n            Parameter(\n                name=\"query\",\n                type=\"string\",\n                description=\"The Cypher query to execute.\",\n                required=True\n            )\n        ]\n        \n        super().__init__(name=\"CypherQueryTool\",\n                         description=\"Executes a Cypher query against a Neo4j database.\",\n                         parameters=parameters)\n\n    def _get_connection(self):\n        return GraphDatabase.driver(self.uri, auth=(self.user, self.password))\n\n    def __call__(self, query) -> str:\n        # Establish connection to the database\n        driver = self._get_connection()\n        session = driver.session()\n\n        # Execute the query\n        result = session.run(query)\n        records = result.data()\n\n        # Close the connection\n        session.close()\n        driver.close()\n\n        # Convert records to JSON string, assuming it's JSON serializable\n        return json.dumps(records)\n```"
    },
    {
        "document_name": "swarmauri/experimental/tools/FileDownloaderTool.py",
        "content": "```swarmauri/experimental/tools/FileDownloaderTool.py\nimport requests\nfrom ....core.tools.ToolBase import ToolBase\nfrom ....core.tools.Parameter import Parameter\n\n\nclass FileDownloaderTool(ToolBase):\n    def __init__(self):\n        parameters = [\n            Parameter(\n                name=\"url\",\n                type=\"string\",\n                description=\"The URL of the file to download\",\n                required=True\n            )\n        ]\n        \n        super().__init__(name=\"FileDownloaderTool\",\n                         description=\"Downloads a file from a specified URL into memory.\",\n                         parameters=parameters)\n    \n    def __call__(self, url: str) -> bytes:\n        \"\"\"\n        Downloads a file from the given URL into memory.\n        \n        Parameters:\n        - url (str): The URL of the file to download.\n        \n        Returns:\n        - bytes: The content of the downloaded file.\n        \"\"\"\n        try:\n            response = requests.get(url)\n            response.raise_for_status()  # Raises an HTTPError if the request resulted in an error\n            return response.content\n        except requests.RequestException as e:\n            raise RuntimeError(f\"Failed to download file from '{url}'. Error: {e}\")\n```"
    },
    {
        "document_name": "swarmauri/experimental/tools/SQLite3QueryTool.py",
        "content": "```swarmauri/experimental/tools/SQLite3QueryTool.py\nimport sqlite3\nfrom ...base.ToolBase import ToolBase\nfrom ...concrete.Parameter import Parameter\n\nclass SQLite3QueryTool(ToolBase):\n    def __init__(self, db_name: str):\n        parameters = [\n            Parameter(\n                name=\"query\",\n                type=\"string\",\n                description=\"SQL query to execute\",\n                required=True\n            )\n        ]\n        super().__init__(name=\"SQLQueryTool\", \n                         description=\"Executes an SQL query and returns the results.\", \n                         parameters=parameters)\n        self.db_name = db_name\n\n    def __call__(self, query) -> str:\n        \"\"\"\n        Execute the provided SQL query.\n\n        Parameters:\n        - query (str): The SQL query to execute.\n\n        Returns:\n        - str: The results of the SQL query as a string.\n        \"\"\"\n        try:\n            connection = sqlite3.connect(self.db_name)  # Connect to the specific database file\n            cursor = connection.cursor()\n            \n            cursor.execute(query)\n            rows = cursor.fetchall()\n            result = \"\\n\".join(str(row) for row in rows)\n        except Exception as e:\n            result = f\"Error executing query: {e}\"\n        finally:\n            connection.close()\n        \n        return f\"Query Result:\\n{result}\"\n```"
    },
    {
        "document_name": "swarmauri/experimental/conversations/__init__.py",
        "content": "```swarmauri/experimental/conversations/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/experimental/conversations/SemanticConversation.py",
        "content": "```swarmauri/experimental/conversations/SemanticConversation.py\nfrom abc import ABC, abstractmethod\nfrom typing import Callable, Dict, Union\nfrom ...core.messages.IMessage import IMessage\nfrom ...core.conversations.IConversation import IConversation\n\nclass SemanticConversation(IConversation, ABC):\n    \"\"\"\n    A concrete implementation of the Conversation class that includes semantic routing.\n    Semantic routing involves analyzing the content of messages to understand their intent\n    or category and then routing them to appropriate handlers based on that analysis.\n\n    This class requires subclasses to implement the _analyze_message method for semantic analysis.\n    \"\"\"\n\n\n    @abstractmethod\n    def register_handler(self, category: str, handler: Callable[[IMessage], None]):\n        \"\"\"\n        Registers a message handler for a specific semantic category.\n\n        Args:\n            category (str): The category of messages this handler should process.\n            handler (Callable[[Message], None]): The function to call for messages of the specified category.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def add_message(self, message: IMessage):\n        \"\"\"\n        Adds a message to the conversation history and routes it to the appropriate handler based on its semantic category.\n\n        Args:\n            message (Message): The message to be added and processed.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def _analyze_message(self, message: IMessage) -> Union[str, None]:\n        \"\"\"\n        Analyzes the content of a message to determine its semantic category.\n\n        This method must be implemented by subclasses to provide specific logic for semantic analysis.\n\n        Args:\n            message (Message): The message to analyze.\n\n        Returns:\n            Union[str, None]: The semantic category of the message, if determined; otherwise, None.\n\n        Raises:\n            NotImplementedError: If the method is not overridden in a subclass.\n        \"\"\"\n        raise NotImplementedError(\"Subclasses must implement the _analyze_message method to provide semantic analysis.\")\n\n    # Additional methods as needed for message retrieval, history management, etc., inherited from Conversation\n```"
    },
    {
        "document_name": "swarmauri/experimental/conversations/ConsensusBuildingConversation.py",
        "content": "```swarmauri/experimental/conversations/ConsensusBuildingConversation.py\nfrom swarmauri.core.conversations.IConversation import IConversation\nfrom swarmauri.core.messages.IMessage import IMessage\n\n\nclass ConsensusBuildingMessage(IMessage):\n    def __init__(self, sender_id: str, content: str, message_type: str):\n        self._sender_id = sender_id\n        self._content = content\n        self._role = 'consensus_message'\n        self._message_type = message_type\n\n    @property\n    def role(self) -> str:\n        return self._role\n\n    @property\n    def content(self) -> str:\n        return self._content\n\n    def as_dict(self) -> dict:\n        return {\n            \"sender_id\": self._sender_id,\n            \"content\": self._content,\n            \"message_type\": self._message_type\n        }\n\n\nclass ConsensusBuildingConversation(IConversation):\n    def __init__(self, topic: str, participants: list):\n        self.topic = topic\n        self.participants = participants  # List of agent IDs\n        self._history = []  # Stores all messages exchanged in the conversation\n        self.proposal_votes = {}  # Tracks votes for each proposal\n\n    @property\n    def history(self) -> list:\n        return self._history\n\n    def add_message(self, message: IMessage):\n        if not isinstance(message, ConsensusBuildingMessage):\n            raise ValueError(\"Only instances of ConsensusBuildingMessage are accepted\")\n        self._history.append(message)\n\n    def get_last(self) -> IMessage:\n        if self._history:\n            return self._history[-1]\n        return None\n\n    def clear_history(self) -> None:\n        self._history.clear()\n\n    def as_dict(self) -> list:\n        return [message.as_dict() for message in self._history]\n\n    def initiate_consensus(self, initiator_id: str, proposal=None):\n        \"\"\"Starts the conversation with an initial proposal, if any.\"\"\"\n        initiate_message = ConsensusBuildingMessage(initiator_id, proposal, \"InitiateConsensusMessage\")\n        self.add_message(initiate_message)\n\n    def add_proposal(self, sender_id: str, proposal: str):\n        \"\"\"Adds a proposal to the conversation.\"\"\"\n        proposal_message = ConsensusBuildingMessage(sender_id, proposal, \"ProposalMessage\")\n        self.add_message(proposal_message)\n\n    def add_comment(self, sender_id: str, comment: str):\n        \"\"\"Adds a comment or feedback regarding a proposal.\"\"\"\n        comment_message = ConsensusBuildingMessage(sender_id, comment, \"CommentMessage\")\n        self.add_message(comment_message)\n\n    def vote(self, sender_id: str, vote: str):\n        \"\"\"Registers a vote for a given proposal.\"\"\"\n        vote_message = ConsensusBuildingMessage(sender_id, vote, \"VoteMessage\")\n        self.add_message(vote_message)\n        # Count the vote\n        self.proposal_votes[vote] = self.proposal_votes.get(vote, 0) + 1\n\n    def check_agreement(self):\n        \"\"\"\n        Checks if there is a consensus on any proposal.\n        A simple majority (>50% of the participants) is required for consensus.\n        \"\"\"\n        consensus_threshold = len(self.participants) / 2  # Define consensus as a simple majority\n\n        for proposal, votes in self.proposal_votes.items():\n            if votes > consensus_threshold:\n                # A consensus has been reached\n                return True, f\"Consensus reached on proposal: {proposal} with {votes} votes.\"\n\n        # If no consensus is reached\n        return False, \"No consensus reached.\"\n```"
    },
    {
        "document_name": "swarmauri/experimental/models/__init__.py",
        "content": "```swarmauri/experimental/models/__init__.py\n\n```"
    },
    {
        "document_name": "swarmauri/experimental/models/SageMaker.py",
        "content": "```swarmauri/experimental/models/SageMaker.py\nimport json\nimport boto3\nfrom ...core.models.IModel import IModel\n\n\nclass AWSSageMakerModel(IModel):\n    def __init__(self, access_key: str, secret_access_key: str, region_name: str, model_name: str):\n        \"\"\"\n        Initialize the AWS SageMaker model with AWS credentials, region, and the model name.\n\n        Parameters:\n        - access_key (str): AWS access key ID.\n        - secret_access_key (str): AWS secret access key.\n        - region_name (str): The region where the SageMaker model is deployed.\n        - model_name (str): The name of the SageMaker model.\n        \"\"\"\n        self.access_key = access_key\n        self.secret_access_key = secret_access_key\n        self.region_name = region_name\n        self.client = boto3.client('sagemaker-runtime',\n                                   aws_access_key_id=access_key,\n                                   aws_secret_access_key=secret_access_key,\n                                   region_name=region_name)\n        super().__init__(model_name)\n\n    def predict(self, payload: str, content_type: str='application/json') -> dict:\n        \"\"\"\n        Generate predictions using the AWS SageMaker model.\n\n        Parameters:\n        - payload (str): Input data in JSON format.\n        - content_type (str): The MIME type of the input data (default: 'application/json').\n        \n        Returns:\n        - dict: The predictions returned by the model.\n        \"\"\"\n        endpoint_name = self.model_name  # Assuming the model name is also the endpoint name\n        response = self.client.invoke_endpoint(EndpointName=endpoint_name,\n                                               Body=payload,\n                                               ContentType=content_type)\n        result = json.loads(response['Body'].read().decode())\n        return result\n```"
    },
    {
        "document_name": "swarmauri/experimental/models/HierarchicalAttentionModel.py",
        "content": "```swarmauri/experimental/models/HierarchicalAttentionModel.py\nimport tensorflow as tf\nfrom swarmauri.core.models.IModel import IModel\nfrom typing import Any\n\nclass HierarchicalAttentionModel(IModel):\n    def __init__(self, model_name: str):\n        self._model_name = model_name\n        self._model = None  # This will hold the TensorFlow model with attention\n\n    @property\n    def model_name(self) -> str:\n        return self._model_name\n\n    @model_name.setter\n    def model_name(self, value: str) -> None:\n        self._model_name = value\n\n    def load_model(self) -> None:\n        \"\"\"\n        Here, we define and compile the TensorFlow model described earlier.\n        \"\"\"\n        # The following code is adapted from the attention model example provided earlier\n        vocab_size = 10000  # Size of the vocabulary\n        embedding_dim = 256  # Dimension of the embedding layer\n        sentence_length = 100  # Max length of a sentence\n        num_sentences = 10  # Number of sentences in a document\n        units = 128  # Dimensionality of the output space of GRU\n        \n        # Word-level attention layer\n        word_input = tf.keras.layers.Input(shape=(sentence_length,), dtype='int32')\n        embedded_word = tf.keras.layers.Embedding(vocab_size, embedding_dim)(word_input)\n        word_gru = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(units, return_sequences=True))(embedded_word)\n        word_attention_layer = tf.keras.layers.Attention(use_scale=True, return_attention_scores=True)\n        word_attention_output, word_attention_weights = word_attention_layer([word_gru, word_gru], return_attention_scores=True)\n        word_encoder_with_attention = tf.keras.Model(inputs=word_input, outputs=[word_attention_output, word_attention_weights])\n        \n        # Sentence-level attention layer\n        sentence_input = tf.keras.layers.Input(shape=(num_sentences, sentence_length), dtype='int32')\n        sentence_encoder_with_attention = tf.keras.layers.TimeDistributed(word_encoder_with_attention)(sentence_input)\n        sentence_gru = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(units, return_sequences=True))(sentence_encoder_with_attention[0])\n        sentence_attention_layer = tf.keras.layers.Attention(use_scale=True, return_attention_scores=True)\n        sentence_attention_output, sentence_attention_weights = sentence_attention_layer([sentence_gru, sentence_gru], return_attention_scores=True)\n        doc_representation = tf.keras.layers.Dense(units, activation='tanh')(sentence_attention_output)\n        \n        # Classifier\n        classifier = tf.keras.layers.Dense(1, activation='sigmoid')(doc_representation)\n        \n        # The model\n        self._model = tf.keras.Model(inputs=sentence_input, outputs=[classifier, sentence_attention_weights])\n        self._model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n    def predict(self, input_data: Any) -> Any:\n        \"\"\"\n        Predict method to use the loaded model for making predictions.\n\n        This example assumes `input_data` is preprocessed appropriately for the model's expected input.\n        \"\"\"\n        if self._model is None:\n            raise ValueError(\"Model is not loaded. Call `load_model` before prediction.\")\n            \n        # Predicting with the model\n        predictions, attention_weights = self._model.predict(input_data)\n        \n        # Additional logic to handle and package the predictions and attention weights could be added here\n        \n        return predictions, attention_weights\n```"
    },
    {
        "document_name": "swarmauri/experimental/utils/__init__.py",
        "content": "```swarmauri/experimental/utils/__init__.py\n# -*- coding: utf-8 -*-\n\n\n```"
    },
    {
        "document_name": "swarmauri/experimental/utils/get_last_frame.py",
        "content": "```swarmauri/experimental/utils/get_last_frame.py\nimport inspect\n\ndef child_function(arg):\n    # Get the stack frame of the caller\n    caller_frame = inspect.currentframe().f_back\n    # Get the name of the caller function\n    caller_name = caller_frame.f_code.co_name\n    # Inspect the arguments of the caller function\n    args, _, _, values = inspect.getargvalues(caller_frame)\n    # Assuming the caller has only one argument for simplicity\n    arg_name = args[0]\n    arg_value = values[arg_name]\n    print(f\"Caller Name: {caller_name}, Argument Name: {arg_name}, Argument Value: {arg_value}\")\n\ndef caller_function(l):\n    child_function(l)\n\n# Example usage\ncaller_function(\"Hello\")\n\n\n```"
    },
    {
        "document_name": "swarmauri/experimental/utils/save_schema.py",
        "content": "```swarmauri/experimental/utils/save_schema.py\nimport inspect\nimport random\n\nclass Storage:\n    def __init__(self):\n        self.logs = []\n\n    def log(self, log_data):\n        self.logs.append(log_data)\n\n    def print_logs(self):\n        for log in self.logs:\n            print(log)\n\nclass Loggable:\n    def __init__(self, name, storage):\n        self.name = name\n        self.storage = storage\n\n    def log_call(self, *args, **kwargs):\n        # Inspect the call stack to get the caller's details\n        caller_frame = inspect.stack()[2]\n        caller_name = inspect.currentframe().f_back.f_code.co_name\n        #caller_name = caller_frame.function\n        module = inspect.getmodule(caller_frame[0])\n        module_name = module.__name__ if module else 'N/A'\n\n        # Log all relevant details\n        log_data = {\n            'caller_name': caller_name,\n            'module_name': module_name,\n            'called_name': self.name,\n            'called_function': caller_frame[3], # The function in which log_call was invoked\n            'args': args,\n            'kwargs': kwargs\n        }\n        self.storage.log(log_data)\n\nclass Caller(Loggable):\n    def __init__(self, name, storage, others):\n        super().__init__(name, storage)\n        self.others = others\n\n    def __call__(self, *args, **kwargs):\n        if len(self.storage.logs)<10:\n            self.log_call(*args, **kwargs)\n            # Randomly call another without causing recursive calls\n            if args:  # Ensures it's not the first call without actual target\n                next_caller_name = random.choice([name for name in self.others if name != self.name])\n                self.others[next_caller_name](self.name)\n\n# Initialize storage and callers\nstorage = Storage()\nothers = {}\n\n# Creating callers\nalice = Caller('Alice', storage, others)\nbob = Caller('Bob', storage, others)\ncharlie = Caller('Charlie', storage, others)\ndan = Caller('Dan', storage, others)\n\nothers['Alice'] = alice\nothers['Bob'] = bob\nothers['Charlie'] = charlie\nothers['Dan'] = dan\n\n# Simulate the calls\ndan(1, taco=23)\n\n# Print the logs\nstorage.print_logs()\n\n```"
    },
    {
        "document_name": "swarmauri/experimental/utils/ISerializable.py",
        "content": "```swarmauri/experimental/utils/ISerializable.py\n\n# class Serializable:\n#     def serialize(self):\n#         raise NotImplementedError(\"Serialization method not implemented\")\n    \n#     @classmethod\n#     def deserialize(cls, data):\n#         raise NotImplementedError(\"Deserialization method not implemented\")\n        \n        \n# class ToolAgent(Serializable):\n#     def serialize(self):\n#         # Simplified example, adapt according to actual attributes\n#         return {\"type\": self.__class__.__name__, \"state\": {\"model_name\": self.model.model_name}}\n\n#     @classmethod\n#     def deserialize(cls, data):\n#         # This method should instantiate the object based on the serialized state.\n#         # Example assumes the presence of model_name in the serialized state.\n#         model = OpenAIToolModel(api_key=\"api_key_placeholder\", model_name=data[\"state\"][\"model_name\"])\n#         return cls(model=model, conversation=None, toolkit=None)  # Simplify, omit optional parameters for illustration\n```"
    },
    {
        "document_name": "swarmauri/experimental/utils/log_prompt_response.py",
        "content": "```swarmauri/experimental/utils/log_prompt_response.py\nimport sqlite3\nfrom functools import wraps\n\ndef log_prompt_response(db_path):\n    def decorator(func):\n        @wraps(func)\n        async def wrapper(*args, **kwargs):\n            # Extracting the 'message' parameter from args which is assumed to be the first argument\n            message = args[0]  \n            response = await func(*args, **kwargs)\n            conn = sqlite3.connect(db_path)\n            cursor = conn.cursor()\n            \n            # Create table if it doesn't exist\n            cursor.execute('''CREATE TABLE IF NOT EXISTS prompts_responses\n                            (id INTEGER PRIMARY KEY AUTOINCREMENT, \n                             prompt TEXT, \n                             response TEXT)''')\n            \n            # Insert a new record\n            cursor.execute('''INSERT INTO prompts_responses (prompt, response) \n                            VALUES (?, ?)''', (message, response))\n            conn.commit()\n            conn.close()\n            return response\n        \n        return wrapper\n    return decorator\n```"
    },
    {
        "document_name": "swarmauri/experimental/parsers/__init__.py",
        "content": "```swarmauri/experimental/parsers/__init__.py\n# -*- coding: utf-8 -*-\n\n\n```"
    },
    {
        "document_name": "swarmauri/experimental/parsers/PDFToTextParser.py",
        "content": "```swarmauri/experimental/parsers/PDFToTextParser.py\nimport fitz  # PyMuPDF\nfrom typing import List, Union, Any\nfrom ....core.parsers.IParser import IParser\nfrom ....core.documents.IDocument import IDocument\nfrom ....standard.documents.concrete.ConcreteDocument import ConcreteDocument\n\nclass PDFtoTextParser(IParser):\n    \"\"\"\n    A parser to extract text from PDF files.\n    \"\"\"\n\n    def parse(self, data: Union[str, Any]) -> List[IDocument]:\n        \"\"\"\n        Parses a PDF file and extracts its text content as Document instances.\n\n        Parameters:\n        - data (Union[str, Any]): The path to the PDF file.\n\n        Returns:\n        - List[IDocument]: A list with a single IDocument instance containing the extracted text.\n        \"\"\"\n        # Ensure data is a valid str path to a PDF file\n        if not isinstance(data, str):\n            raise ValueError(\"PDFtoTextParser expects a file path in str format.\")\n\n        try:\n            # Open the PDF file\n            doc = fitz.open(data)\n            text = \"\"\n\n            # Extract text from each page\n            for page_num in range(len(doc)):\n                page = doc.load_page(page_num)\n                text += page.get_text()\n\n            # Create a document with the extracted text\n            document = ConcreteDocument(doc_id=str(hash(data)), content=text, metadata={\"source\": data})\n            return [document]\n        \n        except Exception as e:\n            print(f\"An error occurred while parsing the PDF: {e}\")\n            return []\n\n```"
    },
    {
        "document_name": "swarmauri/experimental/vector_stores/__init__.py",
        "content": "```swarmauri/experimental/vector_stores/__init__.py\n# -*- coding: utf-8 -*-\n\n\n```"
    },
    {
        "document_name": "swarmauri/experimental/vector_stores/Word2VecDocumentStore.py",
        "content": "```swarmauri/experimental/vector_stores/Word2VecDocumentStore.py\nfrom typing import List, Union, Optional\nimport numpy as np\nfrom gensim.models import Word2Vec\nfrom swarmauri.core.document_stores.IDocumentStore import IDocumentStore\nfrom swarmauri.core.retrievers.IRetriever import IRetriever\nfrom swarmauri.standard.documents.concrete.EmbeddedDocument import EmbeddedDocument\nfrom swarmauri.standard.vector_stores.concrete.CosineDistance import CosineDistance\nfrom swarmauri.standard.vectors.concrete.SimpleVector import SimpleVector\nimport gensim.downloader as api\n\nclass Word2VecDocumentStore(IDocumentStore, IRetriever):\n    def __init__(self):\n        \"\"\"\n        Initializes the Word2VecDocumentStore.\n\n        Parameters:\n        - word2vec_model_path (Optional[str]): File path to a pre-trained Word2Vec model. \n                                               Leave None to use Gensim's pre-trained model.\n        - pre_trained (bool): If True, loads a pre-trained Word2Vec model. If False, an uninitialized model is used that requires further training.\n        \"\"\"\n        self.model = Word2Vec(vector_size=100, window=5, min_count=1, workers=4)  # Example parameters; adjust as needed\n        self.documents = []\n        self.metric = CosineDistance()\n\n    def add_document(self, document: EmbeddedDocument) -> None:\n        # Check if the document already has an embedding, if not generate one using _average_word_vectors\n        if not hasattr(document, 'embedding') or document.embedding is None:\n            words = document.content.split()  # Simple tokenization, consider using a better tokenizer\n            embedding = self._average_word_vectors(words)\n            document.embedding = embedding\n            print(document.embedding)\n        self.documents.append(document)\n        \n    def add_documents(self, documents: List[EmbeddedDocument]) -> None:\n        self.documents.extend(documents)\n        \n    def get_document(self, doc_id: str) -> Union[EmbeddedDocument, None]:\n        for document in self.documents:\n            if document.id == doc_id:\n                return document\n        return None\n        \n    def get_all_documents(self) -> List[EmbeddedDocument]:\n        return self.documents\n        \n    def delete_document(self, doc_id: str) -> None:\n        self.documents = [doc for doc in self.documents if doc.id != doc_id]\n\n    def update_document(self, doc_id: str, updated_document: EmbeddedDocument) -> None:\n        for i, document in enumerate(self.documents):\n            if document.id == doc_id:\n                self.documents[i] = updated_document\n                break\n\n    def _average_word_vectors(self, words: List[str]) -> np.ndarray:\n        \"\"\"\n        Generate document vector by averaging its word vectors.\n        \"\"\"\n        word_vectors = [self.model.wv[word] for word in words if word in self.model.wv]\n        print(word_vectors)\n        if word_vectors:\n            return np.mean(word_vectors, axis=0)\n        else:\n            return np.zeros(self.model.vector_size)\n\n    def retrieve(self, query: str, top_k: int = 5) -> List[EmbeddedDocument]:\n        \"\"\"\n        Retrieve documents similar to the query string based on Word2Vec embeddings.\n        \"\"\"\n        query_vector = self._average_word_vectors(query.split())\n        print('query_vector', query_vector)\n        # Compute similarity scores between the query and each document's stored embedding\n        similarities = self.metric.similarities(SimpleVector(query_vector), [SimpleVector(doc.embedding) for doc in self.documents if doc.embedding])\n        print('similarities', similarities)\n        # Retrieve indices of top_k most similar documents\n        top_k_indices = sorted(range(len(similarities)), key=lambda i: similarities[i], reverse=True)[:top_k]\n        print('top_k_indices', top_k_indices)\n        return [self.documents[i] for i in top_k_indices]\n```"
    },
    {
        "document_name": "swarmauri/experimental/vector_stores/TriplesDocumentStore.py",
        "content": "```swarmauri/experimental/vector_stores/TriplesDocumentStore.py\nfrom typing import List, Union, Optional\nimport numpy as np\nfrom rdflib import Graph, URIRef, Literal, BNode\nfrom ampligraph.latent_features import ComplEx\nfrom ampligraph.evaluation import train_test_split_no_unseen\nfrom ampligraph.latent_features import EmbeddingModel\nfrom ampligraph.utils import save_model, restore_model\n\nfrom swarmauri.core.documents.IDocument import IDocument\nfrom swarmauri.core.document_stores.IDocumentStore import IDocumentStore\nfrom swarmauri.core.retrievers.IRetriever import IRetriever\nfrom swarmauri.standard.documents.concrete.Document import Document\nfrom swarmauri.standard.vector_stores.concrete.CosineDistance import CosineDistance\nfrom swarmauri.standard.vectors.concrete.SimpleVector import SimpleVector\nfrom swarmauri.standard.vectorizers.concrete.AmpligraphVectorizer import AmpligraphVectorizer\n\n\nclass TriplesDocumentStore(IDocumentStore, IRetriever):\n    def __init__(self, rdf_file_path: str, model_path: Optional[str] = None):\n        \"\"\"\n        Initializes the TriplesDocumentStore.\n        \"\"\"\n        self.graph = Graph()\n        self.rdf_file_path = rdf_file_path\n        self.graph.parse(rdf_file_path, format='turtle')\n        self.documents = []\n        self.vectorizer = AmpligraphVectorizer()\n        self.model_path = model_path\n        if model_path:\n            self.model = restore_model(model_path)\n        else:\n            self.model = None\n        self.metric = CosineDistance()\n        self._load_documents()\n        if not self.model:\n            self._train_model()\n\n    def _train_model(self):\n        \"\"\"\n        Trains a model based on triples in the graph.\n        \"\"\"\n        # Extract triples for embedding model\n        triples = np.array([[str(s), str(p), str(o)] for s, p, o in self.graph])\n        # Split data\n        train, test = train_test_split_no_unseen(triples, test_size=0.1)\n        self.model = ComplEx(batches_count=100, seed=0, epochs=20, k=150, eta=1,\n                             optimizer='adam', optimizer_params={'lr': 1e-3},\n                             loss='pairwise', regularizer='LP', regularizer_params={'p': 3, 'lambda': 1e-5},\n                             verbose=True)\n        self.model.fit(train)\n        if self.model_path:\n            save_model(self.model, self.model_path)\n\n    def _load_documents(self):\n        \"\"\"\n        Load documents into the store from the RDF graph.\n        \"\"\"\n        for subj, pred, obj in self.graph:\n            doc_id = str(hash((subj, pred, obj)))\n            content = f\"{subj} {pred} {obj}\"\n            document = Document(content=content, doc_id=doc_id, metadata={})\n            self.documents.append(document)\n\n    def add_document(self, document: IDocument) -> None:\n        \"\"\"\n        Adds a single RDF triple document.\n        \"\"\"\n        subj, pred, obj = document.content.split()  # Splitting content into RDF components\n        self.graph.add((URIRef(subj), URIRef(pred), URIRef(obj) if obj.startswith('http') else Literal(obj)))\n        self.documents.append(document)\n        self._train_model()\n\n    def add_documents(self, documents: List[IDocument]) -> None:\n        \"\"\"\n        Adds multiple RDF triple documents.\n        \"\"\"\n        for document in documents:\n            subj, pred, obj = document.content.split()  # Assuming each document's content is \"subj pred obj\"\n            self.graph.add((URIRef(subj), URIRef(pred), URIRef(obj) if obj.startswith('http') else Literal(obj)))\n        self.documents.extend(documents)\n        self._train_model()\n\n    # Implementation for get_document, get_all_documents, delete_document, update_document remains same as before\n    \n    def retrieve(self, query: str, top_k: int = 5) -> List[IDocument]:\n        \"\"\"\n        Retrieve documents similar to the query string.\n        \"\"\"\n        if not self.model:\n            self._train_model()\n        query_vector = self.vectorizer.infer_vector(model=self.model, samples=[query])[0]\n        document_vectors = [self.vectorizer.infer_vector(model=self.model, samples=[doc.content])[0] for doc in self.documents]\n        similarities = self.metric.distances(SimpleVector(data=query_vector), [SimpleVector(vector) for vector in document_vectors])\n        top_k_indices = sorted(range(len(similarities)), key=lambda i: similarities[i])[:top_k]\n        return [self.documents[i] for i in top_k_indices]\n```"
    },
    {
        "document_name": "swarmauri/experimental/tracing/RemoteTrace.py",
        "content": "```swarmauri/experimental/tracing/RemoteTrace.py\nfrom __future__ import ITraceContext\n\nimport requests\nimport json\nimport uuid\nfrom datetime import datetime\n\nfrom swarmauri.core.tracing.ITracer import ITracer\nfrom swarmauri.core.tracing.ITraceContext import ITraceContext\n\n# Implementing the RemoteTraceContext class\nclass RemoteTraceContext(ITraceContext):\n    def __init__(self, trace_id: str, name: str):\n        self.trace_id = trace_id\n        self.name = name\n        self.start_time = datetime.now()\n        self.attributes = {}\n        self.annotations = {}\n\n    def get_trace_id(self) -> str:\n        return self.trace_id\n\n    def add_attribute(self, key: str, value):\n        self.attributes[key] = value\n        \n    def add_annotation(self, key: str, value):\n        self.annotations[key] = value\n\n# Implementing the RemoteAPITracer class\nclass RemoteAPITracer(ITracer):\n    def __init__(self, api_endpoint: str):\n        self.api_endpoint = api_endpoint\n\n    def start_trace(self, name: str, initial_attributes=None) -> 'RemoteTraceContext':\n        trace_id = str(uuid.uuid4())\n        context = RemoteTraceContext(trace_id, name)\n        if initial_attributes:\n            for key, value in initial_attributes.items():\n                context.add_attribute(key, value)\n        return context\n\n    def end_trace(self, trace_context: 'RemoteTraceContext'):\n        trace_context.end_time = datetime.now()\n        # Pretending to serialize the context information to JSON\n        trace_data = {\n            \"trace_id\": trace_context.get_trace_id(),\n            \"name\": trace_context.name,\n            \"start_time\": str(trace_context.start_time),\n            \"end_time\": str(trace_context.end_time),\n            \"attributes\": trace_context.attributes,\n            \"annotations\": trace_context.annotations\n        }\n        json_data = json.dumps(trace_data)\n        # POST the serialized data to the remote REST API\n        response = requests.post(self.api_endpoint, json=json_data)\n        if not response.ok:\n            raise Exception(f\"Failed to send trace data to {self.api_endpoint}. Status code: {response.status_code}\")\n\n    def annotate_trace(self, trace_context: 'RemoteTraceContext', key: str, value):\n        trace_context.add_annotation(key, value)\n```"
    },
    {
        "document_name": "swarmauri/experimental/tracing/__init__.py",
        "content": "```swarmauri/experimental/tracing/__init__.py\n# -*- coding: utf-8 -*-\n\n\n```"
    },
    {
        "document_name": "swarmauri/experimental/chains/ChainOrderStrategy.py",
        "content": "```swarmauri/experimental/chains/ChainOrderStrategy.py\nfrom typing import List\nfrom swarmauri.core.chains.IChainOrderStrategy import IChainOrderStrategy\nfrom swarmauri.core.chains.IChainStep import IChainStep\n\nclass ChainOrderStrategy(IChainOrderStrategy):\n    def order_steps(self, steps: List[IChainStep]) -> List[IChainStep]:\n        \"\"\"\n        Orders the chain steps in reverse order.\n\n        Args:\n            steps (List[IChainStep]): The original list of chain steps to be ordered.\n\n        Returns:\n            List[IChainStep]: List of chain steps in order.\n        \"\"\"\n        # Reverse the list of steps.\n        steps = list(steps)\n        return steps\n```"
    },
    {
        "document_name": "swarmauri/experimental/chains/ChainOrderStrategyBase.py",
        "content": "```swarmauri/experimental/chains/ChainOrderStrategyBase.py\nfrom typing import List\nfrom swarmauri.core.chains.IChainOrderStrategy import IChainOrderStrategy\nfrom swarmauri.core.chains.IChainStep import IChainStep\n\nclass ChainOrderStrategyBase(IChainOrderStrategy):\n    \"\"\"\n    A base implementation of the IChainOrderStrategy interface.\n    \"\"\"\n\n    def order_steps(self, steps: List[IChainStep]) -> List[IChainStep]:\n        \"\"\"\n        Default implementation doesn't reorder steps but must be overridden by specific strategies.\n        \"\"\"\n        return steps\n```"
    },
    {
        "document_name": "swarmauri/experimental/chains/ChainProcessingStrategy.py",
        "content": "```swarmauri/experimental/chains/ChainProcessingStrategy.py\nfrom typing import List, Any\nfrom swarmauri.core.chains.IChainProcessingStrategy import IChainProcessingStrategy\nfrom swarmauri.core.chains.IChainStep import IChainStep\n\nclass ChainProcessingStrategy(IChainProcessingStrategy):\n    def execute_steps(self, steps: List[IChainStep]) -> Any:\n        \"\"\"\n        Executes the given list of ordered chain steps based on the specific strategy \n        and collects their results.\n\n        Args:\n            steps (List[IChainStep]): The ordered list of chain steps to be executed.\n        \n        Returns:\n            Any: The result of executing the steps. This could be tailored as per requirement.\n        \"\"\"\n        results = []\n        for step in steps:\n            result = step.method(*step.args, **step.kwargs)\n            results.append(result)\n        return results\n```"
    },
    {
        "document_name": "swarmauri/experimental/chains/ChainProcessingStrategyBase.py",
        "content": "```swarmauri/experimental/chains/ChainProcessingStrategyBase.py\nfrom typing import List\nfrom swarmauri.core.chains.IChainProcessingStrategy import IChainProcessingStrategy\nfrom swarmauri.core.chains.IChainStep import IChainStep\n\nclass ChainProcessingStrategyBase(IChainProcessingStrategy):\n    \"\"\"\n    A base implementation of the IChainProcessingStrategy interface.\n    \"\"\"\n    \n    def execute_steps(self, steps: List[IChainStep]):\n        \"\"\"\n        Default implementation which should be overridden by specific processing strategies.\n        \"\"\"\n        for step in steps:\n            print(step)\n            step.method(*step.args, **step.kwargs)\n```"
    },
    {
        "document_name": "swarmauri/experimental/chains/IChainOrderStrategy.py",
        "content": "```swarmauri/experimental/chains/IChainOrderStrategy.py\nfrom abc import ABC, abstractmethod\nfrom typing import List\nfrom swarmauri.core.chains.IChainStep import IChainStep\n\n# Defines how chain steps are ordered\nclass IChainOrderStrategy(ABC):\n    @abstractmethod\n    def order_steps(self, steps: List[IChainStep]) -> List[IChainStep]:\n        pass\n```"
    },
    {
        "document_name": "swarmauri/experimental/chains/IChainProcessingStrategy.py",
        "content": "```swarmauri/experimental/chains/IChainProcessingStrategy.py\nfrom abc import ABC, abstractmethod\nfrom typing import List\nfrom swarmauri.core.chains.IChainStep import IChainStep\n\nclass IChainProcessingStrategy(ABC):\n    \"\"\"\n    Interface for defining the strategy to process the execution of chain steps.\n    \"\"\"\n    \n    @abstractmethod\n    def execute_steps(self, steps: List[IChainStep]):\n        \"\"\"\n        Executes the given list of ordered chain steps based on the specific strategy.\n        \n        Parameters:\n            steps (List[IChainStep]): The ordered list of chain steps to be executed.\n        \"\"\"\n        pass\n```"
    },
    {
        "document_name": "swarmauri/experimental/chains/MatrixOrderStrategy.py",
        "content": "```swarmauri/experimental/chains/MatrixOrderStrategy.py\nfrom typing import List\nfrom swarmauri.core.chains.IChainOrderStrategy import IChainOrderStrategy\nfrom swarmauri.core.chains.IChainStep import IChainStep\n\nclass MatrixOrderStrategy(IChainOrderStrategy):\n    def order_steps(self, steps: List[IChainStep]) -> List[IChainStep]:\n        # Assuming 'steps' are already organized in a matrix-like structure\n        ordered_steps = self.arrange_matrix(steps)\n        return ordered_steps\n\n    def arrange_matrix(self, steps_matrix):\n        # Implement the logic to arrange/order steps based on matrix positions.\n        # This is just a placeholder. The actual implementation would depend on the matrix specifications and task dependencies.\n        return steps_matrix\n```"
    },
    {
        "document_name": "swarmauri/experimental/chains/MatrixProcessingStrategy.py",
        "content": "```swarmauri/experimental/chains/MatrixProcessingStrategy.py\nimport asyncio\nfrom typing import List, Any\nfrom swarmauri.core.chains.IChainProcessingStrategy import IChainProcessingStrategy\nfrom swarmauri.core.chains.IChainStep import IChainStep\n\nclass MatrixProcessingStrategy(IChainProcessingStrategy):\n    async def execute_steps(self, steps: List[IChainStep]) -> Any:\n        # Launch tasks asynchronously, maintaining the matrix structure\n        results = await self.execute_matrix(steps)\n        return results\n\n    async def execute_matrix(self, matrix):\n        matrix_results = []\n\n        # Example: Execute tasks row by row, waiting for each row to complete before moving on.\n        for row in matrix:\n            row_results = await asyncio.gather(*[step.method(*step.args, **step.kwargs) for step in row])\n            matrix_results.append(row_results)\n            # Optionally, add a call to a row callback here\n\n        # After processing all rows, you may call a final matrix callback\n        # This could be a place for final aggregation or analysis of all results\n        return matrix_results\n```"
    },
    {
        "document_name": "swarmauri/experimental/chains/TypeAgnosticCallableChain.py",
        "content": "```swarmauri/experimental/chains/TypeAgnosticCallableChain.py\nfrom typing import Any, Callable, List, Dict, Optional, Tuple, Union\n\nCallableDefinition = Tuple[Callable, List[Any], Dict[str, Any], Union[str, Callable, None]]\n\nclass TypeAgnosticCallableChain:\n    def __init__(self, callables: Optional[List[CallableDefinition]] = None):\n        self.callables = callables if callables is not None else []\n\n    @staticmethod\n    def _ignore_previous(_previous_result, *args, **kwargs):\n        return args, kwargs\n\n    @staticmethod\n    def _use_first_arg(previous_result, *args, **kwargs):\n        return [previous_result] + list(args), kwargs\n\n    @staticmethod\n    def _use_all_previous_args_first(previous_result, *args, **kwargs):\n        if not isinstance(previous_result, (list, tuple)):\n            previous_result = [previous_result]\n        return list(previous_result) + list(args), kwargs\n\n    @staticmethod\n    def _use_all_previous_args_only(previous_result, *_args, **_kwargs):\n        if not isinstance(previous_result, (list, tuple)):\n            previous_result = [previous_result]\n        return list(previous_result), {}\n\n    @staticmethod\n    def _add_previous_kwargs_overwrite(previous_result, args, kwargs):\n        if not isinstance(previous_result, dict):\n            raise ValueError(\"Previous result is not a dictionary.\")\n        return args, {**kwargs, **previous_result}\n\n    @staticmethod\n    def _add_previous_kwargs_no_overwrite(previous_result, args, kwargs):\n        if not isinstance(previous_result, dict):\n            raise ValueError(\"Previous result is not a dictionary.\")\n        return args, {**previous_result, **kwargs}\n\n    @staticmethod\n    def _use_all_args_all_kwargs_overwrite(previous_result_args, previous_result_kwargs, *args, **kwargs):\n        combined_args = list(previous_result_args) + list(args) if isinstance(previous_result_args, (list, tuple)) else list(args)\n        combined_kwargs = previous_result_kwargs if isinstance(previous_result_kwargs, dict) else {}\n        combined_kwargs.update(kwargs)\n        return combined_args, combined_kwargs\n\n    @staticmethod\n    def _use_all_args_all_kwargs_no_overwrite(previous_result_args, previous_result_kwargs, *args, **kwargs):\n        combined_args = list(previous_result_args) + list(args) if isinstance(previous_result_args, (list, tuple)) else list(args)\n        combined_kwargs = kwargs if isinstance(kwargs, dict) else {}\n        combined_kwargs = {**combined_kwargs, **(previous_result_kwargs if isinstance(previous_result_kwargs, dict) else {})}\n        return combined_args, combined_kwargs\n\n    def add_callable(self, func: Callable, args: List[Any] = None, kwargs: Dict[str, Any] = None, input_handler: Union[str, Callable, None] = None) -> None:\n        if isinstance(input_handler, str):\n            # Map the string to the corresponding static method\n            input_handler_method = getattr(self, f\"_{input_handler}\", None)\n            if input_handler_method is None:\n                raise ValueError(f\"Unknown input handler name: {input_handler}\")\n            input_handler = input_handler_method\n        elif input_handler is None:\n            input_handler = self._ignore_previous\n        self.callables.append((func, args or [], kwargs or {}, input_handler))\n\n    def __call__(self, *initial_args, **initial_kwargs) -> Any:\n        result = None\n        for func, args, kwargs, input_handler in self.callables:\n            if isinstance(input_handler, str):\n                # Map the string to the corresponding static method\n                input_handler_method = getattr(self, f\"_{input_handler}\", None)\n                if input_handler_method is None:\n                    raise ValueError(f\"Unknown input handler name: {input_handler}\")\n                input_handler = input_handler_method\n            elif input_handler is None:\n                input_handler = self._ignore_previous\n                \n            args, kwargs = input_handler(result, *args, **kwargs) if result is not None else (args, kwargs)\n            result = func(*args, **kwargs)\n        return result\n\n    def __or__(self, other: \"TypeAgnosticCallableChain\") -> \"TypeAgnosticCallableChain\":\n        if not isinstance(other, TypeAgnosticCallableChain):\n            raise TypeError(\"Operand must be an instance of TypeAgnosticCallableChain\")\n        \n        new_chain = TypeAgnosticCallableChain(self.callables + other.callables)\n        return new_chain\n```"
    },
    {
        "document_name": "swarmauri/experimental/chains/__init__.py",
        "content": "```swarmauri/experimental/chains/__init__.py\n#\n```"
    },
    {
        "document_name": "swarmauri/experimental/vectorizers/DGLVectorizer.py",
        "content": "```swarmauri/experimental/vectorizers/DGLVectorizer.py\nimport dgl\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom dgl.nn import GraphConv\nfrom typing import List, Union, Any\nfrom swarmauri.core.vectorizers.IVectorize import IVectorize\nfrom swarmauri.core.vectors.IVector import IVector\nfrom swarmauri.standard.vectors.concrete.SimpleVector import SimpleVector\n\nclass DGLGraphConv(torch.nn.Module):\n    def __init__(self, in_feats, out_feats, activation=F.relu):\n        super(DGLGraphConv, self).__init__()\n        self.conv1 = GraphConv(in_feats, 128)\n        self.conv2 = GraphConv(128, out_feats)\n        self.activation = activation\n\n    def forward(self, g, inputs):\n        # Apply graph convolution and activation.\n        h = self.conv1(g, inputs)\n        h = self.activation(h)\n        h = self.conv2(g, h)\n        return h\n\nclass DGLVectorizer(IVectorize):\n    def __init__(self, in_feats, out_feats, model=None):\n        self.in_feats = in_feats\n        self.out_feats = out_feats\n        self.model = model or DGLGraphConv(in_feats, out_feats)\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        \n    def fit(self, graphs, features, epochs=10, learning_rate=0.01):\n        self.model.to(self.device)\n        optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n        for epoch in range(epochs):\n            for g, feat in zip(graphs, features):\n                g = g.to(self.device)\n                feat = feat.to(self.device)\n                outputs = self.model(g, feat)\n                loss = F.mse_loss(outputs, feat)  # Example loss; adjust as needed\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n            print(f'Epoch {epoch + 1}, Loss: {loss.item()}')\n    \n    def infer_vector(self, graph, features):\n        graph = graph.to(self.device)\n        features = features.to(self.device)\n        with torch.no_grad():\n            embeddings = self.model(graph, features)\n        return SimpleVector(embeddings.cpu().numpy())\n```"
    },
    {
        "document_name": "swarmauri/experimental/vectorizers/__init__.py",
        "content": "```swarmauri/experimental/vectorizers/__init__.py\n# -*- coding: utf-8 -*-\n\n\n```"
    },
    {
        "document_name": "swarmauri/experimental/document_stores/TriplesDocumentStore.py",
        "content": "```swarmauri/experimental/document_stores/TriplesDocumentStore.py\nfrom typing import List, Union, Optional\nimport numpy as np\nfrom rdflib import Graph, URIRef, Literal, BNode\nfrom ampligraph.latent_features import ComplEx\nfrom ampligraph.evaluation import train_test_split_no_unseen\nfrom ampligraph.latent_features import EmbeddingModel\nfrom ampligraph.utils import save_model, restore_model\n\nfrom swarmauri.core.documents.IDocument import IDocument\nfrom swarmauri.core.document_stores.IDocumentStore import IDocumentStore\nfrom swarmauri.core.retrievers.IRetriever import IRetriever\nfrom swarmauri.standard.documents.concrete.Document import Document\nfrom swarmauri.standard.vector_stores.concrete.CosineDistance import CosineDistance\nfrom swarmauri.standard.vectors.concrete.SimpleVector import SimpleVector\nfrom swarmauri.standard.vectorizers.concrete.AmpligraphVectorizer import AmpligraphVectorizer\n\n\nclass TriplesDocumentStore(IDocumentStore, IRetriever):\n    def __init__(self, rdf_file_path: str, model_path: Optional[str] = None):\n        \"\"\"\n        Initializes the TriplesDocumentStore.\n        \"\"\"\n        self.graph = Graph()\n        self.rdf_file_path = rdf_file_path\n        self.graph.parse(rdf_file_path, format='turtle')\n        self.documents = []\n        self.vectorizer = AmpligraphVectorizer()\n        self.model_path = model_path\n        if model_path:\n            self.model = restore_model(model_path)\n        else:\n            self.model = None\n        self.metric = CosineDistance()\n        self._load_documents()\n        if not self.model:\n            self._train_model()\n\n    def _train_model(self):\n        \"\"\"\n        Trains a model based on triples in the graph.\n        \"\"\"\n        # Extract triples for embedding model\n        triples = np.array([[str(s), str(p), str(o)] for s, p, o in self.graph])\n        # Split data\n        train, test = train_test_split_no_unseen(triples, test_size=0.1)\n        self.model = ComplEx(batches_count=100, seed=0, epochs=20, k=150, eta=1,\n                             optimizer='adam', optimizer_params={'lr': 1e-3},\n                             loss='pairwise', regularizer='LP', regularizer_params={'p': 3, 'lambda': 1e-5},\n                             verbose=True)\n        self.model.fit(train)\n        if self.model_path:\n            save_model(self.model, self.model_path)\n\n    def _load_documents(self):\n        \"\"\"\n        Load documents into the store from the RDF graph.\n        \"\"\"\n        for subj, pred, obj in self.graph:\n            doc_id = str(hash((subj, pred, obj)))\n            content = f\"{subj} {pred} {obj}\"\n            document = Document(content=content, doc_id=doc_id, metadata={})\n            self.documents.append(document)\n\n    def add_document(self, document: IDocument) -> None:\n        \"\"\"\n        Adds a single RDF triple document.\n        \"\"\"\n        subj, pred, obj = document.content.split()  # Splitting content into RDF components\n        self.graph.add((URIRef(subj), URIRef(pred), URIRef(obj) if obj.startswith('http') else Literal(obj)))\n        self.documents.append(document)\n        self._train_model()\n\n    def add_documents(self, documents: List[IDocument]) -> None:\n        \"\"\"\n        Adds multiple RDF triple documents.\n        \"\"\"\n        for document in documents:\n            subj, pred, obj = document.content.split()  # Assuming each document's content is \"subj pred obj\"\n            self.graph.add((URIRef(subj), URIRef(pred), URIRef(obj) if obj.startswith('http') else Literal(obj)))\n        self.documents.extend(documents)\n        self._train_model()\n\n    # Implementation for get_document, get_all_documents, delete_document, update_document remains same as before\n    \n    def retrieve(self, query: str, top_k: int = 5) -> List[IDocument]:\n        \"\"\"\n        Retrieve documents similar to the query string.\n        \"\"\"\n        if not self.model:\n            self._train_model()\n        query_vector = self.vectorizer.infer_vector(model=self.model, samples=[query])[0]\n        document_vectors = [self.vectorizer.infer_vector(model=self.model, samples=[doc.content])[0] for doc in self.documents]\n        similarities = self.metric.distances(SimpleVector(data=query_vector), [SimpleVector(vector) for vector in document_vectors])\n        top_k_indices = sorted(range(len(similarities)), key=lambda i: similarities[i])[:top_k]\n        return [self.documents[i] for i in top_k_indices]\n```"
    },
    {
        "document_name": "swarmauri/experimental/document_stores/Word2VecDocumentStore.py",
        "content": "```swarmauri/experimental/document_stores/Word2VecDocumentStore.py\nfrom typing import List, Union, Optional\nimport numpy as np\nfrom gensim.models import Word2Vec\nfrom swarmauri.core.document_stores.IDocumentStore import IDocumentStore\nfrom swarmauri.core.retrievers.IRetriever import IRetriever\nfrom swarmauri.standard.documents.concrete.EmbeddedDocument import EmbeddedDocument\nfrom swarmauri.standard.vector_stores.concrete.CosineDistance import CosineDistance\nfrom swarmauri.standard.vectors.concrete.SimpleVector import SimpleVector\nimport gensim.downloader as api\n\nclass Word2VecDocumentStore(IDocumentStore, IRetriever):\n    def __init__(self):\n        \"\"\"\n        Initializes the Word2VecDocumentStore.\n\n        Parameters:\n        - word2vec_model_path (Optional[str]): File path to a pre-trained Word2Vec model. \n                                               Leave None to use Gensim's pre-trained model.\n        - pre_trained (bool): If True, loads a pre-trained Word2Vec model. If False, an uninitialized model is used that requires further training.\n        \"\"\"\n        self.model = Word2Vec(vector_size=100, window=5, min_count=1, workers=4)  # Example parameters; adjust as needed\n        self.documents = []\n        self.metric = CosineDistance()\n\n    def add_document(self, document: EmbeddedDocument) -> None:\n        # Check if the document already has an embedding, if not generate one using _average_word_vectors\n        if not hasattr(document, 'embedding') or document.embedding is None:\n            words = document.content.split()  # Simple tokenization, consider using a better tokenizer\n            embedding = self._average_word_vectors(words)\n            document.embedding = embedding\n            print(document.embedding)\n        self.documents.append(document)\n        \n    def add_documents(self, documents: List[EmbeddedDocument]) -> None:\n        self.documents.extend(documents)\n        \n    def get_document(self, doc_id: str) -> Union[EmbeddedDocument, None]:\n        for document in self.documents:\n            if document.id == doc_id:\n                return document\n        return None\n        \n    def get_all_documents(self) -> List[EmbeddedDocument]:\n        return self.documents\n        \n    def delete_document(self, doc_id: str) -> None:\n        self.documents = [doc for doc in self.documents if doc.id != doc_id]\n\n    def update_document(self, doc_id: str, updated_document: EmbeddedDocument) -> None:\n        for i, document in enumerate(self.documents):\n            if document.id == doc_id:\n                self.documents[i] = updated_document\n                break\n\n    def _average_word_vectors(self, words: List[str]) -> np.ndarray:\n        \"\"\"\n        Generate document vector by averaging its word vectors.\n        \"\"\"\n        word_vectors = [self.model.wv[word] for word in words if word in self.model.wv]\n        print(word_vectors)\n        if word_vectors:\n            return np.mean(word_vectors, axis=0)\n        else:\n            return np.zeros(self.model.vector_size)\n\n    def retrieve(self, query: str, top_k: int = 5) -> List[EmbeddedDocument]:\n        \"\"\"\n        Retrieve documents similar to the query string based on Word2Vec embeddings.\n        \"\"\"\n        query_vector = self._average_word_vectors(query.split())\n        print('query_vector', query_vector)\n        # Compute similarity scores between the query and each document's stored embedding\n        similarities = self.metric.similarities(SimpleVector(query_vector), [SimpleVector(doc.embedding) for doc in self.documents if doc.embedding])\n        print('similarities', similarities)\n        # Retrieve indices of top_k most similar documents\n        top_k_indices = sorted(range(len(similarities)), key=lambda i: similarities[i], reverse=True)[:top_k]\n        print('top_k_indices', top_k_indices)\n        return [self.documents[i] for i in top_k_indices]\n```"
    },
    {
        "document_name": "swarmauri/experimental/document_stores/__init__.py",
        "content": "```swarmauri/experimental/document_stores/__init__.py\n# -*- coding: utf-8 -*-\n\n\n```"
    },
    {
        "document_name": "swarmauri/experimental/distances/CanberraDistance.py",
        "content": "```swarmauri/experimental/distances/CanberraDistance.py\nimport numpy as np\nfrom typing import List\nfrom swarmauri.core.vector_stores.IDistanceSimilarity import IDistanceSimilarity\nfrom swarmauri.core.vectors.IVector import IVector\n\n\nclass CanberraDistance(IDistanceSimilarity):\n    \"\"\"\n    Concrete implementation of the IDistanceSimiliarity interface using the Canberra distance metric.\n    This class now processes IVector instances instead of raw lists.\n    \"\"\"\n\n    def distance(self, vector_a: IVector, vector_b: IVector) -> float:\n        \"\"\"\n        Computes the Canberra distance between two IVector instances.\n\n        Args:\n            vector_a (IVector): The first vector in the comparison.\n            vector_b (IVector): The second vector in the comparison.\n\n        Returns:\n            float: The computed Canberra distance between the vectors.\n        \"\"\"\n        # Extract data from IVector\n        data_a = np.array(vector_a.data)\n        data_b = np.array(vector_b.data)\n\n        # Checking dimensions match\n        if data_a.shape != data_b.shape:\n            raise ValueError(\"Vectors must have the same dimensionality.\")\n\n        # Computing Canberra distance\n        distance = np.sum(np.abs(data_a - data_b) / (np.abs(data_a) + np.abs(data_b)))\n        # Handling the case where both vectors have a zero value for the same dimension\n        distance = np.nan_to_num(distance)\n        return distance\n    \n    def similarity(self, vector_a: IVector, vector_b: IVector) -> float:\n        \"\"\"\n        Compute similarity using the Canberra distance. Since this distance metric isn't\n        directly interpretable as a similarity, a transformation is applied to map the distance\n        to a similarity score.\n\n        Args:\n            vector_a (IVector): The first vector in the comparison.\n            vector_b (IVector): The second vector to compare with the first vector.\n\n        Returns:\n            float: A similarity score between vector_a and vector_b.\n        \"\"\"\n        # One way to derive a similarity from distance is through inversion or transformation.\n        # Here we use an exponential decay based on the computed distance. This is a placeholder\n        # that assumes closer vectors (smaller distance) are more similar.\n        distance = self.distance(vector_a, vector_b)\n\n        # Transform the distance into a similarity score\n        similarity = np.exp(-distance)\n\n        return similarity\n    \n    def distances(self, vector_a: IVector, vectors_b: List[IVector]) -> List[float]:\n        distances = [self.distance(vector_a, vector_b) for vector_b in vectors_b]\n        return distances\n    \n    def similarities(self, vector_a: IVector, vectors_b: List[IVector]) -> List[float]:\n        similarities = [self.similarity(vector_a, vector_b) for vector_b in vectors_b]\n        return similarities\n```"
    },
    {
        "document_name": "swarmauri/experimental/distances/ChebyshevDistance.py",
        "content": "```swarmauri/experimental/distances/ChebyshevDistance.py\nfrom typing import List\nfrom swarmauri.core.vectors.IVector import IVector\nfrom swarmauri.core.vector_stores.IDistanceSimilarity import IDistanceSimilarity\n\nclass ChebyshevDistance(IDistanceSimilarity):\n    \"\"\"\n    Concrete implementation of the IDistanceSimiliarity interface using the Chebyshev distance metric.\n    Chebyshev distance is the maximum absolute distance between two vectors' elements.\n    \"\"\"\n\n    def distance(self, vector_a: IVector, vector_b: IVector) -> float:\n        \"\"\"\n        Computes the Chebyshev distance between two vectors.\n\n        Args:\n            vector_a (IVector): The first vector in the comparison.\n            vector_b (IVector): The second vector in the comparison.\n\n        Returns:\n            float: The computed Chebyshev distance between vector_a and vector_b.\n        \"\"\"\n        max_distance = 0\n        for a, b in zip(vector_a.data, vector_b.data):\n            max_distance = max(max_distance, abs(a - b))\n        return max_distance\n\n    def similarity(self, vector_a: IVector, vector_b: IVector) -> float:\n        \"\"\"\n        Computes the similarity between two vectors based on the Chebyshev distance.\n\n        Args:\n            vector_a (IVector): The first vector.\n            vector_b (IVector): The second vector.\n\n        Returns:\n            float: The similarity score between the two vectors.\n        \"\"\"\n\n        return 1 / (1 + self.distance(vector_a, vector_b))\n    \n    def distances(self, vector_a: IVector, vectors_b: List[IVector]) -> List[float]:\n        distances = [self.distance(vector_a, vector_b) for vector_b in vectors_b]\n        return distances\n    \n    def similarities(self, vector_a: IVector, vectors_b: List[IVector]) -> List[float]:\n        similarities = [self.similarity(vector_a, vector_b) for vector_b in vectors_b]\n        return similarities\n```"
    },
    {
        "document_name": "swarmauri/experimental/distances/HaversineDistance.py",
        "content": "```swarmauri/experimental/distances/HaversineDistance.py\nfrom typing import List\nfrom math import radians, cos, sin, sqrt, atan2\nfrom swarmauri.core.vector_stores.IDistanceSimilarity import IDistanceSimilarity\nfrom swarmauri.core.vectors.IVector import IVector\n\n\nclass HaversineDistance(IDistanceSimilarity):\n    \"\"\"\n    Concrete implementation of IDistanceSimiliarity interface using the Haversine formula.\n    \n    Haversine formula determines the great-circle distance between two points on a sphere given their \n    longitudes and latitudes. This implementation is particularly useful for geo-spatial data.\n    \"\"\" \n\n    def distance(self, vector_a: IVector, vector_b: IVector) -> float:\n        \"\"\"\n        Computes the Haversine distance between two geo-spatial points.\n\n        Args:\n            vector_a (IVector): The first point in the format [latitude, longitude].\n            vector_b (IVector): The second point in the same format [latitude, longitude].\n\n        Returns:\n            float: The Haversine distance between vector_a and vector_b in kilometers.\n        \"\"\"\n        # Earth radius in kilometers\n        R = 6371.0\n\n        lat1, lon1 = map(radians, vector_a.data)\n        lat2, lon2 = map(radians, vector_b.data)\n\n        dlat = lat2 - lat1\n        dlon = lon2 - lon1\n\n        # Haversine formula\n        a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n        c = 2 * atan2(sqrt(a), sqrt(1 - a))\n        distance = R * c\n\n        return distance\n\n    def similarity(self, vector_a: IVector, vector_b: IVector) -> float:\n        raise NotImplementedError(\"Similarity not implemented for Haversine distance.\")\n        \n    def distances(self, vector_a: IVector, vectors_b: List[IVector]) -> List[float]:\n        distances = [self.distance(vector_a, vector_b) for vector_b in vectors_b]\n        return distances\n    \n    def similarities(self, vector_a: IVector, vectors_b: List[IVector]) -> List[float]:\n        raise NotImplementedError(\"Similarity not implemented for Haversine distance.\")\n```"
    },
    {
        "document_name": "swarmauri/experimental/distances/ManhattanDistance.py",
        "content": "```swarmauri/experimental/distances/ManhattanDistance.py\nfrom typing import List\nfrom swarmauri.core.vector_stores.IDistanceSimilarity import IDistanceSimilarity\nfrom swarmauri.core.vectors.IVector import IVector\n\nclass ManhattanDistance(IDistanceSimilarity):\n    \"\"\"\n    Concrete implementation of the IDistanceSimiliarity interface using the Manhattan distance.\n    \n    The Manhattan distance between two points is the sum of the absolute differences of their Cartesian coordinates.\n    This is also known as L1 distance.\n    \"\"\"\n\n    def distance(self, vector_a: IVector, vector_b: IVector) -> float:\n        \"\"\"\n        Computes the Manhattan distance between two vectors.\n\n        Args:\n            vector_a (IVector): The first vector in the comparison.\n            vector_b (IVector): The second vector in the comparison.\n\n        Returns:\n            float: The Manhattan distance between vector_a and vector_b.\n        \"\"\"\n        if vector_a.dimensions != vector_b.dimensions:\n            raise ValueError(\"Vectors must have the same dimensionality.\")\n        \n        return sum(abs(a - b) for a, b in zip(vector_a.data, vector_b.data))\n\n    def similarity(self, vector_a: IVector, vector_b: IVector) -> float:\n        \"\"\"\n        The similarity based on Manhattan distance can be inversely related to the distance for some applications,\n        but this method intentionally returns NotImplementedError to signal that Manhattan distance is typically\n        not directly converted to similarity in the conventional sense used in this context.\n\n        Args:\n            vector_a (IVector): The first vector in the comparison.\n            vector_b (IVector): The second vector in the comparison.\n\n        Returns:\n            NotImplementedError: This is intended as this distance metric doesn't directly offer a similarity measure.\n        \"\"\"\n        raise NotImplementedError(\"ManhattanDistance does not directly provide a similarity measure.\")\n        \n    def distances(self, vector_a: IVector, vectors_b: List[IVector]) -> List[float]:\n        distances = [self.distance(vector_a, vector_b) for vector_b in vectors_b]\n        return distances\n    \n    def similarities(self, vector_a: IVector, vectors_b: List[IVector]) -> List[float]:\n        raise NotImplementedError(\"ManhattanDistance does not directly provide a similarity measure.\")\n```"
    },
    {
        "document_name": "swarmauri/experimental/distances/MinkowskiDistance.py",
        "content": "```swarmauri/experimental/distances/MinkowskiDistance.py\nfrom typing import List\nfrom scipy.spatial.distance import minkowski\nfrom swarmauri.core.vector_stores.IDistanceSimilarity import IDistanceSimilarity\nfrom swarmauri.core.vectors.IVector import IVector\n\nclass MinkowskiDistance(IDistanceSimilarity):\n    \"\"\"\n    Implementation of the IDistanceSimiliarity interface using the Minkowski distance metric.\n    Minkowski distance is a generalized metric form that includes Euclidean distance,\n    Manhattan distance, and others depending on the order (p) parameter.\n\n    The class provides methods to compute the Minkowski distance between two vectors.\n    \"\"\"\n\n    def __init__(self, p: int = 2):\n        \"\"\"\n        Initializes the MinkowskiDistance calculator with the specified order.\n\n        Parameters:\n        - p (int): The order of the Minkowski distance. p=2 corresponds to the Euclidean distance,\n                   while p=1 corresponds to the Manhattan distance. Default is 2.\n        \"\"\"\n        self.p = p\n\n    def distance(self, vector_a: IVector, vector_b: IVector) -> float:\n        \"\"\"\n        Computes the Minkowski distance between two vectors.\n\n        Args:\n            vector_a (IVector): The first vector in the comparison.\n            vector_b (IVector): The second vector in the comparison.\n\n        Returns:\n            float: The computed Minkowski distance between vector_a and vector_b.\n        \"\"\"\n        # Check if both vectors have the same dimensionality\n        if vector_a.dimensions != vector_b.dimensions:\n            raise ValueError(\"Vectors must have the same dimensionality.\")\n\n        # Extract data from IVector instances\n        data_a = vector_a.data\n        data_b = vector_b.data\n\n        # Calculate and return the Minkowski distance\n        return minkowski(data_a, data_b, p=self.p)\n\n    def similarity(self, vector_a: IVector, vector_b: IVector) -> float:\n        \"\"\"\n        Compute the similarity between two vectors based on the Minkowski distance.\n        The similarity is inversely related to the distance.\n\n        Args:\n            vector_a (IVector): The first vector to compare for similarity.\n            vector_b (IVector): The second vector to compare with the first vector.\n\n        Returns:\n            float: A similarity score between vector_a and vector_b.\n        \"\"\"\n        dist = self.distance(vector_a, vector_b)\n        return 1 / (1 + dist)  # An example similarity score\n    \n    def distances(self, vector_a: IVector, vectors_b: List[IVector]) -> List[float]:\n        distances = [self.distance(vector_a, vector_b) for vector_b in vectors_b]\n        return distances\n    \n    def similarities(self, vector_a: IVector, vectors_b: List[IVector]) -> List[float]:\n        similarities = [self.similarity(vector_a, vector_b) for vector_b in vectors_b]\n        return similarities\n```"
    },
    {
        "document_name": "swarmauri/experimental/distances/ScannVectorStore.py",
        "content": "```swarmauri/experimental/distances/ScannVectorStore.py\nimport numpy as np\nimport scann\nfrom typing import List, Dict, Union\n\nfrom swarmauri.core.vector_stores.IVectorStore import IVectorStore\nfrom swarmauri.core.vector_stores.ISimiliarityQuery import ISimilarityQuery\nfrom swarmauri.core.vectors.IVector import IVector\nfrom swarmauri.standard.vectors.concrete.SimpleVector import SimpleVector\n\n\nclass ScannVectorStore(IVectorStore, ISimilarityQuery):\n    \"\"\"\n    A vector store that utilizes ScaNN (Scalable Nearest Neighbors) for efficient similarity searches.\n    \"\"\"\n\n    def __init__(self, dimension: int, num_leaves: int = 100, num_leaves_to_search: int = 10, reordering_num_neighbors: int = 100):\n        \"\"\"\n        Initialize the ScaNN vector store with given parameters.\n\n        Parameters:\n        - dimension (int): The dimensionality of the vectors being stored.\n        - num_leaves (int): The number of leaves for the ScaNN partitioning tree.\n        - num_leaves_to_search (int): The number of leaves to search for query time. Must be <= num_leaves.\n        - reordering_num_neighbors (int): The number of neighbors to re-rank based on the exact distance after searching leaves.\n        \"\"\"\n        self.dimension = dimension\n        self.num_leaves = num_leaves\n        self.num_leaves_to_search = num_leaves_to_search\n        self.reordering_num_neighbors = reordering_num_neighbors\n\n        self.searcher = None  # Placeholder for the ScaNN searcher initialized during building\n        self.dataset_vectors = []\n        self.id_to_metadata = {}\n\n    def _build_scann_searcher(self):\n        \"\"\"Build the ScaNN searcher based on current dataset vectors.\"\"\"\n        self.searcher = scann.ScannBuilder(np.array(self.dataset_vectors, dtype=np.float32), num_neighbors=self.reordering_num_neighbors, distance_measure=\"dot_product\").tree(\n            num_leaves=self.num_leaves, num_leaves_to_search=self.num_leaves_to_search, training_sample_size=25000\n        ).score_ah(\n            dimensions_per_block=2\n        ).reorder(self.reordering_num_neighbors).build()\n\n    def add_vector(self, vector_id: str, vector: Union[np.ndarray, List[float]], metadata: Dict = None) -> None:\n        \"\"\"\n        Adds a vector along with its identifier and optional metadata to the store.\n\n        Args:\n            vector_id (str): Unique identifier for the vector.\n            vector (Union[np.ndarray, List[float]]): The high-dimensional vector to be stored.\n            metadata (Dict, optional): Optional metadata related to the vector.\n        \"\"\"\n        if not isinstance(vector, np.ndarray):\n            vector = np.array(vector, dtype=np.float32)\n        \n        if self.searcher is None:\n            self.dataset_vectors.append(vector)\n        else:\n            raise Exception(\"Cannot add vectors after building the index. Rebuild the index to include new vectors.\")\n\n        if metadata is None:\n            metadata = {}\n        self.id_to_metadata[vector_id] = metadata\n\n    def build_index(self):\n        \"\"\"Builds or rebuilds the ScaNN searcher to reflect the current dataset vectors.\"\"\"\n        self._build_scann_searcher()\n\n    def get_vector(self, vector_id: str) -> Union[IVector, None]:\n        \"\"\"\n        Retrieve a vector by its identifier.\n\n        Args:\n            vector_id (str): The unique identifier for the vector.\n\n        Returns:\n            Union[IVector, None]: The vector associated with the given id, or None if not found.\n        \"\"\"\n        if vector_id in self.id_to_metadata:\n            metadata = self.id_to_metadata[vector_id]\n            return SimpleVector(data=metadata.get('vector'), metadata=metadata)\n        return None\n\n    def delete_vector(self, vector_id: str) -> None:\n        \"\"\"\n        Deletes a vector from the ScannVectorStore and marks the index for rebuilding.\n        Note: For simplicity, this function assumes vectors are uniquely identifiable by their metadata.\n\n        Args:\n            vector_id (str): The unique identifier for the vector to be deleted.\n        \"\"\"\n        if vector_id in self.id_to_metadata:\n            # Identify index of the vector to be deleted\n            vector = self.id_to_metadata[vector_id]['vector']\n            index = self.dataset_vectors.index(vector)\n\n            # Remove vector and its metadata\n            del self.dataset_vectors[index]\n            del self.id_to_metadata[vector_id]\n\n            # Since vector order is important for matching ids, rebuild the searcher to reflect deletion\n            self.searcher = None\n        else:\n            # Handle case where vector_id is not found\n            print(f\"Vector ID {vector_id} not found.\")\n\n    def update_vector(self, vector_id: str, new_vector: Union[np.ndarray, List[float]], new_metadata: Dict = None) -> None:\n        \"\"\"\n        Updates an existing vector in the ScannVectorStore and marks the index for rebuilding.\n\n        Args:\n            vector_id (str): The unique identifier for the vector to be updated.\n            new_vector (Union[np.ndarray, List[float]]): The updated vector.\n            new_metadata (Dict, optional): Optional updated metadata for the vector.\n        \"\"\"\n        # Ensure new_vector is numpy array for consistency\n        if not isinstance(new_vector, np.ndarray):\n            new_vector = np.array(new_vector, dtype=np.float32)\n\n        if vector_id in self.id_to_metadata:\n            # Update operation follows delete then add strategy because vector order matters in ScaNN\n            self.delete_vector(vector_id)\n            self.add_vector(vector_id, new_vector, new_metadata)\n        else:\n            # Handle case where vector_id is not found\n            print(f\"Vector ID {vector_id} not found.\")\n\n\n\n    def search_by_similarity_threshold(self, query_vector: Union[np.ndarray, List[float]], similarity_threshold: float, space_name: str = None) -> List[Dict]:\n        \"\"\"\n        Search vectors exceeding a similarity threshold to a query vector within an optional vector space.\n\n        Args:\n            query_vector (Union[np.ndarray, List[float]]): The high-dimensional query vector.\n            similarity_threshold (float): The similarity threshold for filtering results.\n            space_name (str, optional): The name of the vector space to search within. Not used in this implementation.\n\n        Returns:\n            List[Dict]: A list of dictionaries with vector IDs, similarity scores, and optional metadata that meet the similarity threshold.\n        \"\"\"\n        if not isinstance(query_vector, np.ndarray):\n            query_vector = np.array(query_vector, dtype=np.float32)\n        \n        if self.searcher is None:\n            self._build_scann_searcher()\n        \n        _, indices = self.searcher.search(query_vector, final_num_neighbors=self.reordering_num_neighbors)\n        results = [{\"id\": str(idx), \"metadata\": self.id_to_metadata.get(str(idx), {})} for idx in indices if idx < similarity_threshold]\n        return results\n```"
    },
    {
        "document_name": "swarmauri/experimental/distances/SorensenDiceDistance.py",
        "content": "```swarmauri/experimental/distances/SorensenDiceDistance.py\nimport numpy as np\nfrom typing import List\nfrom collections import Counter\n\nfrom swarmauri.core.vector_stores.IDistanceSimilarity import IDistanceSimilarity\nfrom swarmauri.core.vectors.IVector import IVector\n\nclass SorensenDiceDistance(IDistanceSimilarity):\n    \"\"\"\n    Implementing a concrete Vector Store class for calculating S\u00c3\u00b6rensen-Dice Index Distance.\n    The S\u00c3\u00b6rensen-Dice Index, or Dice's coefficient, is a measure of the similarity between two sets.\n    \"\"\"\n\n    def distance(self, vector_a: List[float], vector_b: List[float]) -> float:\n        \"\"\"\n        Compute the S\u00c3\u00b6rensen-Dice distance between two vectors.\n        \n        Args:\n            vector_a (List[float]): The first vector in the comparison.\n            vector_b (List[float]): The second vector in the comparison.\n        \n        Returns:\n            float: The computed S\u00c3\u00b6rensen-Dice distance between vector_a and vector_b.\n        \"\"\"\n        # Convert vectors to binary sets\n        set_a = set([i for i, val in enumerate(vector_a) if val])\n        set_b = set([i for i, val in enumerate(vector_b) if val])\n        \n        # Calculate the intersection size\n        intersection_size = len(set_a.intersection(set_b))\n        \n        # Sorensen-Dice Index calculation\n        try:\n            sorensen_dice_index = (2 * intersection_size) / (len(set_a) + len(set_b))\n        except ZeroDivisionError:\n            sorensen_dice_index = 0.0\n        \n        # Distance is inverse of similarity for S\u00c3\u00b6rensen-Dice\n        distance = 1 - sorensen_dice_index\n        \n        return distance\n    \n    def distances(self, vector_a: IVector, vectors_b: List[IVector]) -> List[float]:\n        distances = [self.distance(vector_a, vector_b) for vector_b in vectors_b]\n        return distances\n    \n    def similarity(self, vector_a: IVector, vectors_b: List[IVector]) -> List[float]:\n        raise NotImplementedError(\"Similarity calculation is not implemented for SorensenDiceDistance.\")\n    \n    def similarities(self, vector_a: IVector, vectors_b: List[IVector]) -> List[float]:\n        raise NotImplementedError(\"Similarity calculation is not implemented for SorensenDiceDistance.\")\n```"
    },
    {
        "document_name": "swarmauri/experimental/distances/SquaredEuclideanDistance.py",
        "content": "```swarmauri/experimental/distances/SquaredEuclideanDistance.py\nfrom typing import List\nfrom swarmauri.core.vector_stores.IDistanceSimilarity import IDistanceSimilarity\nfrom swarmauri.core.vectors.IVector import IVector\n\nclass SquaredEuclideanDistance(IDistanceSimilarity):\n    \"\"\"\n    A concrete class for computing the squared Euclidean distance between two vectors.\n    \"\"\"\n\n    def distance(self, vector_a: IVector, vector_b: IVector) -> float:\n        \"\"\"\n        Computes the squared Euclidean distance between vectors `vector_a` and `vector_b`.\n\n        Parameters:\n        - vector_a (IVector): The first vector in the comparison.\n        - vector_b (IVector): The second vector in the comparison.\n\n        Returns:\n        - float: The computed squared Euclidean distance between vector_a and vector_b.\n        \"\"\"\n        if vector_a.dimensions != vector_b.dimensions:\n            raise ValueError(\"Vectors must be of the same dimensionality.\")\n\n        squared_distance = sum((a - b) ** 2 for a, b in zip(vector_a.data, vector_b.data))\n        return squared_distance\n\n    def similarity(self, vector_a: IVector, vector_b: IVector) -> float:\n        \"\"\"\n        Squared Euclidean distance is not used for calculating similarity.\n        \n        Parameters:\n        - vector_a (IVector): The first vector.\n        - vector_b (IVector): The second vector.\n\n        Raises:\n        - NotImplementedError: Indicates that similarity calculation is not implemented.\n        \"\"\"\n        raise NotImplementedError(\"Similarity calculation is not implemented for Squared Euclidean distance.\")\n        \n        \n    def distances(self, vector_a: IVector, vectors_b: List[IVector]) -> List[float]:\n        distances = [self.distance(vector_a, vector_b) for vector_b in vectors_b]\n        return distances\n    \n    def similarities(self, vector_a: IVector, vectors_b: List[IVector]) -> List[float]:\n        raise NotImplementedError(\"Similarity calculation is not implemented for Squared Euclidean distance.\")\n```"
    },
    {
        "document_name": "swarmauri/experimental/distances/SSASimilarity.py",
        "content": "```swarmauri/experimental/distances/SSASimilarity.py\nfrom typing import Set, List, Dict\nfrom ....core.vector_stores.ISimilarity import ISimilarity\nfrom ....core.vectors.IVector import IVector\n\n\nclass SSASimilarity(ISimilarity):\n    \"\"\"\n    Implements the State Similarity in Arity (SSA) similarity measure to\n    compare states (sets of variables) for their similarity.\n    \"\"\"\n\n    def similarity(self, vector_a: IVector, vector_b: IVector) -> float:\n        \"\"\"\n        Calculate the SSA similarity between two documents by comparing their metadata,\n        assumed to represent states as sets of variables.\n\n        Args:\n        - vector_a (IDocument): The first document.\n        - vector_b (IDocument): The second document to compare with the first document.\n\n        Returns:\n        - float: The SSA similarity measure between vector_a and vector_b, ranging from 0 to 1\n                 where 0 represents no similarity and 1 represents identical states.\n        \"\"\"\n        state_a = set(vector_a.metadata.keys())\n        state_b = set(vector_b.metadata.keys())\n\n        return self.calculate_ssa(state_a, state_b)\n\n    @staticmethod\n    def calculate_ssa(state_a: Set[str], state_b: Set[str]) -> float:\n        \"\"\"\n        Calculate the State Similarity in Arity (SSA) between two states.\n\n        Parameters:\n        - state_a (Set[str]): A set of variables representing state A.\n        - state_b (Set[str]): A set of variables representing state B.\n\n        Returns:6\n        - float: The SSA similarity measure, ranging from 0 (no similarity) to 1 (identical states).\n        \"\"\"\n        # Calculate the intersection (shared variables) between the two states\n        shared_variables = state_a.intersection(state_b)\n        \n        # Calculate the union (total unique variables) of the two states\n        total_variables = state_a.union(state_b)\n        \n        # Calculate the SSA measure as the ratio of shared to total variables\n        ssa = len(shared_variables) / len(total_variables) if total_variables else 1\n        \n        return ssa\n```"
    },
    {
        "document_name": "swarmauri/experimental/distances/SSIVSimilarity.py",
        "content": "```swarmauri/experimental/distances/SSIVSimilarity.py\nfrom typing import List, Dict, Set\nfrom ....core.vector_stores.ISimilarity import ISimilarity\n\nclass SSIVSimilarity(ISimilarity):\n    \"\"\"\n    Concrete class that implements ISimilarity interface using\n    State Similarity of Important Variables (SSIV) as the similarity measure.\n    \"\"\"\n\n    def similarity(self, state_a: Set[str], state_b: Set[str], importance_a: Dict[str, float], importance_b: Dict[str, float]) -> float:\n        \"\"\"\n        Calculate the SSIV between two states represented by sets of variables.\n\n        Parameters:\n        - state_a (Set[str]): A set of variables representing state A.\n        - state_b (Set[str]): A set of variables representing state B.\n        - importance_a (Dict[str, float]): A dictionary where keys are variables in state A and values are their importance weights.\n        - importance_b (Dict[str, float]): A dictionary where keys are variables in state B and values are their importance weights.\n\n        Returns:\n        - float: The SSIV similarity measure, ranging from 0 to 1.\n        \"\"\"\n        return self.calculate_ssiv(state_a, state_b, importance_a, importance_b)\n\n    @staticmethod\n    def calculate_ssiv(state_a: Set[str], state_b: Set[str], importance_a: Dict[str, float], importance_b: Dict[str, float]) -> float:\n        \"\"\"\n        Calculate the State Similarity of Important Variables (SSIV) between two states.\n\n        Parameters:\n        - state_a (Set[str]): A set of variables representing state A.\n        - state_b (Set[str]): A set of variables representing state B.\n        - importance_a (Dict[str, float]): A dictionary where keys are variables in state A and values are their importance weights.\n        - importance_b (Dict[str, float]): A dictionary where keys are variables in state B and values are their importance weights.\n\n        Returns:\n        - float: The SSIV similarity measure, ranging from 0 to 1.\n        \n        Note: It is assumed that the importance weights are non-negative.\n        \"\"\"\n        shared_variables = state_a.intersection(state_b)\n        \n        # Calculate the summed importance of shared variables\n        shared_importance_sum = sum(importance_a[var] for var in shared_variables) + sum(importance_b[var] for var in shared_variables)\n        \n        # Calculate the total importance of all variables in both states\n        total_importance_sum = sum(importance_a.values()) + sum(importance_b.values())\n        \n        # Calculate and return the SSIV\n        ssiv = (2 * shared_importance_sum) / total_importance_sum if total_importance_sum != 0 else 0\n        return ssiv\n\n```"
    },
    {
        "document_name": "swarmauri/experimental/distances/__init__.py",
        "content": "```swarmauri/experimental/distances/__init__.py\n# -*- coding: utf-8 -*-\n\n\n```"
    },
    {
        "document_name": "swarmauri/experimental/apis/CeleryAgentCommands.py",
        "content": "```swarmauri/experimental/apis/CeleryAgentCommands.py\nfrom celery import Celery\nfrom swarmauri.core.agent_apis.IAgentCommands import IAgentCommands\nfrom typing import Callable, Any, Dict\n\nclass CeleryAgentCommands(IAgentCommands):\n    def __init__(self, broker_url: str, backend_url: str):\n        \"\"\"\n        Initializes the Celery application with the specified broker and backend URLs.\n        \"\"\"\n        self.app = Celery('swarmauri_agent_tasks', broker=broker_url, backend=backend_url)\n\n    def register_command(self, command_name: str, function: Callable[..., Any], *args, **kwargs) -> None:\n        \"\"\"\n        Registers a new command as a Celery task.\n        \"\"\"\n        self.app.task(name=command_name, bind=True)(function)\n\n    def execute_command(self, command_name: str, *args, **kwargs) -> Any:\n        \"\"\"\n        Executes a registered command by name asynchronously.\n        \"\"\"\n        result = self.app.send_task(command_name, args=args, kwargs=kwargs)\n        return result.get()\n\n    def get_status(self, task_id: str) -> Dict[str, Any]:\n        \"\"\"\n        Fetches the status of a command execution via its task ID.\n        \"\"\"\n        async_result = self.app.AsyncResult(task_id)\n        return {\"status\": async_result.status, \"result\": async_result.result if async_result.ready() else None}\n\n    def revoke_command(self, task_id: str) -> None:\n        \"\"\"\n        Revokes or terminates a command execution by its task ID.\n        \"\"\"\n        self.app.control.revoke(task_id, terminate=True)\n```"
    }
]